Method,Called_Method,function,option,Method_short,path,class_name,xml_path,Method_body
M:org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions:shouldEncrypt(org.apache.cassandra.locator.InetAddressAndPort),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,shouldEncrypt,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/config/EncryptionOptions.java,EncryptionOptions$ServerEncryptionOptions,../data/xml/cassandra/EncryptionOptions.xml,"public boolean shouldEncrypt(InetAddressAndPort endpoint)
        {
            IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
            switch (internode_encryption)
            {
                case none:
                    return false; // if nothing needs to be encrypted then return immediately.
                case all:
                    break;
                case dc:
                    if (snitch.getDatacenter(endpoint).equals(snitch.getLocalDatacenter()))
                        return false;
                    break;
                case rack:
                    // for rack then check if the DC's are the same.
                    if (snitch.getRack(endpoint).equals(snitch.getLocalRack())
                        && snitch.getDatacenter(endpoint).equals(snitch.getLocalDatacenter()))
                        return false;
                    break;
            }
            return true;
        }

        "
"M:org.apache.cassandra.cql3.statements.DescribeStatement$5:describe(org.apache.cassandra.service.ClientState,org.apache.cassandra.schema.Keyspaces)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,describe,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/cql3/statements/DescribeStatement.java,DescribeStatement$5,../data/xml/cassandra/DescribeStatement.xml,"@Override
        protected Stream<? extends SchemaElement> describe(ClientState state, Keyspaces keyspaces)
        {
            String keyspace = state.getRawKeyspace();
            Stream<KeyspaceMetadata> stream = keyspace == null ? keyspaces.stream().sorted(SchemaElement.NAME_COMPARATOR)
                                                               : Stream.of(validateKeyspace(keyspace, keyspaces));

            return stream.flatMap(k -> elementsProvider.apply(k).sorted(SchemaElement.NAME_COMPARATOR));
        }

        
@Override
            protected Stream<? extends SchemaElement> describe(ClientState state, Keyspaces keyspaces)
            {
                return keyspaces.stream().sorted(SchemaElement.NAME_COMPARATOR);
            }

            
@Override
            protected Stream<? extends SchemaElement> describe(ClientState state, Keyspaces keyspaces)
            {
                return keyspaces.stream()
                                .filter(ks -> includeSystemKeyspaces || !SchemaConstants.isSystemKeyspace(ks.name))
                                .sorted(SchemaElement.NAME_COMPARATOR)
                                .flatMap(ks -> getKeyspaceElements(ks, false));
            }

            
@Override
        protected Stream<? extends SchemaElement> describe(ClientState state, Keyspaces keyspaces)
        {
            String ks = keyspace == null ? checkNotNull(state.getRawKeyspace(), ""No keyspace specified and no current keyspace"")
                                         : keyspace;

            return elementsProvider.apply(validateKeyspace(ks, keyspaces), name);
        }

        
@Override
            protected Stream<? extends SchemaElement> describe(ClientState state, Keyspaces keyspaces)
            {
                delegate = resolve(state, keyspaces);
                return delegate.describe(state, keyspaces);
            }

            
@Override
            protected Stream<List<Object>> describe(ClientState state, Keyspaces keyspaces)
            {
                List<Object> list = new ArrayList<Object>();
                list.add(DatabaseDescriptor.getClusterName());
                list.add(trimIfPresent(DatabaseDescriptor.getPartitionerName(), ""org.apache.cassandra.dht.""));
                list.add(trimIfPresent(DatabaseDescriptor.getEndpointSnitch().getClass().getName(),
                                            ""org.apache.cassandra.locator.""));
 
                String useKs = state.getRawKeyspace();
                if (mustReturnsRangeOwnerships(useKs))
                {
                    list.add(StorageService.instance.getRangeToAddressMap(useKs)
                                                    .entrySet()
                                                    .stream()
                                                    .sorted(Comparator.comparing(Map.Entry::getKey))
                                                    .collect(Collectors.toMap(e -> e.getKey().right.toString(),
                                                                              e -> e.getValue()
                                                                                    .stream()
                                                                                    .map(r -> r.endpoint().toString())
                                                                                    .collect(Collectors.toList()))));
                }
                return Stream.of(list);
            }

            "
M:org.apache.cassandra.db.CounterMutationVerbHandler:doVerb(org.apache.cassandra.net.Message),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,doVerb,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/db/CounterMutationVerbHandler.java,CounterMutationVerbHandler,../data/xml/cassandra/CounterMutationVerbHandler.xml,"public void doVerb(final Message<CounterMutation> message)
    {
        long queryStartNanoTime = System.nanoTime();
        final CounterMutation cm = message.payload;
        logger.trace(""Applying forwarded {}"", cm);

        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
        // We should not wait for the result of the write in this thread,
        // otherwise we could have a distributed deadlock between replicas
        // running this VerbHandler (see #4578).
        // Instead, we use a callback to send the response. Note that the callback
        // will not be called if the request timeout, but this is ok
        // because the coordinator of the counter mutation will timeout on
        // it's own in that case.
        StorageProxy.applyCounterMutationOnLeader(cm,
                                                  localDataCenter,
                                                  () -> MessagingService.instance().send(message.emptyResponse(), message.from()),
                                                  queryStartNanoTime);
    }
}"
M:org.apache.cassandra.db.SystemKeyspace:persistLocalMetadata(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,persistLocalMetadata,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/db/SystemKeyspace.java,SystemKeyspace,../data/xml/cassandra/SystemKeyspace.xml,"public static void persistLocalMetadata()
    {
        String req = ""INSERT INTO system.%s ("" +
                     ""key,"" +
                     ""cluster_name,"" +
                     ""release_version,"" +
                     ""cql_version,"" +
                     ""native_protocol_version,"" +
                     ""data_center,"" +
                     ""rack,"" +
                     ""partitioner,"" +
                     ""rpc_address,"" +
                     ""rpc_port,"" +
                     ""broadcast_address,"" +
                     ""broadcast_port,"" +
                     ""listen_address,"" +
                     ""listen_port"" +
                     "") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"";
        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        executeOnceInternal(format(req, LOCAL),
                            LOCAL,
                            DatabaseDescriptor.getClusterName(),
                            FBUtilities.getReleaseVersionString(),
                            QueryProcessor.CQL_VERSION.toString(),
                            String.valueOf(ProtocolVersion.CURRENT.asInt()),
                            snitch.getLocalDatacenter(),
                            snitch.getLocalRack(),
                            DatabaseDescriptor.getPartitioner().getClass().getName(),
                            DatabaseDescriptor.getRpcAddress(),
                            DatabaseDescriptor.getNativeTransportPort(),
                            FBUtilities.getJustBroadcastAddress(),
                            DatabaseDescriptor.getStoragePort(),
                            FBUtilities.getJustLocalAddress(),
                            DatabaseDescriptor.getStoragePort());
    }

    "
"M:org.apache.cassandra.db.view.ViewUtils:getViewNaturalEndpoint(org.apache.cassandra.locator.AbstractReplicationStrategy,org.apache.cassandra.dht.Token,org.apache.cassandra.dht.Token)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getViewNaturalEndpoint,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/db/view/ViewUtils.java,ViewUtils,../data/xml/cassandra/ViewUtils.xml,"/**
     * Calculate the natural endpoint for the view.
     *
     * The view natural endpoint is the endpoint which has the same cardinality as this node in the replication factor.
     * The cardinality is the number at which this node would store a piece of data, given the change in replication
     * factor. If the keyspace's replication strategy is a NetworkTopologyStrategy, we filter the ring to contain only
     * nodes in the local datacenter when calculating cardinality.
     *
     * For example, if we have the following ring:
     *   {@code A, T1 -> B, T2 -> C, T3 -> A}
     *
     * For the token T1, at RF=1, A would be included, so A's cardinality for T1 is 1. For the token T1, at RF=2, B would
     * be included, so B's cardinality for token T1 is 2. For token T3, at RF = 2, A would be included, so A's cardinality
     * for T3 is 2.
     *
     * For a view whose base token is T1 and whose view token is T3, the pairings between the nodes would be:
     *  A writes to C (A's cardinality is 1 for T1, and C's cardinality is 1 for T3)
     *  B writes to A (B's cardinality is 2 for T1, and A's cardinality is 2 for T3)
     *  C writes to B (C's cardinality is 3 for T1, and B's cardinality is 3 for T3)
     *
     * @return Optional.empty() if this method is called using a base token which does not belong to this replica
     */
public static Optional<Replica> getViewNaturalEndpoint(AbstractReplicationStrategy replicationStrategy, Token baseToken, Token viewToken)
    {
        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
        EndpointsForToken naturalBaseReplicas = replicationStrategy.getNaturalReplicasForToken(baseToken);
        EndpointsForToken naturalViewReplicas = replicationStrategy.getNaturalReplicasForToken(viewToken);

        Optional<Replica> localReplica = Iterables.tryFind(naturalViewReplicas, Replica::isSelf).toJavaUtil();
        if (localReplica.isPresent())
            return localReplica;

        // We only select replicas from our own DC
        // TODO: this is poor encapsulation, leaking implementation details of replication strategy
        Predicate<Replica> isLocalDC = r -> !(replicationStrategy instanceof NetworkTopologyStrategy)
                || DatabaseDescriptor.getEndpointSnitch().getDatacenter(r).equals(localDataCenter);

        // We have to remove any endpoint which is shared between the base and the view, as it will select itself
        // and throw off the counts otherwise.
        EndpointsForToken baseReplicas = naturalBaseReplicas.filter(
                r -> !naturalViewReplicas.endpoints().contains(r.endpoint()) && isLocalDC.test(r)
        );
        EndpointsForToken viewReplicas = naturalViewReplicas.filter(
                r -> !naturalBaseReplicas.endpoints().contains(r.endpoint()) && isLocalDC.test(r)
        );

        // The replication strategy will be the same for the base and the view, as they must belong to the same keyspace.
        // Since the same replication strategy is used, the same placement should be used and we should get the same
        // number of replicas for all of the tokens in the ring.
        assert baseReplicas.size() == viewReplicas.size() : ""Replication strategy should have the same number of endpoints for the base and the view"";

        int baseIdx = -1;
        for (int i=0; i<baseReplicas.size(); i++)
        {
            if (baseReplicas.get(i).isSelf())
            {
                baseIdx = i;
                break;
            }
        }

        if (baseIdx < 0)
            //This node is not a base replica of this key, so we return empty
            return Optional.empty();

        return Optional.of(viewReplicas.get(baseIdx));
    }
}"
"M:org.apache.cassandra.db.view.ViewUtils:lambda$getViewNaturalEndpoint$0(org.apache.cassandra.locator.AbstractReplicationStrategy,java.lang.String,org.apache.cassandra.locator.Replica)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,lambda$getViewNaturalEndpoint$0,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/db/view/ViewUtils.java,ViewUtils,../data/xml/cassandra/ViewUtils.xml,"public final class ViewUtils
{
    private ViewUtils()
    {
    }

    /**
     * Calculate the natural endpoint for the view.
     *
     * The view natural endpoint is the endpoint which has the same cardinality as this node in the replication factor.
     * The cardinality is the number at which this node would store a piece of data, given the change in replication
     * factor. If the keyspace's replication strategy is a NetworkTopologyStrategy, we filter the ring to contain only
     * nodes in the local datacenter when calculating cardinality.
     *
     * For example, if we have the following ring:
     *   {@code A, T1 -> B, T2 -> C, T3 -> A}
     *
     * For the token T1, at RF=1, A would be included, so A's cardinality for T1 is 1. For the token T1, at RF=2, B would
     * be included, so B's cardinality for token T1 is 2. For token T3, at RF = 2, A would be included, so A's cardinality
     * for T3 is 2.
     *
     * For a view whose base token is T1 and whose view token is T3, the pairings between the nodes would be:
     *  A writes to C (A's cardinality is 1 for T1, and C's cardinality is 1 for T3)
     *  B writes to A (B's cardinality is 2 for T1, and A's cardinality is 2 for T3)
     *  C writes to B (C's cardinality is 3 for T1, and B's cardinality is 3 for T3)
     *
     * @return Optional.empty() if this method is called using a base token which does not belong to this replica
     */
    public static Optional<Replica> getViewNaturalEndpoint(AbstractReplicationStrategy replicationStrategy, Token baseToken, Token viewToken)
    {
        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
        EndpointsForToken naturalBaseReplicas = replicationStrategy.getNaturalReplicasForToken(baseToken);
        EndpointsForToken naturalViewReplicas = replicationStrategy.getNaturalReplicasForToken(viewToken);

        Optional<Replica> localReplica = Iterables.tryFind(naturalViewReplicas, Replica::isSelf).toJavaUtil();
        if (localReplica.isPresent())
            return localReplica;

        // We only select replicas from our own DC
        // TODO: this is poor encapsulation, leaking implementation details of replication strategy
        Predicate<Replica> isLocalDC = r -> !(replicationStrategy instanceof NetworkTopologyStrategy)
                || DatabaseDescriptor.getEndpointSnitch().getDatacenter(r).equals(localDataCenter);

        // We have to remove any endpoint which is shared between the base and the view, as it will select itself
        // and throw off the counts otherwise.
        EndpointsForToken baseReplicas = naturalBaseReplicas.filter(
                r -> !naturalViewReplicas.endpoints().contains(r.endpoint()) && isLocalDC.test(r)
        );
        EndpointsForToken viewReplicas = naturalViewReplicas.filter(
                r -> !naturalBaseReplicas.endpoints().contains(r.endpoint()) && isLocalDC.test(r)
        );

        // The replication strategy will be the same for the base and the view, as they must belong to the same keyspace.
        // Since the same replication strategy is used, the same placement should be used and we should get the same
        // number of replicas for all of the tokens in the ring.
        assert baseReplicas.size() == viewReplicas.size() : ""Replication strategy should have the same number of endpoints for the base and the view"";

        int baseIdx = -1;
        for (int i=0; i<baseReplicas.size(); i++)
        {
            if (baseReplicas.get(i).isSelf())
            {
                baseIdx = i;
                break;
            }
        }

        if (baseIdx < 0)
            //This node is not a base replica of this key, so we return empty
            return Optional.empty();

        return Optional.of(viewReplicas.get(baseIdx));
    }
}
"
"M:org.apache.cassandra.db.virtual.InternodeInboundTable:addRow(org.apache.cassandra.db.virtual.SimpleDataSet,org.apache.cassandra.locator.InetAddressAndPort,org.apache.cassandra.net.InboundMessageHandlers)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,addRow,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/db/virtual/InternodeInboundTable.java,InternodeInboundTable,../data/xml/cassandra/InternodeInboundTable.xml,"private void addRow(SimpleDataSet dataSet, InetAddressAndPort addressAndPort, InboundMessageHandlers handlers)
    {
        String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(addressAndPort);
        String rack = DatabaseDescriptor.getEndpointSnitch().getRack(addressAndPort);
        dataSet.row(addressAndPort.address, addressAndPort.port, dc, rack)
               .column(USING_BYTES, handlers.usingCapacity())
               .column(USING_RESERVE_BYTES, handlers.usingEndpointReserveCapacity())
               .column(CORRUPT_FRAMES_RECOVERED, handlers.corruptFramesRecovered())
               .column(CORRUPT_FRAMES_UNRECOVERED, handlers.corruptFramesUnrecovered())
               .column(ERROR_BYTES, handlers.errorBytes())
               .column(ERROR_COUNT, handlers.errorCount())
               .column(EXPIRED_BYTES, handlers.expiredBytes())
               .column(EXPIRED_COUNT, handlers.expiredCount())
               .column(SCHEDULED_BYTES, handlers.scheduledBytes())
               .column(SCHEDULED_COUNT, handlers.scheduledCount())
               .column(PROCESSED_BYTES, handlers.processedBytes())
               .column(PROCESSED_COUNT, handlers.processedCount())
               .column(RECEIVED_BYTES, handlers.receivedBytes())
               .column(RECEIVED_COUNT, handlers.receivedCount())
               .column(THROTTLED_COUNT, handlers.throttledCount())
               .column(THROTTLED_NANOS, handlers.throttledNanos());
    }
}"
"M:org.apache.cassandra.db.virtual.InternodeOutboundTable:addRow(org.apache.cassandra.db.virtual.SimpleDataSet,org.apache.cassandra.locator.InetAddressAndPort,org.apache.cassandra.net.OutboundConnections)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,addRow,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/db/virtual/InternodeOutboundTable.java,InternodeOutboundTable,../data/xml/cassandra/InternodeOutboundTable.xml,"private void addRow(SimpleDataSet dataSet, InetAddressAndPort addressAndPort, OutboundConnections connections)
    {
        String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(addressAndPort);
        String rack = DatabaseDescriptor.getEndpointSnitch().getRack(addressAndPort);
        long pendingBytes = sum(connections, OutboundConnection::pendingBytes);
        dataSet.row(addressAndPort.address, addressAndPort.port, dc, rack)
               .column(USING_BYTES, pendingBytes)
               .column(USING_RESERVE_BYTES, connections.usingReserveBytes())
               .column(PENDING_COUNT, sum(connections, OutboundConnection::pendingCount))
               .column(PENDING_BYTES, pendingBytes)
               .column(SENT_COUNT, sum(connections, OutboundConnection::sentCount))
               .column(SENT_BYTES, sum(connections, OutboundConnection::sentBytes))
               .column(EXPIRED_COUNT, sum(connections, OutboundConnection::expiredCount))
               .column(EXPIRED_BYTES, sum(connections, OutboundConnection::expiredBytes))
               .column(ERROR_COUNT, sum(connections, OutboundConnection::errorCount))
               .column(ERROR_BYTES, sum(connections, OutboundConnection::errorBytes))
               .column(OVERLOAD_COUNT, sum(connections, OutboundConnection::overloadedCount))
               .column(OVERLOAD_BYTES, sum(connections, OutboundConnection::overloadedBytes))
               .column(ACTIVE_CONNECTION_COUNT, sum(connections, c -> c.isConnected() ? 1 : 0))
               .column(CONNECTION_ATTEMPTS, sum(connections, OutboundConnection::connectionAttempts))
               .column(SUCCESSFUL_CONNECTION_ATTEMPTS, sum(connections, OutboundConnection::successfulConnections));
    }

    "
"M:org.apache.cassandra.dht.BootStrapper:bootstrap(org.apache.cassandra.dht.StreamStateStore,boolean)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,bootstrap,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/dht/BootStrapper.java,BootStrapper,../data/xml/cassandra/BootStrapper.xml,"public ListenableFuture<StreamState> bootstrap(StreamStateStore stateStore, boolean useStrictConsistency)
    {
        logger.trace(""Beginning bootstrap process"");

        RangeStreamer streamer = new RangeStreamer(tokenMetadata,
                                                   tokens,
                                                   address,
                                                   StreamOperation.BOOTSTRAP,
                                                   useStrictConsistency,
                                                   DatabaseDescriptor.getEndpointSnitch(),
                                                   stateStore,
                                                   true,
                                                   DatabaseDescriptor.getStreamingConnectionsPerHost());
        final List<String> nonLocalStrategyKeyspaces = Schema.instance.getNonLocalStrategyKeyspaces();
        if (nonLocalStrategyKeyspaces.isEmpty())
            logger.debug(""Schema does not contain any non-local keyspaces to stream on bootstrap"");
        for (String keyspaceName : nonLocalStrategyKeyspaces)
        {
            AbstractReplicationStrategy strategy = Keyspace.open(keyspaceName).getReplicationStrategy();
            streamer.addRanges(keyspaceName, strategy.getPendingAddressRanges(tokenMetadata, tokens, address));
        }

        StreamResultFuture bootstrapStreamResult = streamer.fetchAsync();
        bootstrapStreamResult.addEventListener(new StreamEventHandler()
        {
            private final AtomicInteger receivedFiles = new AtomicInteger();
            private final AtomicInteger totalFilesToReceive = new AtomicInteger();

            @Override
            public void handleStreamEvent(StreamEvent event)
            {
                switch (event.eventType)
                {
                    case STREAM_PREPARED:
                        StreamEvent.SessionPreparedEvent prepared = (StreamEvent.SessionPreparedEvent) event;
                        int currentTotal = totalFilesToReceive.addAndGet((int) prepared.session.getTotalFilesToReceive());
                        ProgressEvent prepareProgress = new ProgressEvent(ProgressEventType.PROGRESS, receivedFiles.get(), currentTotal, ""prepare with "" + prepared.session.peer + "" complete"");
                        fireProgressEvent(""bootstrap"", prepareProgress);
                        break;

                    case FILE_PROGRESS:
                        StreamEvent.ProgressEvent progress = (StreamEvent.ProgressEvent) event;
                        if (progress.progress.isCompleted())
                        {
                            int received = receivedFiles.incrementAndGet();
                            ProgressEvent currentProgress = new ProgressEvent(ProgressEventType.PROGRESS, received, totalFilesToReceive.get(), ""received file "" + progress.progress.fileName);
                            fireProgressEvent(""bootstrap"", currentProgress);
                        }
                        break;

                    case STREAM_COMPLETE:
                        StreamEvent.SessionCompleteEvent completeEvent = (StreamEvent.SessionCompleteEvent) event;
                        ProgressEvent completeProgress = new ProgressEvent(ProgressEventType.PROGRESS, receivedFiles.get(), totalFilesToReceive.get(), ""session with "" + completeEvent.peer + "" complete"");
                        fireProgressEvent(""bootstrap"", completeProgress);
                        break;
                }
            }

            @Override
            public void onSuccess(StreamState streamState)
            {
                ProgressEventType type;
                String message;

                if (streamState.hasFailedSession())
                {
                    type = ProgressEventType.ERROR;
                    message = ""Some bootstrap stream failed"";
                }
                else
                {
                    type = ProgressEventType.SUCCESS;
                    message = ""Bootstrap streaming success"";
                }
                ProgressEvent currentProgress = new ProgressEvent(type, receivedFiles.get(), totalFilesToReceive.get(), message);
                fireProgressEvent(""bootstrap"", currentProgress);
            }

            @Override
            public void onFailure(Throwable throwable)
            {
                ProgressEvent currentProgress = new ProgressEvent(ProgressEventType.ERROR, receivedFiles.get(), totalFilesToReceive.get(), throwable.getMessage());
                fireProgressEvent(""bootstrap"", currentProgress);
            }
        });
        return bootstrapStreamResult;
    }

    "
M:org.apache.cassandra.dht.Datacenters$DCHandle:<clinit>(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,<clinit>,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/dht/Datacenters.java,Datacenters$DCHandle,../data/xml/cassandra/Datacenters.xml,"public class Datacenters
{

    private static class DCHandle
    {
        private static final String thisDc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
    }

    public static String thisDatacenter()
    {
        return DCHandle.thisDc;
    }

    /*
     * (non-javadoc) Method to generate list of valid data center names to be used to validate the replication parameters during CREATE / ALTER keyspace operations.
     * All peers of current node are fetched from {@link TokenMetadata} and then a set is build by fetching DC name of each peer.
     * @return a set of valid DC names
     */
    public static Set<String> getValidDatacenters()
    {
        final Set<String> validDataCenters = new HashSet<>();
        final IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();

        // Add data center of localhost.
        validDataCenters.add(thisDatacenter());
        // Fetch and add DCs of all peers.
        for (InetAddressAndPort peer : StorageService.instance.getTokenMetadata().getAllEndpoints())
        {
            validDataCenters.add(snitch.getDatacenter(peer));
        }

        return validDataCenters;
    }
}
"
M:org.apache.cassandra.dht.Datacenters:getValidDatacenters(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getValidDatacenters,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/dht/Datacenters.java,Datacenters,../data/xml/cassandra/Datacenters.xml,"/*
     * (non-javadoc) Method to generate list of valid data center names to be used to validate the replication parameters during CREATE / ALTER keyspace operations.
     * All peers of current node are fetched from {@link TokenMetadata} and then a set is build by fetching DC name of each peer.
     * @return a set of valid DC names
     */
public static Set<String> getValidDatacenters()
    {
        final Set<String> validDataCenters = new HashSet<>();
        final IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();

        // Add data center of localhost.
        validDataCenters.add(thisDatacenter());
        // Fetch and add DCs of all peers.
        for (InetAddressAndPort peer : StorageService.instance.getTokenMetadata().getAllEndpoints())
        {
            validDataCenters.add(snitch.getDatacenter(peer));
        }

        return validDataCenters;
    }
}"
M:org.apache.cassandra.dht.RangeFetchMapCalculator:isInLocalDC(org.apache.cassandra.locator.Replica),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,isInLocalDC,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/dht/RangeFetchMapCalculator.java,RangeFetchMapCalculator,../data/xml/cassandra/RangeFetchMapCalculator.xml,"private boolean isInLocalDC(Replica replica)
    {
        return DatabaseDescriptor.getLocalDataCenter().equals(DatabaseDescriptor.getEndpointSnitch().getDatacenter(replica));
    }

    "
"M:org.apache.cassandra.dht.tokenallocator.TokenAllocation:allocateTokens(org.apache.cassandra.locator.TokenMetadata,int,org.apache.cassandra.locator.InetAddressAndPort,int)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,allocateTokens,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java,TokenAllocation,../data/xml/cassandra/TokenAllocation.xml,"public static Collection<Token> allocateTokens(final TokenMetadata tokenMetadata,
                                                   final AbstractReplicationStrategy rs,
                                                   final InetAddressAndPort endpoint,
                                                   int numTokens)
    {
        return create(tokenMetadata, rs, numTokens).allocate(endpoint);
    }

    
public static Collection<Token> allocateTokens(final TokenMetadata tokenMetadata,
                                                   final int replicas,
                                                   final InetAddressAndPort endpoint,
                                                   int numTokens)
    {
        return create(DatabaseDescriptor.getEndpointSnitch(), tokenMetadata, replicas, numTokens).allocate(endpoint);
    }

    "
"M:org.apache.cassandra.gms.Gossiper:start(int,java.util.Map)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,start,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/gms/Gossiper.java,Gossiper,../data/xml/cassandra/Gossiper.xml,"public void start(int generationNumber)
    {
        start(generationNumber, new EnumMap<>(ApplicationState.class));
    }

    
/**
     * Start the gossiper with the generation number, preloading the map of application states before starting
     */
public void start(int generationNbr, Map<ApplicationState, VersionedValue> preloadLocalStates)
    {
        buildSeedsList();
        /* initialize the heartbeat state for this localEndpoint */
        maybeInitializeLocalState(generationNbr);
        EndpointState localState = endpointStateMap.get(FBUtilities.getBroadcastAddressAndPort());
        localState.addApplicationStates(preloadLocalStates);
        minVersionSupplier.recompute();

        //notify snitches that Gossiper is about to start
        DatabaseDescriptor.getEndpointSnitch().gossiperStarting();
        if (logger.isTraceEnabled())
            logger.trace(""gossip started with generation {}"", localState.getHeartBeatState().getGeneration());

        scheduledGossipTask = executor.scheduleWithFixedDelay(new GossipTask(),
                                                              Gossiper.intervalInMillis,
                                                              Gossiper.intervalInMillis,
                                                              TimeUnit.MILLISECONDS);
    }

    "
M:org.apache.cassandra.locator.EndpointSnitchInfo:getDatacenter(java.lang.String),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getDatacenter,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/EndpointSnitchInfo.java,EndpointSnitchInfo,../data/xml/cassandra/EndpointSnitchInfo.xml,"public String getDatacenter(String host) throws UnknownHostException
    {
        return DatabaseDescriptor.getEndpointSnitch().getDatacenter(InetAddressAndPort.getByName(host));
    }

    
public String getDatacenter()
    {
        return DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
    }

    "
M:org.apache.cassandra.locator.EndpointSnitchInfo:getRack(java.lang.String),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getRack,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/EndpointSnitchInfo.java,EndpointSnitchInfo,../data/xml/cassandra/EndpointSnitchInfo.xml,"public String getRack(String host) throws UnknownHostException
    {
        return DatabaseDescriptor.getEndpointSnitch().getRack(InetAddressAndPort.getByName(host));
    }

    
public String getRack()
    {
        return DatabaseDescriptor.getEndpointSnitch().getLocalRack();
    }

    "
M:org.apache.cassandra.locator.EndpointSnitchInfo:getDatacenter(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getDatacenter,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/EndpointSnitchInfo.java,EndpointSnitchInfo,../data/xml/cassandra/EndpointSnitchInfo.xml,"public String getDatacenter(String host) throws UnknownHostException
    {
        return DatabaseDescriptor.getEndpointSnitch().getDatacenter(InetAddressAndPort.getByName(host));
    }

    
public String getDatacenter()
    {
        return DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
    }

    "
M:org.apache.cassandra.locator.EndpointSnitchInfo:getRack(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getRack,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/EndpointSnitchInfo.java,EndpointSnitchInfo,../data/xml/cassandra/EndpointSnitchInfo.xml,"public String getRack(String host) throws UnknownHostException
    {
        return DatabaseDescriptor.getEndpointSnitch().getRack(InetAddressAndPort.getByName(host));
    }

    
public String getRack()
    {
        return DatabaseDescriptor.getEndpointSnitch().getLocalRack();
    }

    "
M:org.apache.cassandra.locator.EndpointSnitchInfo:getSnitchName(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getSnitchName,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/EndpointSnitchInfo.java,EndpointSnitchInfo,../data/xml/cassandra/EndpointSnitchInfo.xml,"public String getSnitchName()
    {
        return DatabaseDescriptor.getEndpointSnitch().getClass().getName();
    }
}"
M:org.apache.cassandra.locator.InOurDcTester:stale(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,stale,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/InOurDcTester.java,InOurDcTester,../data/xml/cassandra/InOurDcTester.xml,"boolean stale()
    {
        return dc != DatabaseDescriptor.getLocalDataCenter()
                || snitch != DatabaseDescriptor.getEndpointSnitch()
                // this final clause checks if somehow the snitch/localDc have got out of whack;
                // presently, this is possible but very unlikely, but this check will also help
                // resolve races on these global fields as well
                || !dc.equals(snitch.getLocalDatacenter());
    }

    "
M:org.apache.cassandra.locator.InOurDcTester:replicas(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,replicas,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/InOurDcTester.java,InOurDcTester,../data/xml/cassandra/InOurDcTester.xml,"public static Predicate<Replica> replicas()
    {
        ReplicaTester cur = replicas;
        if (cur == null || cur.stale())
            replicas = cur = new ReplicaTester(DatabaseDescriptor.getLocalDataCenter(), DatabaseDescriptor.getEndpointSnitch());
        return cur;
    }

    "
M:org.apache.cassandra.locator.InOurDcTester:endpoints(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,endpoints,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/InOurDcTester.java,InOurDcTester,../data/xml/cassandra/InOurDcTester.xml,"public static Predicate<InetAddressAndPort> endpoints()
    {
        EndpointTester cur = endpoints;
        if (cur == null || cur.stale())
            endpoints = cur = new EndpointTester(DatabaseDescriptor.getLocalDataCenter(), DatabaseDescriptor.getEndpointSnitch());
        return cur;
    }

}"
"M:org.apache.cassandra.locator.ReplicaLayout:forTokenReadLiveSorted(org.apache.cassandra.locator.AbstractReplicationStrategy,org.apache.cassandra.dht.Token)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,forTokenReadLiveSorted,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/ReplicaLayout.java,ReplicaLayout,../data/xml/cassandra/ReplicaLayout.xml,"/**
     * @return the read layout for a token - this includes only live natural replicas, i.e. those that are not pending
     * and not marked down by the failure detector. these are reverse sorted by the badness score of the configured snitch
     */
static ReplicaLayout.ForTokenRead forTokenReadLiveSorted(AbstractReplicationStrategy replicationStrategy, Token token)
    {
        EndpointsForToken replicas = replicationStrategy.getNaturalReplicasForToken(token);
        replicas = DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(), replicas);
        replicas = replicas.filter(FailureDetector.isReplicaAlive);
        return new ReplicaLayout.ForTokenRead(replicationStrategy, replicas);
    }

    "
"M:org.apache.cassandra.locator.ReplicaLayout:forRangeReadLiveSorted(org.apache.cassandra.locator.AbstractReplicationStrategy,org.apache.cassandra.dht.AbstractBounds)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,forRangeReadLiveSorted,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/ReplicaLayout.java,ReplicaLayout,../data/xml/cassandra/ReplicaLayout.xml,"/**
     * TODO: we should really double check that the provided range does not overlap multiple token ring regions
     * @return the read layout for a range - this includes only live natural replicas, i.e. those that are not pending
     * and not marked down by the failure detector. these are reverse sorted by the badness score of the configured snitch
     */
static ReplicaLayout.ForRangeRead forRangeReadLiveSorted(AbstractReplicationStrategy replicationStrategy, AbstractBounds<PartitionPosition> range)
    {
        EndpointsForRange replicas = replicationStrategy.getNaturalReplicas(range.right);
        replicas = DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(), replicas);
        replicas = replicas.filter(FailureDetector.isReplicaAlive);
        return new ReplicaLayout.ForRangeRead(replicationStrategy, range, replicas);
    }

}"
"M:org.apache.cassandra.locator.ReplicaPlans$2:select(org.apache.cassandra.db.ConsistencyLevel,org.apache.cassandra.locator.ReplicaLayout$ForWrite,org.apache.cassandra.locator.ReplicaLayout$ForWrite)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,select,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/ReplicaPlans.java,ReplicaPlans$2,../data/xml/cassandra/ReplicaPlans.xml,"@Override
        public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>
        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)
        {
            return liveAndDown.all();
        }
    }
@Override
        public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>
        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)
        {
            if (!any(liveAndDown.all(), Replica::isTransient))
                return liveAndDown.all();

            ReplicaCollection.Builder<E> contacts = liveAndDown.all().newBuilder(liveAndDown.all().size());
            contacts.addAll(filter(liveAndDown.natural(), Replica::isFull));
            contacts.addAll(liveAndDown.pending());

            /**
             * Per CASSANDRA-14768, we ensure we write to at least a QUORUM of nodes in every DC,
             * regardless of how many responses we need to wait for and our requested consistencyLevel.
             * This is to minimally surprise users with transient replication; with normal writes, we
             * soft-ensure that we reach QUORUM in all DCs we are able to, by writing to every node;
             * even if we don't wait for ACK, we have in both cases sent sufficient messages.
              */
            ObjectIntHashMap<String> requiredPerDc = eachQuorumForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending());
            addToCountPerDc(requiredPerDc, live.natural().filter(Replica::isFull), -1);
            addToCountPerDc(requiredPerDc, live.pending(), -1);

            IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
            for (Replica replica : filter(live.natural(), Replica::isTransient))
            {
                String dc = snitch.getDatacenter(replica);
                if (requiredPerDc.addTo(dc, -1) >= 0)
                    contacts.add(replica);
            }
            return contacts.build();
        }
    }
@Override
            public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>
            E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)
            {
                assert !any(liveAndDown.all(), Replica::isTransient);

                ReplicaCollection.Builder<E> contacts = live.all().newBuilder(live.all().size());
                // add all live nodes we might write to that we have already contacted on read
                contacts.addAll(filter(live.all(), r -> readPlan.contacts().endpoints().contains(r.endpoint())));

                // finally, add sufficient nodes to achieve our consistency level
                if (consistencyLevel != EACH_QUORUM)
                {
                    int add = consistencyLevel.blockForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending()) - contacts.size();
                    if (add > 0)
                    {
                        for (Replica replica : filter(live.all(), r -> !contacts.contains(r)))
                        {
                            contacts.add(replica);
                            if (--add == 0)
                                break;
                        }
                    }
                }
                "
"M:org.apache.cassandra.locator.ReplicaPlans$3:select(org.apache.cassandra.db.ConsistencyLevel,org.apache.cassandra.locator.ReplicaLayout$ForWrite,org.apache.cassandra.locator.ReplicaLayout$ForWrite)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,select,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/ReplicaPlans.java,ReplicaPlans$3,../data/xml/cassandra/ReplicaPlans.xml,"@Override
        public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>
        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)
        {
            return liveAndDown.all();
        }
    }
@Override
        public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>
        E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)
        {
            if (!any(liveAndDown.all(), Replica::isTransient))
                return liveAndDown.all();

            ReplicaCollection.Builder<E> contacts = liveAndDown.all().newBuilder(liveAndDown.all().size());
            contacts.addAll(filter(liveAndDown.natural(), Replica::isFull));
            contacts.addAll(liveAndDown.pending());

            /**
             * Per CASSANDRA-14768, we ensure we write to at least a QUORUM of nodes in every DC,
             * regardless of how many responses we need to wait for and our requested consistencyLevel.
             * This is to minimally surprise users with transient replication; with normal writes, we
             * soft-ensure that we reach QUORUM in all DCs we are able to, by writing to every node;
             * even if we don't wait for ACK, we have in both cases sent sufficient messages.
              */
            ObjectIntHashMap<String> requiredPerDc = eachQuorumForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending());
            addToCountPerDc(requiredPerDc, live.natural().filter(Replica::isFull), -1);
            addToCountPerDc(requiredPerDc, live.pending(), -1);

            IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
            for (Replica replica : filter(live.natural(), Replica::isTransient))
            {
                String dc = snitch.getDatacenter(replica);
                if (requiredPerDc.addTo(dc, -1) >= 0)
                    contacts.add(replica);
            }
            return contacts.build();
        }
    }
@Override
            public <E extends Endpoints<E>, L extends ReplicaLayout.ForWrite<E>>
            E select(ConsistencyLevel consistencyLevel, L liveAndDown, L live)
            {
                assert !any(liveAndDown.all(), Replica::isTransient);

                ReplicaCollection.Builder<E> contacts = live.all().newBuilder(live.all().size());
                // add all live nodes we might write to that we have already contacted on read
                contacts.addAll(filter(live.all(), r -> readPlan.contacts().endpoints().contains(r.endpoint())));

                // finally, add sufficient nodes to achieve our consistency level
                if (consistencyLevel != EACH_QUORUM)
                {
                    int add = consistencyLevel.blockForWrite(liveAndDown.replicationStrategy(), liveAndDown.pending()) - contacts.size();
                    if (add > 0)
                    {
                        for (Replica replica : filter(live.all(), r -> !contacts.contains(r)))
                        {
                            contacts.add(replica);
                            if (--add == 0)
                                break;
                        }
                    }
                }
                "
M:org.apache.cassandra.locator.ReplicaPlans:forBatchlogWrite(boolean),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,forBatchlogWrite,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/ReplicaPlans.java,ReplicaPlans,../data/xml/cassandra/ReplicaPlans.xml,"/**
     * Requires that the provided endpoints are alive.  Converts them to their relevant system replicas.
     * Note that the liveAndDown collection and live are equal to the provided endpoints.
     *
     * @param isAny if batch consistency level is ANY, in which case a local node will be picked
     */
public static ReplicaPlan.ForTokenWrite forBatchlogWrite(boolean isAny) throws UnavailableException
    {
        // A single case we write not for range or token, but multiple mutations to many tokens
        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();

        TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cachedOnlyTokenMap().getTopology();
        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        Multimap<String, InetAddressAndPort> localEndpoints = HashMultimap.create(topology.getDatacenterRacks()
                                                                                          .get(snitch.getLocalDatacenter()));
        // Replicas are picked manually:
        //  - replicas should be alive according to the failure detector
        //  - replicas should be in the local datacenter
        //  - choose min(2, number of qualifying candiates above)
        //  - allow the local node to be the only replica only if it's a single-node DC
        Collection<InetAddressAndPort> chosenEndpoints = filterBatchlogEndpoints(snitch.getLocalRack(), localEndpoints);

        if (chosenEndpoints.isEmpty() && isAny)
            chosenEndpoints = Collections.singleton(FBUtilities.getBroadcastAddressAndPort());

        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);
        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(
                systemKeypsace.getReplicationStrategy(),
                SystemReplicas.getSystemReplicas(chosenEndpoints).forToken(token),
                EndpointsForToken.empty(token)
        );
        // Batchlog is hosted by either one node or two nodes from different racks.
        ConsistencyLevel consistencyLevel = liveAndDown.all().size() == 1 ? ConsistencyLevel.ONE : ConsistencyLevel.TWO;
        // assume that we have already been given live endpoints, and skip applying the failure detector
        return forWrite(systemKeypsace, consistencyLevel, liveAndDown, liveAndDown, writeAll);
    }

    "
"M:org.apache.cassandra.locator.ReplicaPlans:contactForEachQuorumRead(org.apache.cassandra.locator.NetworkTopologyStrategy,org.apache.cassandra.locator.Endpoints)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,contactForEachQuorumRead,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/ReplicaPlans.java,ReplicaPlans,../data/xml/cassandra/ReplicaPlans.xml,"private static <E extends Endpoints<E>> E contactForEachQuorumRead(NetworkTopologyStrategy replicationStrategy, E candidates)
    {
        ObjectIntHashMap<String> perDc = eachQuorumForRead(replicationStrategy);

        final IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        return candidates.filter(replica -> {
            String dc = snitch.getDatacenter(replica);
            return perDc.addTo(dc, -1) >= 0;
        });
    }

    "
"M:org.apache.cassandra.locator.ReplicaPlans:maybeMerge(org.apache.cassandra.db.Keyspace,org.apache.cassandra.db.ConsistencyLevel,org.apache.cassandra.locator.ReplicaPlan$ForRangeRead,org.apache.cassandra.locator.ReplicaPlan$ForRangeRead)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,maybeMerge,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/ReplicaPlans.java,ReplicaPlans,../data/xml/cassandra/ReplicaPlans.xml,"/**
     * Take two range read plans for adjacent ranges, and check if it is OK (and worthwhile) to combine them into a single plan
     */
public static ReplicaPlan.ForRangeRead maybeMerge(Keyspace keyspace, ConsistencyLevel consistencyLevel, ReplicaPlan.ForRangeRead left, ReplicaPlan.ForRangeRead right)
    {
        // TODO: should we be asserting that the ranges are adjacent?
        AbstractBounds<PartitionPosition> newRange = left.range().withNewRight(right.range().right);
        EndpointsForRange mergedCandidates = left.candidates().keep(right.candidates().endpoints());
        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();

        // Check if there are enough shared endpoints for the merge to be possible.
        if (!isSufficientLiveReplicasForRead(replicationStrategy, consistencyLevel, mergedCandidates))
            return null;

        EndpointsForRange contacts = contactForRead(replicationStrategy, consistencyLevel, false, mergedCandidates);

        // Estimate whether merging will be a win or not
        if (!DatabaseDescriptor.getEndpointSnitch().isWorthMergingForRangeQuery(contacts, left.contacts(), right.contacts()))
            return null;

        // If we get there, merge this range and the next one
        return new ReplicaPlan.ForRangeRead(keyspace, replicationStrategy, consistencyLevel, newRange, mergedCandidates, contacts, left.vnodeCount() + right.vnodeCount());
    }
}
"
"M:org.apache.cassandra.locator.Replicas:countPerDc(java.util.Collection,java.lang.Iterable)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,countPerDc,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/Replicas.java,Replicas,../data/xml/cassandra/Replicas.xml,"/**
     * count the number of full and transient replicas, separately, for each DC
     */
public static ObjectObjectHashMap<String, ReplicaCount> countPerDc(Collection<String> dataCenters, Iterable<Replica> replicas)
    {
        ObjectObjectHashMap<String, ReplicaCount> perDc = new ObjectObjectHashMap<>(dataCenters.size());
        for (String dc: dataCenters)
            perDc.put(dc, new ReplicaCount());

        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        for (Replica replica : replicas)
        {
            String dc = snitch.getDatacenter(replica);
            perDc.get(dc).increment(replica);
        }
        return perDc;
    }

    "
"M:org.apache.cassandra.locator.Replicas:addToCountPerDc(com.carrotsearch.hppc.ObjectIntHashMap,java.lang.Iterable,int)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,addToCountPerDc,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/Replicas.java,Replicas,../data/xml/cassandra/Replicas.xml,"/**
     * increment each of the map's DC entries for each matching replica provided
     */
public static void addToCountPerDc(ObjectIntHashMap<String> perDc, Iterable<Replica> replicas, int add)
    {
        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        for (Replica replica : replicas)
        {
            String dc = snitch.getDatacenter(replica);
            perDc.addTo(dc, add);
        }
    }

    "
M:org.apache.cassandra.locator.TokenMetadata$Topology$Builder:updateEndpoint(org.apache.cassandra.locator.InetAddressAndPort),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,updateEndpoint,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/TokenMetadata.java,TokenMetadata$Topology$Builder,../data/xml/cassandra/TokenMetadata.xml,"Builder updateEndpoint(InetAddressAndPort ep)
            {
                IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
                if (snitch == null || !currentLocations.containsKey(ep))
                    return this;

                updateEndpoint(ep, snitch);
                return this;
            }

            
private void updateEndpoint(InetAddressAndPort ep, IEndpointSnitch snitch)
            {
                Pair<String, String> current = currentLocations.get(ep);
                String dc = snitch.getDatacenter(ep);
                String rack = snitch.getRack(ep);
                if (dc.equals(current.left) && rack.equals(current.right))
                    return;

                doRemoveEndpoint(ep, current);
                doAddEndpoint(ep, dc, rack);
            }

            "
M:org.apache.cassandra.locator.TokenMetadata$Topology$Builder:updateEndpoints(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,updateEndpoints,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/TokenMetadata.java,TokenMetadata$Topology$Builder,../data/xml/cassandra/TokenMetadata.xml,"Builder updateEndpoints()
            {
                IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
                if (snitch == null)
                    return this;

                for (InetAddressAndPort ep : currentLocations.keySet())
                    updateEndpoint(ep, snitch);

                return this;
            }

            "
M:org.apache.cassandra.locator.TokenMetadata$Topology:lambda$empty$0(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,lambda$empty$0,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/locator/TokenMetadata.java,TokenMetadata$Topology,../data/xml/cassandra/TokenMetadata.xml,"public class TokenMetadata
{
    private static final Logger logger = LoggerFactory.getLogger(TokenMetadata.class);

    /**
     * Maintains token to endpoint map of every node in the cluster.
     * Each Token is associated with exactly one Address, but each Address may have
     * multiple tokens.  Hence, the BiMultiValMap collection.
     */
    private final BiMultiValMap<Token, InetAddressAndPort> tokenToEndpointMap;

    /** Maintains endpoint to host ID map of every node in the cluster */
    private final BiMap<InetAddressAndPort, UUID> endpointToHostIdMap;

    // Prior to CASSANDRA-603, we just had <tt>Map<Range, InetAddressAndPort> pendingRanges<tt>,
    // which was added to when a node began bootstrap and removed from when it finished.
    //
    // This is inadequate when multiple changes are allowed simultaneously.  For example,
    // suppose that there is a ring of nodes A, C and E, with replication factor 3.
    // Node D bootstraps between C and E, so its pending ranges will be E-A, A-C and C-D.
    // Now suppose node B bootstraps between A and C at the same time. Its pending ranges
    // would be C-E, E-A and A-B. Now both nodes need to be assigned pending range E-A,
    // which we would be unable to represent with the old Map.  The same thing happens
    // even more obviously for any nodes that boot simultaneously between same two nodes.
    //
    // So, we made two changes:
    //
    // First, we changed pendingRanges to a <tt>Multimap<Range, InetAddressAndPort></tt> (now
    // <tt>Map<String, Multimap<Range, InetAddressAndPort>></tt>, because replication strategy
    // and options are per-KeySpace).
    //
    // Second, we added the bootstrapTokens and leavingEndpoints collections, so we can
    // rebuild pendingRanges from the complete information of what is going on, when
    // additional changes are made mid-operation.
    //
    // Finally, note that recording the tokens of joining nodes in bootstrapTokens also
    // means we can detect and reject the addition of multiple nodes at the same token
    // before one becomes part of the ring.
    private final BiMultiValMap<Token, InetAddressAndPort> bootstrapTokens = new BiMultiValMap<>();

    private final BiMap<InetAddressAndPort, InetAddressAndPort> replacementToOriginal = HashBiMap.create();

    // (don't need to record Token here since it's still part of tokenToEndpointMap until it's done leaving)
    private final Set<InetAddressAndPort> leavingEndpoints = new HashSet<>();
    // this is a cache of the calculation from {tokenToEndpointMap, bootstrapTokens, leavingEndpoints}
    // NOTE: this may contain ranges that conflict with the those implied by sortedTokens when a range is changing its transient status
    private final ConcurrentMap<String, PendingRangeMaps> pendingRanges = new ConcurrentHashMap<String, PendingRangeMaps>();

    // nodes which are migrating to the new tokens in the ring
    private final Set<Pair<Token, InetAddressAndPort>> movingEndpoints = new HashSet<>();

    /* Use this lock for manipulating the token map */
    private final ReadWriteLock lock = new ReentrantReadWriteLock(true);
    private volatile ArrayList<Token> sortedTokens; // safe to be read without a lock, as it's never mutated

    private volatile Topology topology;

    public final IPartitioner partitioner;

    // signals replication strategies that nodes have joined or left the ring and they need to recompute ownership
    @GuardedBy(""lock"")
    private long ringVersion = 0;

    public TokenMetadata()
    {
        this(SortedBiMultiValMap.create(),
             HashBiMap.create(),
             Topology.empty(),
             DatabaseDescriptor.getPartitioner());
    }

    public TokenMetadata(IEndpointSnitch snitch)
    {
        this(SortedBiMultiValMap.create(),
             HashBiMap.create(),
             Topology.builder(() -> snitch).build(),
             DatabaseDescriptor.getPartitioner());
    }

    private TokenMetadata(BiMultiValMap<Token, InetAddressAndPort> tokenToEndpointMap, BiMap<InetAddressAndPort, UUID> endpointsMap, Topology topology, IPartitioner partitioner)
    {
        this(tokenToEndpointMap, endpointsMap, topology, partitioner, 0);
    }

    private TokenMetadata(BiMultiValMap<Token, InetAddressAndPort> tokenToEndpointMap, BiMap<InetAddressAndPort, UUID> endpointsMap, Topology topology, IPartitioner partitioner, long ringVersion)
    {
        this.tokenToEndpointMap = tokenToEndpointMap;
        this.topology = topology;
        this.partitioner = partitioner;
        endpointToHostIdMap = endpointsMap;
        sortedTokens = sortTokens();
        this.ringVersion = ringVersion;
    }

    /**
     * To be used by tests only (via {@link org.apache.cassandra.service.StorageService#setPartitionerUnsafe}).
     */
    @VisibleForTesting
    public TokenMetadata cloneWithNewPartitioner(IPartitioner newPartitioner)
    {
        return new TokenMetadata(tokenToEndpointMap, endpointToHostIdMap, topology, newPartitioner);
    }

    private ArrayList<Token> sortTokens()
    {
        return new ArrayList<>(tokenToEndpointMap.keySet());
    }

    /** @return the number of nodes bootstrapping into source's primary range */
    public int pendingRangeChanges(InetAddressAndPort source)
    {
        int n = 0;
        Collection<Range<Token>> sourceRanges = getPrimaryRangesFor(getTokens(source));
        lock.readLock().lock();
        try
        {
            for (Token token : bootstrapTokens.keySet())
                for (Range<Token> range : sourceRanges)
                    if (range.contains(token))
                        n++;
        }
        finally
        {
            lock.readLock().unlock();
        }
        return n;
    }

    /**
     * Update token map with a single token/endpoint pair in normal state.
     */
    public void updateNormalToken(Token token, InetAddressAndPort endpoint)
    {
        updateNormalTokens(Collections.singleton(token), endpoint);
    }

    public void updateNormalTokens(Collection<Token> tokens, InetAddressAndPort endpoint)
    {
        Multimap<InetAddressAndPort, Token> endpointTokens = HashMultimap.create();
        for (Token token : tokens)
            endpointTokens.put(endpoint, token);
        updateNormalTokens(endpointTokens);
    }

    /**
     * Update token map with a set of token/endpoint pairs in normal state.
     *
     * Prefer this whenever there are multiple pairs to update, as each update (whether a single or multiple)
     * is expensive (CASSANDRA-3831).
     */
    public void updateNormalTokens(Multimap<InetAddressAndPort, Token> endpointTokens)
    {
        if (endpointTokens.isEmpty())
            return;

        lock.writeLock().lock();
        try
        {
            boolean shouldSortTokens = false;
            Topology.Builder topologyBuilder = topology.unbuild();
            for (InetAddressAndPort endpoint : endpointTokens.keySet())
            {
                Collection<Token> tokens = endpointTokens.get(endpoint);

                assert tokens != null && !tokens.isEmpty();

                bootstrapTokens.removeValue(endpoint);
                tokenToEndpointMap.removeValue(endpoint);
                topologyBuilder.addEndpoint(endpoint);
                leavingEndpoints.remove(endpoint);
                replacementToOriginal.remove(endpoint);
                removeFromMoving(endpoint); // also removing this endpoint from moving

                for (Token token : tokens)
                {
                    InetAddressAndPort prev = tokenToEndpointMap.put(token, endpoint);
                    if (!endpoint.equals(prev))
                    {
                        if (prev != null)
                            logger.warn(""Token {} changing ownership from {} to {}"", token, prev, endpoint);
                        shouldSortTokens = true;
                    }
                }
            }
            topology = topologyBuilder.build();

            if (shouldSortTokens)
                sortedTokens = sortTokens();
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    /**
     * Store an end-point to host ID mapping.  Each ID must be unique, and
     * cannot be changed after the fact.
     */
    public void updateHostId(UUID hostId, InetAddressAndPort endpoint)
    {
        assert hostId != null;
        assert endpoint != null;

        lock.writeLock().lock();
        try
        {
            InetAddressAndPort storedEp = endpointToHostIdMap.inverse().get(hostId);
            if (storedEp != null)
            {
                if (!storedEp.equals(endpoint) && (FailureDetector.instance.isAlive(storedEp)))
                {
                    throw new RuntimeException(String.format(""Host ID collision between active endpoint %s and %s (id=%s)"",
                                                             storedEp,
                                                             endpoint,
                                                             hostId));
                }
            }

            UUID storedId = endpointToHostIdMap.get(endpoint);
            if ((storedId != null) && (!storedId.equals(hostId)))
                logger.warn(""Changing {}'s host ID from {} to {}"", endpoint, storedId, hostId);

            endpointToHostIdMap.forcePut(endpoint, hostId);
        }
        finally
        {
            lock.writeLock().unlock();
        }

    }

    /** Return the unique host ID for an end-point. */
    public UUID getHostId(InetAddressAndPort endpoint)
    {
        lock.readLock().lock();
        try
        {
            return endpointToHostIdMap.get(endpoint);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    /** Return the end-point for a unique host ID */
    public InetAddressAndPort getEndpointForHostId(UUID hostId)
    {
        lock.readLock().lock();
        try
        {
            return endpointToHostIdMap.inverse().get(hostId);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    /** @return a copy of the endpoint-to-id map for read-only operations */
    public Map<InetAddressAndPort, UUID> getEndpointToHostIdMapForReading()
    {
        lock.readLock().lock();
        try
        {
            Map<InetAddressAndPort, UUID> readMap = new HashMap<>();
            readMap.putAll(endpointToHostIdMap);
            return readMap;
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    @Deprecated
    public void addBootstrapToken(Token token, InetAddressAndPort endpoint)
    {
        addBootstrapTokens(Collections.singleton(token), endpoint);
    }

    public void addBootstrapTokens(Collection<Token> tokens, InetAddressAndPort endpoint)
    {
        addBootstrapTokens(tokens, endpoint, null);
    }

    private void addBootstrapTokens(Collection<Token> tokens, InetAddressAndPort endpoint, InetAddressAndPort original)
    {
        assert tokens != null && !tokens.isEmpty();
        assert endpoint != null;

        lock.writeLock().lock();
        try
        {

            InetAddressAndPort oldEndpoint;

            for (Token token : tokens)
            {
                oldEndpoint = bootstrapTokens.get(token);
                if (oldEndpoint != null && !oldEndpoint.equals(endpoint))
                    throw new RuntimeException(""Bootstrap Token collision between "" + oldEndpoint + "" and "" + endpoint + "" (token "" + token);

                oldEndpoint = tokenToEndpointMap.get(token);
                if (oldEndpoint != null && !oldEndpoint.equals(endpoint) && !oldEndpoint.equals(original))
                    throw new RuntimeException(""Bootstrap Token collision between "" + oldEndpoint + "" and "" + endpoint + "" (token "" + token);
            }

            bootstrapTokens.removeValue(endpoint);

            for (Token token : tokens)
                bootstrapTokens.put(token, endpoint);
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    public void addReplaceTokens(Collection<Token> replacingTokens, InetAddressAndPort newNode, InetAddressAndPort oldNode)
    {
        assert replacingTokens != null && !replacingTokens.isEmpty();
        assert newNode != null && oldNode != null;

        lock.writeLock().lock();
        try
        {
            Collection<Token> oldNodeTokens = tokenToEndpointMap.inverse().get(oldNode);
            if (!replacingTokens.containsAll(oldNodeTokens) || !oldNodeTokens.containsAll(replacingTokens))
            {
                throw new RuntimeException(String.format(""Node %s is trying to replace node %s with tokens %s with a "" +
                                                         ""different set of tokens %s."", newNode, oldNode, oldNodeTokens,
                                                         replacingTokens));
            }

            logger.debug(""Replacing {} with {}"", newNode, oldNode);
            replacementToOriginal.put(newNode, oldNode);

            addBootstrapTokens(replacingTokens, newNode, oldNode);
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    public Optional<InetAddressAndPort> getReplacementNode(InetAddressAndPort endpoint)
    {
        lock.readLock().lock();
        try
        {
            return Optional.ofNullable(replacementToOriginal.inverse().get(endpoint));
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public Optional<InetAddressAndPort> getReplacingNode(InetAddressAndPort endpoint)
    {
        lock.readLock().lock();
        try
        {
            return Optional.ofNullable((replacementToOriginal.get(endpoint)));
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public void removeBootstrapTokens(Collection<Token> tokens)
    {
        assert tokens != null && !tokens.isEmpty();

        lock.writeLock().lock();
        try
        {
            for (Token token : tokens)
                bootstrapTokens.remove(token);
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    public void addLeavingEndpoint(InetAddressAndPort endpoint)
    {
        assert endpoint != null;

        lock.writeLock().lock();
        try
        {
            leavingEndpoints.add(endpoint);
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    /**
     * Add a new moving endpoint
     * @param token token which is node moving to
     * @param endpoint address of the moving node
     */
    public void addMovingEndpoint(Token token, InetAddressAndPort endpoint)
    {
        assert endpoint != null;

        lock.writeLock().lock();
        try
        {
            movingEndpoints.add(Pair.create(token, endpoint));
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    public void removeEndpoint(InetAddressAndPort endpoint)
    {
        assert endpoint != null;

        lock.writeLock().lock();
        try
        {
            bootstrapTokens.removeValue(endpoint);
            tokenToEndpointMap.removeValue(endpoint);
            topology = topology.unbuild().removeEndpoint(endpoint).build();
            leavingEndpoints.remove(endpoint);
            if (replacementToOriginal.remove(endpoint) != null)
            {
                logger.debug(""Node {} failed during replace."", endpoint);
            }
            endpointToHostIdMap.remove(endpoint);
            sortedTokens = sortTokens();
            invalidateCachedRingsUnsafe();
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    /**
     * This is called when the snitch properties for this endpoint are updated, see CASSANDRA-10238.
     */
    public Topology updateTopology(InetAddressAndPort endpoint)
    {
        assert endpoint != null;

        lock.writeLock().lock();
        try
        {
            logger.info(""Updating topology for {}"", endpoint);
            topology = topology.unbuild().updateEndpoint(endpoint).build();
            invalidateCachedRingsUnsafe();
            return topology;
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    /**
     * This is called when the snitch properties for many endpoints are updated, it will update
     * the topology mappings of any endpoints whose snitch has changed, see CASSANDRA-10238.
     */
    public Topology updateTopology()
    {
        lock.writeLock().lock();
        try
        {
            logger.info(""Updating topology for all endpoints that have changed"");
            topology = topology.unbuild().updateEndpoints().build();
            invalidateCachedRingsUnsafe();
            return topology;
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    /**
     * Remove pair of token/address from moving endpoints
     * @param endpoint address of the moving node
     */
    public void removeFromMoving(InetAddressAndPort endpoint)
    {
        assert endpoint != null;

        lock.writeLock().lock();
        try
        {
            for (Pair<Token, InetAddressAndPort> pair : movingEndpoints)
            {
                if (pair.right.equals(endpoint))
                {
                    movingEndpoints.remove(pair);
                    break;
                }
            }

            invalidateCachedRingsUnsafe();
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    public Collection<Token> getTokens(InetAddressAndPort endpoint)
    {
        assert endpoint != null;

        lock.readLock().lock();
        try
        {
            assert isMember(endpoint); // don't want to return nulls
            return new ArrayList<>(tokenToEndpointMap.inverse().get(endpoint));
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    @Deprecated
    public Token getToken(InetAddressAndPort endpoint)
    {
        return getTokens(endpoint).iterator().next();
    }

    public boolean isMember(InetAddressAndPort endpoint)
    {
        assert endpoint != null;

        lock.readLock().lock();
        try
        {
            return tokenToEndpointMap.inverse().containsKey(endpoint);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public boolean isLeaving(InetAddressAndPort endpoint)
    {
        assert endpoint != null;

        lock.readLock().lock();
        try
        {
            return leavingEndpoints.contains(endpoint);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public boolean isMoving(InetAddressAndPort endpoint)
    {
        assert endpoint != null;

        lock.readLock().lock();
        try
        {
            for (Pair<Token, InetAddressAndPort> pair : movingEndpoints)
            {
                if (pair.right.equals(endpoint))
                    return true;
            }

            return false;
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    private final AtomicReference<TokenMetadata> cachedTokenMap = new AtomicReference<>();

    /**
     * Create a copy of TokenMetadata with only tokenToEndpointMap. That is, pending ranges,
     * bootstrap tokens and leaving endpoints are not included in the copy.
     */
    public TokenMetadata cloneOnlyTokenMap()
    {
        lock.readLock().lock();
        try
        {
            return new TokenMetadata(SortedBiMultiValMap.create(tokenToEndpointMap),
                                     HashBiMap.create(endpointToHostIdMap),
                                     topology,
                                     partitioner,
                                     ringVersion);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    /**
     * Return a cached TokenMetadata with only tokenToEndpointMap, i.e., the same as cloneOnlyTokenMap but
     * uses a cached copy that is invalided when the ring changes, so in the common case
     * no extra locking is required.
     *
     * Callers must *NOT* mutate the returned metadata object.
     */
    public TokenMetadata cachedOnlyTokenMap()
    {
        TokenMetadata tm = cachedTokenMap.get();
        if (tm != null)
            return tm;

        // synchronize to prevent thundering herd (CASSANDRA-6345)
        synchronized (this)
        {
            if ((tm = cachedTokenMap.get()) != null)
                return tm;

            tm = cloneOnlyTokenMap();
            cachedTokenMap.set(tm);
            return tm;
        }
    }

    /**
     * Create a copy of TokenMetadata with tokenToEndpointMap reflecting situation after all
     * current leave operations have finished.
     *
     * @return new token metadata
     */
    public TokenMetadata cloneAfterAllLeft()
    {
        lock.readLock().lock();
        try
        {
            return removeEndpoints(cloneOnlyTokenMap(), leavingEndpoints);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    private static TokenMetadata removeEndpoints(TokenMetadata allLeftMetadata, Set<InetAddressAndPort> leavingEndpoints)
    {
        for (InetAddressAndPort endpoint : leavingEndpoints)
            allLeftMetadata.removeEndpoint(endpoint);

        return allLeftMetadata;
    }

    /**
     * Create a copy of TokenMetadata with tokenToEndpointMap reflecting situation after all
     * current leave, and move operations have finished.
     *
     * @return new token metadata
     */
    public TokenMetadata cloneAfterAllSettled()
    {
        lock.readLock().lock();
        try
        {
            TokenMetadata metadata = cloneOnlyTokenMap();

            for (InetAddressAndPort endpoint : leavingEndpoints)
                metadata.removeEndpoint(endpoint);


            for (Pair<Token, InetAddressAndPort> pair : movingEndpoints)
                metadata.updateNormalToken(pair.left, pair.right);

            return metadata;
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public InetAddressAndPort getEndpoint(Token token)
    {
        lock.readLock().lock();
        try
        {
            return tokenToEndpointMap.get(token);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public Collection<Range<Token>> getPrimaryRangesFor(Collection<Token> tokens)
    {
        Collection<Range<Token>> ranges = new ArrayList<>(tokens.size());
        for (Token right : tokens)
            ranges.add(new Range<>(getPredecessor(right), right));
        return ranges;
    }

    @Deprecated
    public Range<Token> getPrimaryRangeFor(Token right)
    {
        return getPrimaryRangesFor(Arrays.asList(right)).iterator().next();
    }

    public ArrayList<Token> sortedTokens()
    {
        return sortedTokens;
    }

    public EndpointsByRange getPendingRangesMM(String keyspaceName)
    {
        EndpointsByRange.Builder byRange = new EndpointsByRange.Builder();
        PendingRangeMaps pendingRangeMaps = this.pendingRanges.get(keyspaceName);

        if (pendingRangeMaps != null)
        {
            for (Map.Entry<Range<Token>, EndpointsForRange.Builder> entry : pendingRangeMaps)
            {
                byRange.putAll(entry.getKey(), entry.getValue(), Conflict.ALL);
            }
        }

        return byRange.build();
    }

    /** a mutable map may be returned but caller should not modify it */
    public PendingRangeMaps getPendingRanges(String keyspaceName)
    {
        return this.pendingRanges.get(keyspaceName);
    }

    public RangesAtEndpoint getPendingRanges(String keyspaceName, InetAddressAndPort endpoint)
    {
        RangesAtEndpoint.Builder builder = RangesAtEndpoint.builder(endpoint);
        for (Map.Entry<Range<Token>, Replica> entry : getPendingRangesMM(keyspaceName).flattenEntries())
        {
            Replica replica = entry.getValue();
            if (replica.endpoint().equals(endpoint))
            {
                builder.add(replica, Conflict.DUPLICATE);
            }
        }
        return builder.build();
    }

     /**
     * Calculate pending ranges according to bootsrapping and leaving nodes. Reasoning is:
     *
     * (1) When in doubt, it is better to write too much to a node than too little. That is, if
     * there are multiple nodes moving, calculate the biggest ranges a node could have. Cleaning
     * up unneeded data afterwards is better than missing writes during movement.
     * (2) When a node leaves, ranges for other nodes can only grow (a node might get additional
     * ranges, but it will not lose any of its current ranges as a result of a leave). Therefore
     * we will first remove _all_ leaving tokens for the sake of calculation and then check what
     * ranges would go where if all nodes are to leave. This way we get the biggest possible
     * ranges with regard current leave operations, covering all subsets of possible final range
     * values.
     * (3) When a node bootstraps, ranges of other nodes can only get smaller. Without doing
     * complex calculations to see if multiple bootstraps overlap, we simply base calculations
     * on the same token ring used before (reflecting situation after all leave operations have
     * completed). Bootstrapping nodes will be added and removed one by one to that metadata and
     * checked what their ranges would be. This will give us the biggest possible ranges the
     * node could have. It might be that other bootstraps make our actual final ranges smaller,
     * but it does not matter as we can clean up the data afterwards.
     *
     * NOTE: This is heavy and ineffective operation. This will be done only once when a node
     * changes state in the cluster, so it should be manageable.
     */
    public void calculatePendingRanges(AbstractReplicationStrategy strategy, String keyspaceName)
    {
        // avoid race between both branches - do not use a lock here as this will block any other unrelated operations!
        long startedAt = System.currentTimeMillis();
        synchronized (pendingRanges)
        {
            TokenMetadataDiagnostics.pendingRangeCalculationStarted(this, keyspaceName);

            // create clone of current state
            BiMultiValMap<Token, InetAddressAndPort> bootstrapTokensClone;
            Set<InetAddressAndPort> leavingEndpointsClone;
            Set<Pair<Token, InetAddressAndPort>> movingEndpointsClone;
            TokenMetadata metadata;

            lock.readLock().lock();
            try
            {

                if (bootstrapTokens.isEmpty() && leavingEndpoints.isEmpty() && movingEndpoints.isEmpty())
                {
                    if (logger.isTraceEnabled())
                        logger.trace(""No bootstrapping, leaving or moving nodes -> empty pending ranges for {}"", keyspaceName);
                    if (bootstrapTokens.isEmpty() && leavingEndpoints.isEmpty() && movingEndpoints.isEmpty())
                    {
                        if (logger.isTraceEnabled())
                            logger.trace(""No bootstrapping, leaving or moving nodes -> empty pending ranges for {}"", keyspaceName);
                        pendingRanges.put(keyspaceName, new PendingRangeMaps());

                        return;
                    }
                }

                bootstrapTokensClone  = new BiMultiValMap<>(this.bootstrapTokens);
                leavingEndpointsClone = new HashSet<>(this.leavingEndpoints);
                movingEndpointsClone = new HashSet<>(this.movingEndpoints);
                metadata = this.cloneOnlyTokenMap();
            }
            finally
            {
                lock.readLock().unlock();
            }

            pendingRanges.put(keyspaceName, calculatePendingRanges(strategy, metadata, bootstrapTokensClone,
                                                                   leavingEndpointsClone, movingEndpointsClone));
            if (logger.isDebugEnabled())
                logger.debug(""Starting pending range calculation for {}"", keyspaceName);

            long took = System.currentTimeMillis() - startedAt;

            if (logger.isDebugEnabled())
                logger.debug(""Pending range calculation for {} completed (took: {}ms)"", keyspaceName, took);
            if (logger.isTraceEnabled())
                logger.trace(""Calculated pending ranges for {}:\n{}"", keyspaceName, (pendingRanges.isEmpty() ? ""<empty>"" : printPendingRanges()));
        }
    }

    /**
     * @see TokenMetadata#calculatePendingRanges(AbstractReplicationStrategy, String)
     */
    private static PendingRangeMaps calculatePendingRanges(AbstractReplicationStrategy strategy,
                                                           TokenMetadata metadata,
                                                           BiMultiValMap<Token, InetAddressAndPort> bootstrapTokens,
                                                           Set<InetAddressAndPort> leavingEndpoints,
                                                           Set<Pair<Token, InetAddressAndPort>> movingEndpoints)
    {
        PendingRangeMaps newPendingRanges = new PendingRangeMaps();

        RangesByEndpoint addressRanges = strategy.getAddressReplicas(metadata);

        // Copy of metadata reflecting the situation after all leave operations are finished.
        TokenMetadata allLeftMetadata = removeEndpoints(metadata.cloneOnlyTokenMap(), leavingEndpoints);

        // get all ranges that will be affected by leaving nodes
        Set<Range<Token>> removeAffectedRanges = new HashSet<>();
        for (InetAddressAndPort endpoint : leavingEndpoints)
            removeAffectedRanges.addAll(addressRanges.get(endpoint).ranges());

        // for each of those ranges, find what new nodes will be responsible for the range when
        // all leaving nodes are gone.
        for (Range<Token> range : removeAffectedRanges)
        {
            EndpointsForRange currentReplicas = strategy.calculateNaturalReplicas(range.right, metadata);
            EndpointsForRange newReplicas = strategy.calculateNaturalReplicas(range.right, allLeftMetadata);
            for (Replica newReplica : newReplicas)
            {
                if (currentReplicas.endpoints().contains(newReplica.endpoint()))
                    continue;

                // we calculate pending replicas for leave- and move- affected ranges in the same way to avoid
                // a possible conflict when 2 pending replicas have the same endpoint and different ranges.
                for (Replica pendingReplica : newReplica.subtractSameReplication(addressRanges.get(newReplica.endpoint())))
                    newPendingRanges.addPendingRange(range, pendingReplica);
            }
        }

        // At this stage newPendingRanges has been updated according to leave operations. We can
        // now continue the calculation by checking bootstrapping nodes.

        // For each of the bootstrapping nodes, simply add to the allLeftMetadata and check what their
        // ranges would be. We actually need to clone allLeftMetadata each time as resetting its state
        // after getting the new pending ranges is not as simple as just removing the bootstrapping
        // endpoint. If the bootstrapping endpoint constitutes a replacement, removing it after checking
        // the newly pending ranges means there are now fewer endpoints that there were originally and
        // causes its next neighbour to take over its primary range which affects the next RF endpoints
        // in the ring.
        Multimap<InetAddressAndPort, Token> bootstrapAddresses = bootstrapTokens.inverse();
        for (InetAddressAndPort endpoint : bootstrapAddresses.keySet())
        {
            Collection<Token> tokens = bootstrapAddresses.get(endpoint);
            TokenMetadata cloned = allLeftMetadata.cloneOnlyTokenMap();
            cloned.updateNormalTokens(tokens, endpoint);
            for (Replica replica : strategy.getAddressReplicas(cloned, endpoint))
            {
                newPendingRanges.addPendingRange(replica.range(), replica);
            }
        }

        // At this stage newPendingRanges has been updated according to leaving and bootstrapping nodes.
        // We can now finish the calculation by checking moving nodes.

        // For each of the moving nodes, we do the same thing we did for bootstrapping:
        // simply add and remove them one by one to allLeftMetadata and check in between what their ranges would be.
        for (Pair<Token, InetAddressAndPort> moving : movingEndpoints)
        {
            //Calculate all the ranges which will could be affected. This will include the ranges before and after the move.
            Set<Replica> moveAffectedReplicas = new HashSet<>();
            InetAddressAndPort endpoint = moving.right; // address of the moving node
            //Add ranges before the move
            for (Replica replica : strategy.getAddressReplicas(allLeftMetadata, endpoint))
            {
                moveAffectedReplicas.add(replica);
            }

            allLeftMetadata.updateNormalToken(moving.left, endpoint);
            //Add ranges after the move
            for (Replica replica : strategy.getAddressReplicas(allLeftMetadata, endpoint))
            {
                moveAffectedReplicas.add(replica);
            }

            for (Replica replica : moveAffectedReplicas)
            {
                Set<InetAddressAndPort> currentEndpoints = strategy.calculateNaturalReplicas(replica.range().right, metadata).endpoints();
                Set<InetAddressAndPort> newEndpoints = strategy.calculateNaturalReplicas(replica.range().right, allLeftMetadata).endpoints();
                Set<InetAddressAndPort> difference = Sets.difference(newEndpoints, currentEndpoints);
                for (final InetAddressAndPort address : difference)
                {
                    RangesAtEndpoint newReplicas = strategy.getAddressReplicas(allLeftMetadata, address);
                    RangesAtEndpoint oldReplicas = strategy.getAddressReplicas(metadata, address);

                    // Filter out the things that are already replicated
                    newReplicas = newReplicas.filter(r -> !oldReplicas.contains(r));
                    for (Replica newReplica : newReplicas)
                    {
                        // for correctness on write, we need to treat ranges that are becoming full differently
                        // to those that are presently transient; however reads must continue to use the current view
                        // for ranges that are becoming transient. We could choose to ignore them here, but it's probably
                        // cleaner to ensure this is dealt with at point of use, where we can make a conscious decision
                        // about which to use
                        for (Replica pendingReplica : newReplica.subtractSameReplication(oldReplicas))
                        {
                            newPendingRanges.addPendingRange(pendingReplica.range(), pendingReplica);
                        }
                    }
                }
            }

            allLeftMetadata.removeEndpoint(endpoint);
        }

        return newPendingRanges;
    }

    public Token getPredecessor(Token token)
    {
        List<Token> tokens = sortedTokens();
        int index = Collections.binarySearch(tokens, token);
        assert index >= 0 : token + "" not found in "" + tokenToEndpointMapKeysAsStrings();
        return index == 0 ? tokens.get(tokens.size() - 1) : tokens.get(index - 1);
    }

    public Token getSuccessor(Token token)
    {
        List<Token> tokens = sortedTokens();
        int index = Collections.binarySearch(tokens, token);
        assert index >= 0 : token + "" not found in "" + tokenToEndpointMapKeysAsStrings();
        return (index == (tokens.size() - 1)) ? tokens.get(0) : tokens.get(index + 1);
    }

    private String tokenToEndpointMapKeysAsStrings()
    {
        lock.readLock().lock();
        try
        {
            return StringUtils.join(tokenToEndpointMap.keySet(), "", "");
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    /** @return a copy of the bootstrapping tokens map */
    public BiMultiValMap<Token, InetAddressAndPort> getBootstrapTokens()
    {
        lock.readLock().lock();
        try
        {
            return new BiMultiValMap<>(bootstrapTokens);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public Set<InetAddressAndPort> getAllEndpoints()
    {
        lock.readLock().lock();
        try
        {
            return ImmutableSet.copyOf(endpointToHostIdMap.keySet());
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public int getSizeOfAllEndpoints()
    {
        lock.readLock().lock();
        try
        {
            return endpointToHostIdMap.size();
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    /** caller should not modify leavingEndpoints */
    public Set<InetAddressAndPort> getLeavingEndpoints()
    {
        lock.readLock().lock();
        try
        {
            return ImmutableSet.copyOf(leavingEndpoints);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public int getSizeOfLeavingEndpoints()
    {
        lock.readLock().lock();
        try
        {
            return leavingEndpoints.size();
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    /**
     * Endpoints which are migrating to the new tokens
     * @return set of addresses of moving endpoints
     */
    public Set<Pair<Token, InetAddressAndPort>> getMovingEndpoints()
    {
        lock.readLock().lock();
        try
        {
            return ImmutableSet.copyOf(movingEndpoints);
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public int getSizeOfMovingEndpoints()
    {
        lock.readLock().lock();
        try
        {
            return movingEndpoints.size();
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public static int firstTokenIndex(final ArrayList<Token> ring, Token start, boolean insertMin)
    {
        assert ring.size() > 0;
        // insert the minimum token (at index == -1) if we were asked to include it and it isn't a member of the ring
        int i = Collections.binarySearch(ring, start);
        if (i < 0)
        {
            i = (i + 1) * (-1);
            if (i >= ring.size())
                i = insertMin ? -1 : 0;
        }
        return i;
    }

    public static Token firstToken(final ArrayList<Token> ring, Token start)
    {
        return ring.get(firstTokenIndex(ring, start, false));
    }

    /**
     * iterator over the Tokens in the given ring, starting with the token for the node owning start
     * (which does not have to be a Token in the ring)
     * @param includeMin True if the minimum token should be returned in the ring even if it has no owner.
     */
    public static Iterator<Token> ringIterator(final ArrayList<Token> ring, Token start, boolean includeMin)
    {
        if (ring.isEmpty())
            return includeMin ? Iterators.singletonIterator(start.getPartitioner().getMinimumToken())
                              : Collections.emptyIterator();

        final boolean insertMin = includeMin && !ring.get(0).isMinimum();
        final int startIndex = firstTokenIndex(ring, start, insertMin);
        return new AbstractIterator<Token>()
        {
            int j = startIndex;
            protected Token computeNext()
            {
                if (j < -1)
                    return endOfData();
                try
                {
                    // return minimum for index == -1
                    if (j == -1)
                        return start.getPartitioner().getMinimumToken();
                    // return ring token for other indexes
                    return ring.get(j);
                }
                finally
                {
                    j++;
                    if (j == ring.size())
                        j = insertMin ? -1 : 0;
                    if (j == startIndex)
                        // end iteration
                        j = -2;
                }
            }
        };
    }

    /** used by tests */
    public void clearUnsafe()
    {
        lock.writeLock().lock();
        try
        {
            tokenToEndpointMap.clear();
            endpointToHostIdMap.clear();
            bootstrapTokens.clear();
            leavingEndpoints.clear();
            pendingRanges.clear();
            movingEndpoints.clear();
            sortedTokens.clear();
            topology = Topology.empty();
            invalidateCachedRingsUnsafe();
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }

    public String toString()
    {
        StringBuilder sb = new StringBuilder();
        lock.readLock().lock();
        try
        {
            Multimap<InetAddressAndPort, Token> endpointToTokenMap = tokenToEndpointMap.inverse();
            Set<InetAddressAndPort> eps = endpointToTokenMap.keySet();

            if (!eps.isEmpty())
            {
                sb.append(""Normal Tokens:"");
                sb.append(LINE_SEPARATOR.getString());
                for (InetAddressAndPort ep : eps)
                {
                    sb.append(ep);
                    sb.append(':');
                    sb.append(endpointToTokenMap.get(ep));
                    sb.append(LINE_SEPARATOR.getString());
                }
            }

            if (!bootstrapTokens.isEmpty())
            {
                sb.append(""Bootstrapping Tokens:"" );
                sb.append(LINE_SEPARATOR.getString());
                for (Map.Entry<Token, InetAddressAndPort> entry : bootstrapTokens.entrySet())
                {
                    sb.append(entry.getValue()).append(':').append(entry.getKey());
                    sb.append(LINE_SEPARATOR.getString());
                }
            }

            if (!leavingEndpoints.isEmpty())
            {
                sb.append(""Leaving Endpoints:"");
                sb.append(LINE_SEPARATOR.getString());
                for (InetAddressAndPort ep : leavingEndpoints)
                {
                    sb.append(ep);
                    sb.append(LINE_SEPARATOR.getString());
                }
            }

            if (!pendingRanges.isEmpty())
            {
                sb.append(""Pending Ranges:"");
                sb.append(LINE_SEPARATOR.getString());
                sb.append(printPendingRanges());
            }
        }
        finally
        {
            lock.readLock().unlock();
        }

        return sb.toString();
    }

    private String printPendingRanges()
    {
        StringBuilder sb = new StringBuilder();

        for (PendingRangeMaps pendingRangeMaps : pendingRanges.values())
        {
            sb.append(pendingRangeMaps.printPendingRanges());
        }

        return sb.toString();
    }

    public EndpointsForToken pendingEndpointsForToken(Token token, String keyspaceName)
    {
        PendingRangeMaps pendingRangeMaps = this.pendingRanges.get(keyspaceName);
        if (pendingRangeMaps == null)
            return EndpointsForToken.empty(token);

        return pendingRangeMaps.pendingEndpointsFor(token);
    }

    /**
     * @deprecated retained for benefit of old tests
     */
    @Deprecated
    public EndpointsForToken getWriteEndpoints(Token token, String keyspaceName, EndpointsForToken natural)
    {
        EndpointsForToken pending = pendingEndpointsForToken(token, keyspaceName);
        return ReplicaLayout.forTokenWrite(Keyspace.open(keyspaceName).getReplicationStrategy(), natural, pending).all();
    }

    /** @return an endpoint to token multimap representation of tokenToEndpointMap (a copy) */
    public Multimap<InetAddressAndPort, Token> getEndpointToTokenMapForReading()
    {
        lock.readLock().lock();
        try
        {
            Multimap<InetAddressAndPort, Token> cloned = HashMultimap.create();
            for (Map.Entry<Token, InetAddressAndPort> entry : tokenToEndpointMap.entrySet())
                cloned.put(entry.getValue(), entry.getKey());
            return cloned;
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    /**
     * @return a (stable copy, won't be modified) Token to Endpoint map for all the normal and bootstrapping nodes
     *         in the cluster.
     */
    public Map<Token, InetAddressAndPort> getNormalAndBootstrappingTokenToEndpointMap()
    {
        lock.readLock().lock();
        try
        {
            Map<Token, InetAddressAndPort> map = new HashMap<>(tokenToEndpointMap.size() + bootstrapTokens.size());
            map.putAll(tokenToEndpointMap);
            map.putAll(bootstrapTokens);
            return map;
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    /**
     * @return a (stable copy, won't be modified) datacenter to Endpoint map for all the nodes in the cluster.
     */
    public ImmutableMultimap<String, InetAddressAndPort> getDC2AllEndpoints(IEndpointSnitch snitch)
    {
        return Multimaps.index(getAllEndpoints(), snitch::getDatacenter);
    }

    /**
     * @return the Topology map of nodes to DCs + Racks
     *
     * This is only allowed when a copy has been made of TokenMetadata, to avoid concurrent modifications
     * when Topology methods are subsequently used by the caller.
     */
    public Topology getTopology()
    {
        assert this != StorageService.instance.getTokenMetadata();
        return topology;
    }

    public long getRingVersion()
    {
        lock.readLock().lock();

        try
        {
            return ringVersion;
        }
        finally
        {
            lock.readLock().unlock();
        }
    }

    public void invalidateCachedRings()
    {   
        lock.writeLock().lock();

        try
        {   
            invalidateCachedRingsUnsafe();
        }
        finally
        {
            lock.writeLock().unlock();
        }
    }
    
    private void invalidateCachedRingsUnsafe()
    {
        ringVersion++;
        cachedTokenMap.set(null);
    }

    public DecoratedKey decorateKey(ByteBuffer key)
    {
        return partitioner.decorateKey(key);
    }

    /**
     * Tracks the assignment of racks and endpoints in each datacenter for all the ""normal"" endpoints
     * in this TokenMetadata. This allows faster calculation of endpoints in NetworkTopologyStrategy.
     */
    public static class Topology
    {
        /** multi-map of DC to endpoints in that DC */
        private final ImmutableMultimap<String, InetAddressAndPort> dcEndpoints;
        /** map of DC to multi-map of rack to endpoints in that rack */
        private final ImmutableMap<String, ImmutableMultimap<String, InetAddressAndPort>> dcRacks;
        /** reverse-lookup map for endpoint to current known dc/rack assignment */
        private final ImmutableMap<InetAddressAndPort, Pair<String, String>> currentLocations;
        private final Supplier<IEndpointSnitch> snitchSupplier;

        private Topology(Builder builder)
        {
            this.dcEndpoints = ImmutableMultimap.copyOf(builder.dcEndpoints);

            ImmutableMap.Builder<String, ImmutableMultimap<String, InetAddressAndPort>> dcRackBuilder = ImmutableMap.builder();
            for (Map.Entry<String, Multimap<String, InetAddressAndPort>> entry : builder.dcRacks.entrySet())
                dcRackBuilder.put(entry.getKey(), ImmutableMultimap.copyOf(entry.getValue()));
            this.dcRacks = dcRackBuilder.build();

            this.currentLocations = ImmutableMap.copyOf(builder.currentLocations);
            this.snitchSupplier = builder.snitchSupplier;
        }

        /**
         * @return multi-map of DC to endpoints in that DC
         */
        public Multimap<String, InetAddressAndPort> getDatacenterEndpoints()
        {
            return dcEndpoints;
        }

        /**
         * @return map of DC to multi-map of rack to endpoints in that rack
         */
        public ImmutableMap<String, ImmutableMultimap<String, InetAddressAndPort>> getDatacenterRacks()
        {
            return dcRacks;
        }

        /**
         * @return The DC and rack of the given endpoint.
         */
        public Pair<String, String> getLocation(InetAddressAndPort addr)
        {
            return currentLocations.get(addr);
        }

        Builder unbuild()
        {
            return new Builder(this);
        }

        static Builder builder(Supplier<IEndpointSnitch> snitchSupplier)
        {
            return new Builder(snitchSupplier);
        }

        static Topology empty()
        {
            return builder(() -> DatabaseDescriptor.getEndpointSnitch()).build();
        }

        private static class Builder
        {
            /** multi-map of DC to endpoints in that DC */
            private final Multimap<String, InetAddressAndPort> dcEndpoints;
            /** map of DC to multi-map of rack to endpoints in that rack */
            private final Map<String, Multimap<String, InetAddressAndPort>> dcRacks;
            /** reverse-lookup map for endpoint to current known dc/rack assignment */
            private final Map<InetAddressAndPort, Pair<String, String>> currentLocations;
            private final Supplier<IEndpointSnitch> snitchSupplier;

            Builder(Supplier<IEndpointSnitch> snitchSupplier)
            {
                this.dcEndpoints = HashMultimap.create();
                this.dcRacks = new HashMap<>();
                this.currentLocations = new HashMap<>();
                this.snitchSupplier = snitchSupplier;
            }

            Builder(Topology from)
            {
                this.dcEndpoints = HashMultimap.create(from.dcEndpoints);

                this.dcRacks = Maps.newHashMapWithExpectedSize(from.dcRacks.size());
                for (Map.Entry<String, ImmutableMultimap<String, InetAddressAndPort>> entry : from.dcRacks.entrySet())
                    dcRacks.put(entry.getKey(), HashMultimap.create(entry.getValue()));

                this.currentLocations = new HashMap<>(from.currentLocations);
                this.snitchSupplier = from.snitchSupplier;
            }

            /**
             * Stores current DC/rack assignment for ep
             */
            Builder addEndpoint(InetAddressAndPort ep)
            {
                String dc = snitchSupplier.get().getDatacenter(ep);
                String rack = snitchSupplier.get().getRack(ep);
                Pair<String, String> current = currentLocations.get(ep);
                if (current != null)
                {
                    if (current.left.equals(dc) && current.right.equals(rack))
                        return this;
                    doRemoveEndpoint(ep, current);
                }

                doAddEndpoint(ep, dc, rack);
                return this;
            }

            private void doAddEndpoint(InetAddressAndPort ep, String dc, String rack)
            {
                dcEndpoints.put(dc, ep);

                if (!dcRacks.containsKey(dc))
                    dcRacks.put(dc, HashMultimap.<String, InetAddressAndPort>create());
                dcRacks.get(dc).put(rack, ep);

                currentLocations.put(ep, Pair.create(dc, rack));
            }

            /**
             * Removes current DC/rack assignment for ep
             */
            Builder removeEndpoint(InetAddressAndPort ep)
            {
                if (!currentLocations.containsKey(ep))
                    return this;

                doRemoveEndpoint(ep, currentLocations.remove(ep));
                return this;
            }

            private void doRemoveEndpoint(InetAddressAndPort ep, Pair<String, String> current)
            {
                dcRacks.get(current.left).remove(current.right, ep);
                dcEndpoints.remove(current.left, ep);
            }

            Builder updateEndpoint(InetAddressAndPort ep)
            {
                IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
                if (snitch == null || !currentLocations.containsKey(ep))
                    return this;

                updateEndpoint(ep, snitch);
                return this;
            }

            Builder updateEndpoints()
            {
                IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
                if (snitch == null)
                    return this;

                for (InetAddressAndPort ep : currentLocations.keySet())
                    updateEndpoint(ep, snitch);

                return this;
            }

            private void updateEndpoint(InetAddressAndPort ep, IEndpointSnitch snitch)
            {
                Pair<String, String> current = currentLocations.get(ep);
                String dc = snitch.getDatacenter(ep);
                String rack = snitch.getRack(ep);
                if (dc.equals(current.left) && rack.equals(current.right))
                    return;

                doRemoveEndpoint(ep, current);
                doAddEndpoint(ep, dc, rack);
            }

            Topology build()
            {
                return new Topology(this);
            }
        }
    }
}

TokenMetadata.class
public TokenMetadata()
    {
        this(SortedBiMultiValMap.create(),
             HashBiMap.create(),
             Topology.empty(),
             DatabaseDescriptor.getPartitioner());
    }

    
public TokenMetadata(IEndpointSnitch snitch)
    {
        this(SortedBiMultiValMap.create(),
             HashBiMap.create(),
             Topology.builder(() -> snitch).build(),
             DatabaseDescriptor.getPartitioner());
    }

    
private TokenMetadata(BiMultiValMap<Token, InetAddressAndPort> tokenToEndpointMap, BiMap<InetAddressAndPort, UUID> endpointsMap, Topology topology, IPartitioner partitioner)
    {
        this(tokenToEndpointMap, endpointsMap, topology, partitioner, 0);
    }

    
private TokenMetadata(BiMultiValMap<Token, InetAddressAndPort> tokenToEndpointMap, BiMap<InetAddressAndPort, UUID> endpointsMap, Topology topology, IPartitioner partitioner, long ringVersion)
    {
        this.tokenToEndpointMap = tokenToEndpointMap;
        this.topology = topology;
        this.partitioner = partitioner;
        endpointToHostIdMap = endpointsMap;
        sortedTokens = sortTokens();
        this.ringVersion = ringVersion;
    }

    
public TokenMetadata 
TokenMetadata(tokenToEndpointMap, endpointToHostIdMap, topology, newPartitioner)
TokenMetadata>
public TokenMetadata 
TokenMetadata(SortedBiMultiValMap.create(tokenToEndpointMap),
                                     HashBiMap.create(endpointToHostIdMap),
                                     topology,
                                     partitioner,
                                     ringVersion)
public TokenMetadata 
TokenMetadata 
public TokenMetadata 
private static TokenMetadata 
TokenMetadata 
public TokenMetadata 
TokenMetadata 
TokenMetadata 
TokenMetadata 
TokenMetadata 
TokenMetadata "
M:org.apache.cassandra.metrics.MessagingMetrics:internodeLatencyRecorder(org.apache.cassandra.locator.InetAddressAndPort),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,internodeLatencyRecorder,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/metrics/MessagingMetrics.java,MessagingMetrics,../data/xml/cassandra/MessagingMetrics.xml,"public DCLatencyRecorder internodeLatencyRecorder(InetAddressAndPort from)
    {
        String dcName = DatabaseDescriptor.getEndpointSnitch().getDatacenter(from);
        DCLatencyRecorder dcUpdater = dcLatency.get(dcName);
        if (dcUpdater == null)
            dcUpdater = dcLatency.computeIfAbsent(dcName, k -> new DCLatencyRecorder(Metrics.timer(factory.createMetricName(dcName + ""-Latency"")), allLatency));
        return dcUpdater;
    }

    "
M:org.apache.cassandra.net.OutboundConnectionSettings:tcpNoDelay(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,tcpNoDelay,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/net/OutboundConnectionSettings.java,OutboundConnectionSettings,../data/xml/cassandra/OutboundConnectionSettings.xml,"public boolean tcpNoDelay()
    {
        if (tcpNoDelay != null)
            return tcpNoDelay;

        if (isInLocalDC(getEndpointSnitch(), getBroadcastAddressAndPort(), to))
            return INTRADC_TCP_NODELAY;

        return DatabaseDescriptor.getInterDCTcpNoDelay();
    }

    "
M:org.apache.cassandra.net.OutboundConnectionSettings:framing(org.apache.cassandra.net.ConnectionCategory),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,framing,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/net/OutboundConnectionSettings.java,OutboundConnectionSettings,../data/xml/cassandra/OutboundConnectionSettings.xml,"public Framing framing(ConnectionCategory category)
    {
        if (framing != null)
            return framing;

        if (category.isStreaming())
            return Framing.UNPROTECTED;

        return shouldCompressConnection(getEndpointSnitch(), getBroadcastAddressAndPort(), to)
               ? Framing.LZ4 : Framing.CRC;
    }

    "
M:org.apache.cassandra.repair.RepairJob:getDC(org.apache.cassandra.locator.InetAddressAndPort),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getDC,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/repair/RepairJob.java,RepairJob,../data/xml/cassandra/RepairJob.xml,"private String getDC(InetAddressAndPort address)
    {
        return DatabaseDescriptor.getEndpointSnitch().getDatacenter(address);
    }

    "
M:org.apache.cassandra.repair.RepairJob:sendDCAwareValidationRequest(java.util.Collection),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,sendDCAwareValidationRequest,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/repair/RepairJob.java,RepairJob,../data/xml/cassandra/RepairJob.xml,"/**
     * Creates {@link ValidationTask} and submit them to task executor so that tasks run sequentially within each dc.
     */
private ListenableFuture<List<TreeResponse>> sendDCAwareValidationRequest(Collection<InetAddressAndPort> endpoints)
    {
        String message = String.format(""Requesting merkle trees for %s (to %s)"", desc.columnFamily, endpoints);
        logger.info(""{} {}"", session.previewKind.logPrefix(desc.sessionId), message);
        Tracing.traceRepair(message);
        int nowInSec = getNowInSeconds();
        List<ListenableFuture<TreeResponse>> tasks = new ArrayList<>(endpoints.size());

        Map<String, Queue<InetAddressAndPort>> requestsByDatacenter = new HashMap<>();
        for (InetAddressAndPort endpoint : endpoints)
        {
            String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(endpoint);
            Queue<InetAddressAndPort> queue = requestsByDatacenter.get(dc);
            if (queue == null)
            {
                queue = new LinkedList<>();
                requestsByDatacenter.put(dc, queue);
            }
            queue.add(endpoint);
        }

        for (Map.Entry<String, Queue<InetAddressAndPort>> entry : requestsByDatacenter.entrySet())
        {
            Queue<InetAddressAndPort> requests = entry.getValue();
            InetAddressAndPort address = requests.poll();
            ValidationTask firstTask = new ValidationTask(desc, address, nowInSec, session.previewKind);
            logger.info(""{} Validating {}"", session.previewKind.logPrefix(session.getId()), address);
            session.trackValidationCompletion(Pair.create(desc, address), firstTask);
            tasks.add(firstTask);
            ValidationTask currentTask = firstTask;
            while (requests.size() > 0)
            {
                final InetAddressAndPort nextAddress = requests.poll();
                final ValidationTask nextTask = new ValidationTask(desc, nextAddress, nowInSec, session.previewKind);
                tasks.add(nextTask);
                Futures.addCallback(currentTask, new FutureCallback<TreeResponse>()
                {
                    public void onSuccess(TreeResponse result)
                    {
                        logger.info(""{} Validating {}"", session.previewKind.logPrefix(session.getId()), nextAddress);
                        session.trackValidationCompletion(Pair.create(desc, nextAddress), nextTask);
                        taskExecutor.execute(nextTask);
                    }

                    // failure is handled at root of job chain
                    public void onFailure(Throwable t) {}
                }, MoreExecutors.directExecutor());
                currentTask = nextTask;
            }
            // start running tasks
            taskExecutor.execute(firstTask);
        }
        return Futures.allAsList(tasks);
    }
}"
M:org.apache.cassandra.schema.KeyspaceMetadata:createReplicationStrategy(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,createReplicationStrategy,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/schema/KeyspaceMetadata.java,KeyspaceMetadata,../data/xml/cassandra/KeyspaceMetadata.xml,"public AbstractReplicationStrategy createReplicationStrategy()
    {
        return AbstractReplicationStrategy.createReplicationStrategy(name,
                                                                     params.replication.klass,
                                                                     StorageService.instance.getTokenMetadata(),
                                                                     DatabaseDescriptor.getEndpointSnitch(),
                                                                     params.replication.options);
    }

    "
M:org.apache.cassandra.schema.ReplicationParams:validate(java.lang.String),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,validate,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/schema/ReplicationParams.java,ReplicationParams,../data/xml/cassandra/ReplicationParams.xml,"public void validate(String name)
    {
        // Attempt to instantiate the ARS, which will throw a ConfigurationException if the options aren't valid.
        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
        IEndpointSnitch eps = DatabaseDescriptor.getEndpointSnitch();
        AbstractReplicationStrategy.validateReplicationStrategy(name, klass, tmd, eps, options);
    }

    "
M:org.apache.cassandra.service.CassandraDaemon:start(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,start,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/CassandraDaemon.java,CassandraDaemon,../data/xml/cassandra/CassandraDaemon.xml,"/**
     * Start the Cassandra Daemon, assuming that it has already been
     * initialized via {@link #init(String[])}
     *
     * Hook for JSVC
     */
public void start()
    {
        StartupClusterConnectivityChecker connectivityChecker = StartupClusterConnectivityChecker.create(DatabaseDescriptor.getBlockForPeersTimeoutInSeconds(),
                                                                                                         DatabaseDescriptor.getBlockForPeersInRemoteDatacenters());
        connectivityChecker.execute(Gossiper.instance.getEndpoints(), DatabaseDescriptor.getEndpointSnitch()::getDatacenter);

        // check to see if transports may start else return without starting.  This is needed when in survey mode or
        // when bootstrap has not completed.
        try
        {
            validateTransportsCanStart();
        }
        catch (IllegalStateException isx)
        {
            // If there are any errors, we just log and return in this case
            logger.warn(isx.getMessage());
            return;
        }

        startClientTransports();
    }

    "
M:org.apache.cassandra.service.DatacenterSyncWriteResponseHandler:<clinit>(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,<clinit>,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/DatacenterSyncWriteResponseHandler.java,DatacenterSyncWriteResponseHandler,../data/xml/cassandra/DatacenterSyncWriteResponseHandler.xml,not found
"M:org.apache.cassandra.service.RangeRelocator:calculateRangesToFetchWithPreferredEndpoints(org.apache.cassandra.locator.RangesAtEndpoint,org.apache.cassandra.locator.AbstractReplicationStrategy,java.lang.String,org.apache.cassandra.locator.TokenMetadata,org.apache.cassandra.locator.TokenMetadata)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,calculateRangesToFetchWithPreferredEndpoints,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/RangeRelocator.java,RangeRelocator,../data/xml/cassandra/RangeRelocator.xml,"/**
     * Wrapper that supplies accessors to the real implementations of the various dependencies for this method
     */
private static Multimap<InetAddressAndPort, RangeStreamer.FetchReplica> calculateRangesToFetchWithPreferredEndpoints(RangesAtEndpoint fetchRanges,
                                                                                                                         AbstractReplicationStrategy strategy,
                                                                                                                         String keyspace,
                                                                                                                         TokenMetadata tmdBefore,
                                                                                                                         TokenMetadata tmdAfter)
    {
        EndpointsByReplica preferredEndpoints =
        RangeStreamer.calculateRangesToFetchWithPreferredEndpoints(DatabaseDescriptor.getEndpointSnitch()::sortedByProximity,
                                                                   strategy,
                                                                   fetchRanges,
                                                                   StorageService.useStrictConsistency,
                                                                   tmdBefore,
                                                                   tmdAfter,
                                                                   keyspace,
                                                                   Arrays.asList(new RangeStreamer.FailureDetectorSourceFilter(FailureDetector.instance),
                                                                                 new RangeStreamer.ExcludeLocalNodeFilter()));
        return RangeStreamer.convertPreferredEndpointsToWorkMap(preferredEndpoints);
    }

    "
M:org.apache.cassandra.service.RangeRelocator:calculateToFromStreams(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,calculateToFromStreams,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/RangeRelocator.java,RangeRelocator,../data/xml/cassandra/RangeRelocator.xml,"public void calculateToFromStreams()
    {
        logger.debug(""Current tmd: {}, Updated tmd: {}"", tokenMetaClone, tokenMetaCloneAllSettled);

        for (String keyspace : keyspaceNames)
        {
            // replication strategy of the current keyspace
            AbstractReplicationStrategy strategy = Keyspace.open(keyspace).getReplicationStrategy();

            logger.info(""Calculating ranges to stream and request for keyspace {}"", keyspace);
            //From what I have seen we only ever call this with a single token from StorageService.move(Token)
            for (Token newToken : tokens)
            {
                Collection<Token> currentTokens = tokenMetaClone.getTokens(localAddress);
                if (currentTokens.size() > 1 || currentTokens.isEmpty())
                {
                    throw new AssertionError(""Unexpected current tokens: "" + currentTokens);
                }

                // calculated parts of the ranges to request/stream from/to nodes in the ring
                Pair<RangesAtEndpoint, RangesAtEndpoint> streamAndFetchOwnRanges;

                //In the single node token move there is nothing to do and Range subtraction is broken
                //so it's easier to just identify this case up front.
                if (tokenMetaClone.getTopology().getDatacenterEndpoints().get(DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter()).size() > 1)
                {
                    // getting collection of the currently used ranges by this keyspace
                    RangesAtEndpoint currentReplicas = strategy.getAddressReplicas(localAddress);

                    // collection of ranges which this node will serve after move to the new token
                    RangesAtEndpoint updatedReplicas = strategy.getPendingAddressRanges(tokenMetaClone, newToken, localAddress);

                    streamAndFetchOwnRanges = calculateStreamAndFetchRanges(currentReplicas, updatedReplicas);
                }
                else
                {
                     streamAndFetchOwnRanges = Pair.create(RangesAtEndpoint.empty(localAddress), RangesAtEndpoint.empty(localAddress));
                }

                RangesByEndpoint rangesToStream = calculateRangesToStreamWithEndpoints(streamAndFetchOwnRanges.left, strategy, tokenMetaClone, tokenMetaCloneAllSettled);
                logger.info(""Endpoint ranges to stream to "" + rangesToStream);

                // stream ranges
                for (InetAddressAndPort address : rangesToStream.keySet())
                {
                    logger.debug(""Will stream range {} of keyspace {} to endpoint {}"", rangesToStream.get(address), keyspace, address);
                    RangesAtEndpoint ranges = rangesToStream.get(address);
                    streamPlan.transferRanges(address, keyspace, ranges);
                }

                Multimap<InetAddressAndPort, RangeStreamer.FetchReplica> rangesToFetch = calculateRangesToFetchWithPreferredEndpoints(streamAndFetchOwnRanges.right, strategy, keyspace, tokenMetaClone, tokenMetaCloneAllSettled);

                // stream requests
                rangesToFetch.asMap().forEach((address, sourceAndOurReplicas) -> {
                    RangesAtEndpoint full = sourceAndOurReplicas.stream()
                            .filter(pair -> pair.remote.isFull())
                            .map(pair -> pair.local)
                            .collect(RangesAtEndpoint.collector(localAddress));
                    RangesAtEndpoint trans = sourceAndOurReplicas.stream()
                            .filter(pair -> pair.remote.isTransient())
                            .map(pair -> pair.local)
                            .collect(RangesAtEndpoint.collector(localAddress));
                    logger.debug(""Will request range {} of keyspace {} from endpoint {}"", rangesToFetch.get(address), keyspace, address);
                    streamPlan.requestRanges(address, keyspace, full, trans);
                });

                logger.debug(""Keyspace {}: work map {}."", keyspace, rangesToFetch);
            }
        }
    }

    "
M:org.apache.cassandra.service.StartupChecks$11:execute(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,execute,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StartupChecks.java,StartupChecks$11,../data/xml/cassandra/StartupChecks.xml,"public void execute()
        {
            if (FBUtilities.isWindows)
                return;
            String jemalloc = System.getProperty(""cassandra.libjemalloc"");
            if (jemalloc == null)
                logger.warn(""jemalloc shared library could not be preloaded to speed up memory allocations"");
            else if (""-"".equals(jemalloc))
                logger.info(""jemalloc preload explicitly disabled"");
            else
                logger.info(""jemalloc seems to be preloaded from {}"", jemalloc);
        }
    }
public void execute() throws StartupException
        {
            long now = System.currentTimeMillis();
            if (now < EARLIEST_LAUNCH_DATE)
                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE,
                                           String.format(""current machine time is %s, but that is seemingly incorrect. exiting now."",
                                                         new Date(now).toString()));
        }
    }
public void execute()
        {
            String jmxPort = System.getProperty(""cassandra.jmx.remote.port"");
            if (jmxPort == null)
            {
                logger.warn(""JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info."");
                jmxPort = System.getProperty(""cassandra.jmx.local.port"");
                if (jmxPort == null)
                    logger.error(""cassandra.jmx.local.port missing from cassandra-env.sh, unable to start local JMX service."");
            }
            else
            {
                logger.info(""JMX is enabled to receive remote connections on port: {}"", jmxPort);
            }
        }
    }
public void execute()
        {
            if (COM_SUN_MANAGEMENT_JMXREMOTE_PORT.isPresent())
            {
                logger.warn(""Use of com.sun.management.jmxremote.port at startup is deprecated. "" +
                            ""Please use cassandra.jmx.remote.port instead."");
            }
        }
    }
public void execute()
        {
            // log warnings for different kinds of sub-optimal JVMs.  tldr use 64-bit Oracle >= 1.6u32
            if (!DatabaseDescriptor.hasLargeAddressSpace())
                logger.warn(""32bit JVM detected.  It is recommended to run Cassandra on a 64bit JVM for better performance."");

            String javaVmName = JAVA_VM_NAME.getString();
            if (!(javaVmName.contains(""HotSpot"") || javaVmName.contains(""OpenJDK"")))
            {
                logger.warn(""Non-Oracle JVM detected.  Some features, such as immediate unmap of compacted SSTables, may not work as intended"");
            }
            else
            {
                checkOutOfMemoryHandling();
            }
        }

        
public void execute() throws StartupException
        {
            // Fail-fast if the native library could not be linked.
            if (!NativeLibrary.isAvailable())
                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE, ""The native library could not be initialized properly. "");
        }
    }
public void execute()
        {
            SigarLibrary.instance.warnIfRunningInDegradedMode();
        }
    }
public void execute()
        {
            if (!FBUtilities.isLinux)
                return;

            if (DatabaseDescriptor.getDiskAccessMode() == Config.DiskAccessMode.standard &&
                DatabaseDescriptor.getIndexAccessMode() == Config.DiskAccessMode.standard)
                return; // no need to check if disk access mode is only standard and not mmap

            long maxMapCount = getMaxMapCount();
            if (maxMapCount < EXPECTED_MAX_MAP_COUNT)
                logger.warn(""Maximum number of memory map areas per process (vm.max_map_count) {} "" +
                            ""is too low, recommended value: {}, you can change it with sysctl."",
                            maxMapCount, EXPECTED_MAX_MAP_COUNT);
        }
    }
public void execute() throws StartupException
        {
            final Set<String> invalid = new HashSet<>();
            final Set<String> nonSSTablePaths = new HashSet<>();
            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getCommitLogLocation()));
            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getSavedCachesLocation()));
            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getHintsDirectory()));

            FileVisitor<Path> sstableVisitor = new SimpleFileVisitor<Path>()
            {
                public FileVisitResult visitFile(Path path, BasicFileAttributes attrs)
                {
                    File file = path.toFile();
                    if (!Descriptor.isValidFile(file))
                        return FileVisitResult.CONTINUE;

                    try
                    {
                        if (!Descriptor.fromFilename(file).isCompatible())
                            invalid.add(file.toString());
                    }
                    catch (Exception e)
                    {
                        invalid.add(file.toString());
                    }
                    return FileVisitResult.CONTINUE;
                }

                public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException
                {
                    String name = dir.getFileName().toString();
                    return (name.equals(Directories.SNAPSHOT_SUBDIR)
                            || name.equals(Directories.BACKUPS_SUBDIR)
                            || nonSSTablePaths.contains(dir.toFile().getCanonicalPath()))
                           ? FileVisitResult.SKIP_SUBTREE
                           : FileVisitResult.CONTINUE;
                }
            };

            for (String dataDir : DatabaseDescriptor.getAllDataFileLocations())
            {
                try
                {
                    Files.walkFileTree(Paths.get(dataDir), sstableVisitor);
                }
                catch (IOException e)
                {
                    throw new StartupException(3, ""Unable to verify sstable files on disk"", e);
                }
            }

            if (!invalid.isEmpty())
                throw new StartupException(StartupException.ERR_WRONG_DISK_STATE,
                                           String.format(""Detected unreadable sstables %s, please check "" +
                                                         ""NEWS.txt and ensure that you have upgraded through "" +
                                                         ""all required intermediate versions, running "" +
                                                         ""upgradesstables"",
                                                         Joiner.on("","").join(invalid)));

        }
    }
public void execute() throws StartupException
        {
            // check the system keyspace to keep user from shooting self in foot by changing partitioner, cluster name, etc.
            // we do a one-off scrub of the system keyspace first; we can't load the list of the rest of the keyspaces,
            // until system keyspace is opened.

            for (TableMetadata cfm : Schema.instance.getTablesAndViews(SchemaConstants.SYSTEM_KEYSPACE_NAME))
                ColumnFamilyStore.scrubDataDirectories(cfm);

            try
            {
                SystemKeyspace.checkHealth();
            }
            catch (ConfigurationException e)
            {
                throw new StartupException(StartupException.ERR_WRONG_CONFIG, ""Fatal exception during initialization"", e);
            }
        }
    }
public void execute() throws StartupException
        {
            if (!Boolean.getBoolean(""cassandra.ignore_dc""))
            {
                String storedDc = SystemKeyspace.getDatacenter();
                if (storedDc != null)
                {
                    String currentDc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
                    if (!storedDc.equals(currentDc))
                    {
                        String formatMessage = ""Cannot start node if snitch's data center (%s) differs from previous data center (%s). "" +
                                               ""Please fix the snitch configuration, decommission and rebootstrap this node or use the flag -Dcassandra.ignore_dc=true."";

                        throw new StartupException(StartupException.ERR_WRONG_CONFIG, String.format(formatMessage, currentDc, storedDc));
                    }
                }
            }
        }
    }
public void execute() throws StartupException
        {
            if (!Boolean.getBoolean(""cassandra.ignore_rack""))
            {
                String storedRack = SystemKeyspace.getRack();
                if (storedRack != null)
                {
                    String currentRack = DatabaseDescriptor.getEndpointSnitch().getLocalRack();
                    if (!storedRack.equals(currentRack))
                    {
                        String formatMessage = ""Cannot start node if snitch's rack (%s) differs from previous rack (%s). "" +
                                               ""Please fix the snitch configuration, decommission and rebootstrap this node or use the flag -Dcassandra.ignore_rack=true."";

                        throw new StartupException(StartupException.ERR_WRONG_CONFIG, String.format(formatMessage, currentRack, storedRack));
                    }
                }
            }
        }
    }"
M:org.apache.cassandra.service.StartupChecks$12:execute(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,execute,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StartupChecks.java,StartupChecks$12,../data/xml/cassandra/StartupChecks.xml,"public void execute()
        {
            if (FBUtilities.isWindows)
                return;
            String jemalloc = System.getProperty(""cassandra.libjemalloc"");
            if (jemalloc == null)
                logger.warn(""jemalloc shared library could not be preloaded to speed up memory allocations"");
            else if (""-"".equals(jemalloc))
                logger.info(""jemalloc preload explicitly disabled"");
            else
                logger.info(""jemalloc seems to be preloaded from {}"", jemalloc);
        }
    }
public void execute() throws StartupException
        {
            long now = System.currentTimeMillis();
            if (now < EARLIEST_LAUNCH_DATE)
                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE,
                                           String.format(""current machine time is %s, but that is seemingly incorrect. exiting now."",
                                                         new Date(now).toString()));
        }
    }
public void execute()
        {
            String jmxPort = System.getProperty(""cassandra.jmx.remote.port"");
            if (jmxPort == null)
            {
                logger.warn(""JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info."");
                jmxPort = System.getProperty(""cassandra.jmx.local.port"");
                if (jmxPort == null)
                    logger.error(""cassandra.jmx.local.port missing from cassandra-env.sh, unable to start local JMX service."");
            }
            else
            {
                logger.info(""JMX is enabled to receive remote connections on port: {}"", jmxPort);
            }
        }
    }
public void execute()
        {
            if (COM_SUN_MANAGEMENT_JMXREMOTE_PORT.isPresent())
            {
                logger.warn(""Use of com.sun.management.jmxremote.port at startup is deprecated. "" +
                            ""Please use cassandra.jmx.remote.port instead."");
            }
        }
    }
public void execute()
        {
            // log warnings for different kinds of sub-optimal JVMs.  tldr use 64-bit Oracle >= 1.6u32
            if (!DatabaseDescriptor.hasLargeAddressSpace())
                logger.warn(""32bit JVM detected.  It is recommended to run Cassandra on a 64bit JVM for better performance."");

            String javaVmName = JAVA_VM_NAME.getString();
            if (!(javaVmName.contains(""HotSpot"") || javaVmName.contains(""OpenJDK"")))
            {
                logger.warn(""Non-Oracle JVM detected.  Some features, such as immediate unmap of compacted SSTables, may not work as intended"");
            }
            else
            {
                checkOutOfMemoryHandling();
            }
        }

        
public void execute() throws StartupException
        {
            // Fail-fast if the native library could not be linked.
            if (!NativeLibrary.isAvailable())
                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE, ""The native library could not be initialized properly. "");
        }
    }
public void execute()
        {
            SigarLibrary.instance.warnIfRunningInDegradedMode();
        }
    }
public void execute()
        {
            if (!FBUtilities.isLinux)
                return;

            if (DatabaseDescriptor.getDiskAccessMode() == Config.DiskAccessMode.standard &&
                DatabaseDescriptor.getIndexAccessMode() == Config.DiskAccessMode.standard)
                return; // no need to check if disk access mode is only standard and not mmap

            long maxMapCount = getMaxMapCount();
            if (maxMapCount < EXPECTED_MAX_MAP_COUNT)
                logger.warn(""Maximum number of memory map areas per process (vm.max_map_count) {} "" +
                            ""is too low, recommended value: {}, you can change it with sysctl."",
                            maxMapCount, EXPECTED_MAX_MAP_COUNT);
        }
    }
public void execute() throws StartupException
        {
            final Set<String> invalid = new HashSet<>();
            final Set<String> nonSSTablePaths = new HashSet<>();
            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getCommitLogLocation()));
            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getSavedCachesLocation()));
            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getHintsDirectory()));

            FileVisitor<Path> sstableVisitor = new SimpleFileVisitor<Path>()
            {
                public FileVisitResult visitFile(Path path, BasicFileAttributes attrs)
                {
                    File file = path.toFile();
                    if (!Descriptor.isValidFile(file))
                        return FileVisitResult.CONTINUE;

                    try
                    {
                        if (!Descriptor.fromFilename(file).isCompatible())
                            invalid.add(file.toString());
                    }
                    catch (Exception e)
                    {
                        invalid.add(file.toString());
                    }
                    return FileVisitResult.CONTINUE;
                }

                public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException
                {
                    String name = dir.getFileName().toString();
                    return (name.equals(Directories.SNAPSHOT_SUBDIR)
                            || name.equals(Directories.BACKUPS_SUBDIR)
                            || nonSSTablePaths.contains(dir.toFile().getCanonicalPath()))
                           ? FileVisitResult.SKIP_SUBTREE
                           : FileVisitResult.CONTINUE;
                }
            };

            for (String dataDir : DatabaseDescriptor.getAllDataFileLocations())
            {
                try
                {
                    Files.walkFileTree(Paths.get(dataDir), sstableVisitor);
                }
                catch (IOException e)
                {
                    throw new StartupException(3, ""Unable to verify sstable files on disk"", e);
                }
            }

            if (!invalid.isEmpty())
                throw new StartupException(StartupException.ERR_WRONG_DISK_STATE,
                                           String.format(""Detected unreadable sstables %s, please check "" +
                                                         ""NEWS.txt and ensure that you have upgraded through "" +
                                                         ""all required intermediate versions, running "" +
                                                         ""upgradesstables"",
                                                         Joiner.on("","").join(invalid)));

        }
    }
public void execute() throws StartupException
        {
            // check the system keyspace to keep user from shooting self in foot by changing partitioner, cluster name, etc.
            // we do a one-off scrub of the system keyspace first; we can't load the list of the rest of the keyspaces,
            // until system keyspace is opened.

            for (TableMetadata cfm : Schema.instance.getTablesAndViews(SchemaConstants.SYSTEM_KEYSPACE_NAME))
                ColumnFamilyStore.scrubDataDirectories(cfm);

            try
            {
                SystemKeyspace.checkHealth();
            }
            catch (ConfigurationException e)
            {
                throw new StartupException(StartupException.ERR_WRONG_CONFIG, ""Fatal exception during initialization"", e);
            }
        }
    }
public void execute() throws StartupException
        {
            if (!Boolean.getBoolean(""cassandra.ignore_dc""))
            {
                String storedDc = SystemKeyspace.getDatacenter();
                if (storedDc != null)
                {
                    String currentDc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
                    if (!storedDc.equals(currentDc))
                    {
                        String formatMessage = ""Cannot start node if snitch's data center (%s) differs from previous data center (%s). "" +
                                               ""Please fix the snitch configuration, decommission and rebootstrap this node or use the flag -Dcassandra.ignore_dc=true."";

                        throw new StartupException(StartupException.ERR_WRONG_CONFIG, String.format(formatMessage, currentDc, storedDc));
                    }
                }
            }
        }
    }
public void execute() throws StartupException
        {
            if (!Boolean.getBoolean(""cassandra.ignore_rack""))
            {
                String storedRack = SystemKeyspace.getRack();
                if (storedRack != null)
                {
                    String currentRack = DatabaseDescriptor.getEndpointSnitch().getLocalRack();
                    if (!storedRack.equals(currentRack))
                    {
                        String formatMessage = ""Cannot start node if snitch's rack (%s) differs from previous rack (%s). "" +
                                               ""Please fix the snitch configuration, decommission and rebootstrap this node or use the flag -Dcassandra.ignore_rack=true."";

                        throw new StartupException(StartupException.ERR_WRONG_CONFIG, String.format(formatMessage, currentRack, storedRack));
                    }
                }
            }
        }
    }"
"M:org.apache.cassandra.service.StorageProxy:mutate(java.util.List,org.apache.cassandra.db.ConsistencyLevel,long)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,mutate,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java,StorageProxy,../data/xml/cassandra/StorageProxy.xml,"/**
     * Use this method to have these Mutations applied
     * across all replicas. This method will take care
     * of the possibility of a replica being down and hint
     * the data across to some other replica.
     *
     * @param mutations the mutations to be applied across the replicas
     * @param consistencyLevel the consistency level for the operation
     * @param queryStartNanoTime the value of System.nanoTime() when the query started to be processed
     */
public static void mutate(List<? extends IMutation> mutations, ConsistencyLevel consistencyLevel, long queryStartNanoTime)
    throws UnavailableException, OverloadedException, WriteTimeoutException, WriteFailureException
    {
        Tracing.trace(""Determining replicas for mutation"");
        final String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();

        long startTime = System.nanoTime();

        List<AbstractWriteResponseHandler<IMutation>> responseHandlers = new ArrayList<>(mutations.size());
        WriteType plainWriteType = mutations.size() <= 1 ? WriteType.SIMPLE : WriteType.UNLOGGED_BATCH;

        try
        {
            for (IMutation mutation : mutations)
            {
                if (mutation instanceof CounterMutation)
                    responseHandlers.add(mutateCounter((CounterMutation)mutation, localDataCenter, queryStartNanoTime));
                else
                    responseHandlers.add(performWrite(mutation, consistencyLevel, localDataCenter, standardWritePerformer, null, plainWriteType, queryStartNanoTime));
            }

            // upgrade to full quorum any failed cheap quorums
            for (int i = 0 ; i < mutations.size() ; ++i)
            {
                if (!(mutations.get(i) instanceof CounterMutation)) // at the moment, only non-counter writes support cheap quorums
                    responseHandlers.get(i).maybeTryAdditionalReplicas(mutations.get(i), standardWritePerformer, localDataCenter);
            }

            // wait for writes.  throws TimeoutException if necessary
            for (AbstractWriteResponseHandler<IMutation> responseHandler : responseHandlers)
                responseHandler.get();
        }
        catch (WriteTimeoutException|WriteFailureException ex)
        {
            if (consistencyLevel == ConsistencyLevel.ANY)
            {
                hintMutations(mutations);
            }
            else
            {
                if (ex instanceof WriteFailureException)
                {
                    writeMetrics.failures.mark();
                    writeMetricsForLevel(consistencyLevel).failures.mark();
                    WriteFailureException fe = (WriteFailureException)ex;
                    Tracing.trace(""Write failure; received {} of {} required replies, failed {} requests"",
                                  fe.received, fe.blockFor, fe.failureReasonByEndpoint.size());
                }
                else
                {
                    writeMetrics.timeouts.mark();
                    writeMetricsForLevel(consistencyLevel).timeouts.mark();
                    WriteTimeoutException te = (WriteTimeoutException)ex;
                    Tracing.trace(""Write timeout; received {} of {} required replies"", te.received, te.blockFor);
                }
                throw ex;
            }
        }
        catch (UnavailableException e)
        {
            writeMetrics.unavailables.mark();
            writeMetricsForLevel(consistencyLevel).unavailables.mark();
            Tracing.trace(""Unavailable"");
            throw e;
        }
        catch (OverloadedException e)
        {
            writeMetrics.unavailables.mark();
            writeMetricsForLevel(consistencyLevel).unavailables.mark();
            Tracing.trace(""Overloaded"");
            throw e;
        }
        finally
        {
            long latency = System.nanoTime() - startTime;
            writeMetrics.addNano(latency);
            writeMetricsForLevel(consistencyLevel).addNano(latency);
            updateCoordinatorWriteLatencyTableMetric(mutations, latency);
        }
    }

    "
"M:org.apache.cassandra.service.StorageProxy:mutateMV(java.nio.ByteBuffer,java.util.Collection,boolean,java.util.concurrent.atomic.AtomicLong,long)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,mutateMV,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java,StorageProxy,../data/xml/cassandra/StorageProxy.xml,"/**
     * Use this method to have these Mutations applied
     * across all replicas.
     *
     * @param mutations the mutations to be applied across the replicas
     * @param writeCommitLog if commitlog should be written
     * @param baseComplete time from epoch in ms that the local base mutation was(or will be) completed
     * @param queryStartNanoTime the value of System.nanoTime() when the query started to be processed
     */
public static void mutateMV(ByteBuffer dataKey, Collection<Mutation> mutations, boolean writeCommitLog, AtomicLong baseComplete, long queryStartNanoTime)
    throws UnavailableException, OverloadedException, WriteTimeoutException
    {
        Tracing.trace(""Determining replicas for mutation"");
        final String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();

        long startTime = System.nanoTime();


        try
        {
            // if we haven't joined the ring, write everything to batchlog because paired replicas may be stale
            final UUID batchUUID = UUIDGen.getTimeUUID();

            if (StorageService.instance.isStarting() || StorageService.instance.isJoining() || StorageService.instance.isMoving())
            {
                BatchlogManager.store(Batch.createLocal(batchUUID, FBUtilities.timestampMicros(),
                                                        mutations), writeCommitLog);
            }
            else
            {
                List<WriteResponseHandlerWrapper> wrappers = new ArrayList<>(mutations.size());
                //non-local mutations rely on the base mutation commit-log entry for eventual consistency
                Set<Mutation> nonLocalMutations = new HashSet<>(mutations);
                Token baseToken = StorageService.instance.getTokenMetadata().partitioner.getToken(dataKey);

                ConsistencyLevel consistencyLevel = ConsistencyLevel.ONE;

                //Since the base -> view replication is 1:1 we only need to store the BL locally
                ReplicaPlan.ForTokenWrite replicaPlan = ReplicaPlans.forLocalBatchlogWrite();
                BatchlogCleanup cleanup = new BatchlogCleanup(mutations.size(),
                                                              () -> asyncRemoveFromBatchlog(replicaPlan, batchUUID));

                // add a handler for each mutation - includes checking availability, but doesn't initiate any writes, yet
                for (Mutation mutation : mutations)
                {
                    String keyspaceName = mutation.getKeyspaceName();
                    Token tk = mutation.key().getToken();
                    AbstractReplicationStrategy replicationStrategy = Keyspace.open(keyspaceName).getReplicationStrategy();
                    Optional<Replica> pairedEndpoint = ViewUtils.getViewNaturalEndpoint(replicationStrategy, baseToken, tk);
                    EndpointsForToken pendingReplicas = StorageService.instance.getTokenMetadata().pendingEndpointsForToken(tk, keyspaceName);

                    // if there are no paired endpoints there are probably range movements going on, so we write to the local batchlog to replay later
                    if (!pairedEndpoint.isPresent())
                    {
                        if (pendingReplicas.isEmpty())
                            logger.warn(""Received base materialized view mutation for key {} that does not belong "" +
                                        ""to this node. There is probably a range movement happening (move or decommission),"" +
                                        ""but this node hasn't updated its ring metadata yet. Adding mutation to "" +
                                        ""local batchlog to be replayed later."",
                                        mutation.key());
                        continue;
                    }

                    // When local node is the endpoint we can just apply the mutation locally,
                    // unless there are pending endpoints, in which case we want to do an ordinary
                    // write so the view mutation is sent to the pending endpoint
                    if (pairedEndpoint.get().isSelf() && StorageService.instance.isJoined()
                        && pendingReplicas.isEmpty())
                    {
                        try
                        {
                            mutation.apply(writeCommitLog);
                            nonLocalMutations.remove(mutation);
                            // won't trigger cleanup
                            cleanup.decrement();
                        }
                        catch (Exception exc)
                        {
                            logger.error(""Error applying local view update: Mutation (keyspace {}, tables {}, partition key {})"",
                                         mutation.getKeyspaceName(), mutation.getTableIds(), mutation.key());
                            throw exc;
                        }
                    }
                    else
                    {
                        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(replicationStrategy,
                                                                                              EndpointsForToken.of(tk, pairedEndpoint.get()),
                                                                                              pendingReplicas);
                        wrappers.add(wrapViewBatchResponseHandler(mutation,
                                                                  consistencyLevel,
                                                                  consistencyLevel,
                                                                  liveAndDown,
                                                                  baseComplete,
                                                                  WriteType.BATCH,
                                                                  cleanup,
                                                                  queryStartNanoTime));
                    }
                }

                // Apply to local batchlog memtable in this thread
                if (!nonLocalMutations.isEmpty())
                    BatchlogManager.store(Batch.createLocal(batchUUID, FBUtilities.timestampMicros(), nonLocalMutations), writeCommitLog);

                // Perform remote writes
                if (!wrappers.isEmpty())
                    asyncWriteBatchedMutations(wrappers, localDataCenter, Stage.VIEW_MUTATION);
            }
        }
        finally
        {
            viewWriteMetrics.addNano(System.nanoTime() - startTime);
        }
    }

    "
"M:org.apache.cassandra.service.StorageProxy:syncWriteBatchedMutations(java.util.List,org.apache.cassandra.concurrent.Stage)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,syncWriteBatchedMutations,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java,StorageProxy,../data/xml/cassandra/StorageProxy.xml,"private static void syncWriteBatchedMutations(List<WriteResponseHandlerWrapper> wrappers, Stage stage)
    throws WriteTimeoutException, OverloadedException
    {
        String localDataCenter = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();

        for (WriteResponseHandlerWrapper wrapper : wrappers)
        {
            EndpointsForToken sendTo = wrapper.handler.replicaPlan.liveAndDown();
            Replicas.temporaryAssertFull(sendTo); // TODO: CASSANDRA-14549
            sendToHintedReplicas(wrapper.mutation, wrapper.handler.replicaPlan.withContact(sendTo), wrapper.handler, localDataCenter, stage);
        }

        for (WriteResponseHandlerWrapper wrapper : wrappers)
            wrapper.handler.get();
    }

    "
"M:org.apache.cassandra.service.StorageProxy:sendToHintedReplicas(org.apache.cassandra.db.Mutation,org.apache.cassandra.locator.ReplicaPlan$ForTokenWrite,org.apache.cassandra.service.AbstractWriteResponseHandler,java.lang.String,org.apache.cassandra.concurrent.Stage)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,sendToHintedReplicas,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java,StorageProxy,../data/xml/cassandra/StorageProxy.xml,"/**
     * Send the mutations to the right targets, write it locally if it corresponds or writes a hint when the node
     * is not available.
     *
     * Note about hints:
     * <pre>
     * {@code
     * | Hinted Handoff | Consist. Level |
     * | on             |       >=1      | --> wait for hints. We DO NOT notify the handler with handler.response() for hints;
     * | on             |       ANY      | --> wait for hints. Responses count towards consistency.
     * | off            |       >=1      | --> DO NOT fire hints. And DO NOT wait for them to complete.
     * | off            |       ANY      | --> DO NOT fire hints. And DO NOT wait for them to complete.
     * }
     * </pre>
     *
     * @throws OverloadedException if the hints cannot be written/enqueued
     */
public static void sendToHintedReplicas(final Mutation mutation,
                                            ReplicaPlan.ForTokenWrite plan,
                                            AbstractWriteResponseHandler<IMutation> responseHandler,
                                            String localDataCenter,
                                            Stage stage)
    throws OverloadedException
    {
        // this dc replicas:
        Collection<Replica> localDc = null;
        // extra-datacenter replicas, grouped by dc
        Map<String, Collection<Replica>> dcGroups = null;
        // only need to create a Message for non-local writes
        Message<Mutation> message = null;

        boolean insertLocal = false;
        Replica localReplica = null;
        Collection<Replica> endpointsToHint = null;

        List<InetAddressAndPort> backPressureHosts = null;

        for (Replica destination : plan.contacts())
        {
            checkHintOverload(destination);

            if (plan.isAlive(destination))
            {
                if (destination.isSelf())
                {
                    insertLocal = true;
                    localReplica = destination;
                }
                else
                {
                    // belongs on a different server
                    if (message == null)
                        message = Message.outWithFlag(MUTATION_REQ, mutation, MessageFlag.CALL_BACK_ON_FAILURE);

                    String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(destination);

                    // direct writes to local DC or old Cassandra versions
                    // (1.1 knows how to forward old-style String message IDs; updated to int in 2.0)
                    if (localDataCenter.equals(dc))
                    {
                        if (localDc == null)
                            localDc = new ArrayList<>(plan.contacts().size());

                        localDc.add(destination);
                    }
                    else
                    {
                        if (dcGroups == null)
                            dcGroups = new HashMap<>();

                        Collection<Replica> messages = dcGroups.get(dc);
                        if (messages == null)
                            messages = dcGroups.computeIfAbsent(dc, (v) -> new ArrayList<>(3)); // most DCs will have <= 3 replicas

                        messages.add(destination);
                    }

                    if (backPressureHosts == null)
                        backPressureHosts = new ArrayList<>(plan.contacts().size());

                    backPressureHosts.add(destination.endpoint());
                }
            }
            else
            {
                //Immediately mark the response as expired since the request will not be sent
                responseHandler.expired();
                if (shouldHint(destination))
                {
                    if (endpointsToHint == null)
                        endpointsToHint = new ArrayList<>();

                    endpointsToHint.add(destination);
                }
            }
        }

        if (endpointsToHint != null)
            submitHint(mutation, EndpointsForToken.copyOf(mutation.key().getToken(), endpointsToHint), responseHandler);

        if (insertLocal)
        {
            Preconditions.checkNotNull(localReplica);
            performLocally(stage, localReplica, mutation::apply, responseHandler);
        }

        if (localDc != null)
        {
            for (Replica destination : localDc)
                MessagingService.instance().sendWriteWithCallback(message, destination, responseHandler, true);
        }
        if (dcGroups != null)
        {
            // for each datacenter, send the message to one node to relay the write to other replicas
            for (Collection<Replica> dcTargets : dcGroups.values())
                sendMessagesToNonlocalDC(message, EndpointsForToken.copyOf(mutation.key().getToken(), dcTargets), responseHandler);
        }
    }

    "
"M:org.apache.cassandra.service.StorageProxy:findSuitableReplica(java.lang.String,org.apache.cassandra.db.DecoratedKey,java.lang.String,org.apache.cassandra.db.ConsistencyLevel)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,findSuitableReplica,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java,StorageProxy,../data/xml/cassandra/StorageProxy.xml,"/**
     * Find a suitable replica as leader for counter update.
     * For now, we pick a random replica in the local DC (or ask the snitch if
     * there is no replica alive in the local DC).
     * TODO: if we track the latency of the counter writes (which makes sense
     * contrarily to standard writes since there is a read involved), we could
     * trust the dynamic snitch entirely, which may be a better solution. It
     * is unclear we want to mix those latencies with read latencies, so this
     * may be a bit involved.
     */
private static Replica findSuitableReplica(String keyspaceName, DecoratedKey key, String localDataCenter, ConsistencyLevel cl) throws UnavailableException
    {
        Keyspace keyspace = Keyspace.open(keyspaceName);
        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        AbstractReplicationStrategy replicationStrategy = keyspace.getReplicationStrategy();
        EndpointsForToken replicas = replicationStrategy.getNaturalReplicasForToken(key);

        // CASSANDRA-13043: filter out those endpoints not accepting clients yet, maybe because still bootstrapping
        replicas = replicas.filter(replica -> StorageService.instance.isRpcReady(replica.endpoint()));

        // CASSANDRA-17411: filter out endpoints that are not alive
        replicas = replicas.filter(replica -> FailureDetector.instance.isAlive(replica.endpoint()));

        // TODO have a way to compute the consistency level
        if (replicas.isEmpty())
            throw UnavailableException.create(cl, cl.blockFor(replicationStrategy), 0);

        List<Replica> localReplicas = new ArrayList<>(replicas.size());

        for (Replica replica : replicas)
            if (snitch.getDatacenter(replica).equals(localDataCenter))
                localReplicas.add(replica);

        if (localReplicas.isEmpty())
        {
            // If the consistency required is local then we should not involve other DCs
            if (cl.isDatacenterLocal())
                throw UnavailableException.create(cl, cl.blockFor(replicationStrategy), 0);

            // No endpoint in local DC, pick the closest endpoint according to the snitch
            replicas = snitch.sortedByProximity(FBUtilities.getBroadcastAddressAndPort(), replicas);
            return replicas.get(0);
        }

        return localReplicas.get(ThreadLocalRandom.current().nextInt(localReplicas.size()));
    }

    "
M:org.apache.cassandra.service.StorageProxy:shouldHint(org.apache.cassandra.locator.Replica),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,shouldHint,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java,StorageProxy,../data/xml/cassandra/StorageProxy.xml,"public static boolean shouldHint(Replica replica)
    {
        if (!DatabaseDescriptor.hintedHandoffEnabled())
            return false;
        if (replica.isTransient() || replica.isSelf())
            return false;

        Set<String> disabledDCs = DatabaseDescriptor.hintedHandoffDisabledDCs();
        if (!disabledDCs.isEmpty())
        {
            final String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(replica);
            if (disabledDCs.contains(dc))
            {
                Tracing.trace(""Not hinting {} since its data center {} has been disabled {}"", replica, dc, disabledDCs);
                return false;
            }
        }
        boolean hintWindowExpired = Gossiper.instance.getEndpointDowntime(replica.endpoint()) > DatabaseDescriptor.getMaxHintWindow();
        if (hintWindowExpired)
        {
            HintsService.instance.metrics.incrPastWindow(replica.endpoint());
            Tracing.trace(""Not hinting {} which has been down {} ms"", replica, Gossiper.instance.getEndpointDowntime(replica.endpoint()));
        }
        return !hintWindowExpired;
    }

    "
M:org.apache.cassandra.service.StorageService:validateEndpointSnitch(java.util.Iterator),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,validateEndpointSnitch,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"private static void validateEndpointSnitch(Iterator<EndpointState> endpointStates)
    {
        Set<String> datacenters = new HashSet<>();
        Set<String> racks = new HashSet<>();
        while (endpointStates.hasNext())
        {
            EndpointState state = endpointStates.next();
            VersionedValue val = state.getApplicationState(ApplicationState.DC);
            if (val != null)
                datacenters.add(val.value);
            val = state.getApplicationState(ApplicationState.RACK);
            if (val != null)
                racks.add(val.value);
        }

        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        if (!snitch.validate(datacenters, racks))
        {
            throw new IllegalStateException();
        }
    }

    "
M:org.apache.cassandra.service.StorageService:gossipSnitchInfo(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,gossipSnitchInfo,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"public void gossipSnitchInfo()
    {
        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        String dc = snitch.getLocalDatacenter();
        String rack = snitch.getLocalRack();
        Gossiper.instance.addLocalApplicationState(ApplicationState.DC, StorageService.instance.valueFactory.datacenter(dc));
        Gossiper.instance.addLocalApplicationState(ApplicationState.RACK, StorageService.instance.valueFactory.rack(rack));
    }

    "
"M:org.apache.cassandra.service.StorageService:rebuild(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,rebuild,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"public void rebuild(String sourceDc)
    {
        rebuild(sourceDc, null, null, null);
    }

    
public void rebuild(String sourceDc, String keyspace, String tokens, String specificSources)
    {
        // check ongoing rebuild
        if (!isRebuilding.compareAndSet(false, true))
        {
            throw new IllegalStateException(""Node is still rebuilding. Check nodetool netstats."");
        }

        // check the arguments
        if (keyspace == null && tokens != null)
        {
            throw new IllegalArgumentException(""Cannot specify tokens without keyspace."");
        }

        logger.info(""rebuild from dc: {}, {}, {}"", sourceDc == null ? ""(any dc)"" : sourceDc,
                    keyspace == null ? ""(All keyspaces)"" : keyspace,
                    tokens == null ? ""(All tokens)"" : tokens);

        try
        {
            RangeStreamer streamer = new RangeStreamer(tokenMetadata,
                                                       null,
                                                       FBUtilities.getBroadcastAddressAndPort(),
                                                       StreamOperation.REBUILD,
                                                       useStrictConsistency && !replacing,
                                                       DatabaseDescriptor.getEndpointSnitch(),
                                                       streamStateStore,
                                                       false,
                                                       DatabaseDescriptor.getStreamingConnectionsPerHost());
            if (sourceDc != null)
                streamer.addSourceFilter(new RangeStreamer.SingleDatacenterFilter(DatabaseDescriptor.getEndpointSnitch(), sourceDc));

            if (keyspace == null)
            {
                for (String keyspaceName : Schema.instance.getNonLocalStrategyKeyspaces())
                    streamer.addRanges(keyspaceName, getLocalReplicas(keyspaceName));
            }
            else if (tokens == null)
            {
                streamer.addRanges(keyspace, getLocalReplicas(keyspace));
            }
            else
            {
                Token.TokenFactory factory = getTokenFactory();
                List<Range<Token>> ranges = new ArrayList<>();
                Pattern rangePattern = Pattern.compile(""\\(\\s*(-?\\w+)\\s*,\\s*(-?\\w+)\\s*\\]"");
                try (Scanner tokenScanner = new Scanner(tokens))
                {
                    while (tokenScanner.findInLine(rangePattern) != null)
                    {
                        MatchResult range = tokenScanner.match();
                        Token startToken = factory.fromString(range.group(1));
                        Token endToken = factory.fromString(range.group(2));
                        logger.info(""adding range: ({},{}]"", startToken, endToken);
                        ranges.add(new Range<>(startToken, endToken));
                    }
                    if (tokenScanner.hasNext())
                        throw new IllegalArgumentException(""Unexpected string: "" + tokenScanner.next());
                }

                // Ensure all specified ranges are actually ranges owned by this host
                RangesAtEndpoint localReplicas = getLocalReplicas(keyspace);
                RangesAtEndpoint.Builder streamRanges = new RangesAtEndpoint.Builder(FBUtilities.getBroadcastAddressAndPort(), ranges.size());
                for (Range<Token> specifiedRange : ranges)
                {
                    boolean foundParentRange = false;
                    for (Replica localReplica : localReplicas)
                    {
                        if (localReplica.contains(specifiedRange))
                        {
                            streamRanges.add(localReplica.decorateSubrange(specifiedRange));
                            foundParentRange = true;
                            break;
                        }
                    }
                    if (!foundParentRange)
                    {
                        throw new IllegalArgumentException(String.format(""The specified range %s is not a range that is owned by this node. Please ensure that all token ranges specified to be rebuilt belong to this node."", specifiedRange.toString()));
                    }
                }

                if (specificSources != null)
                {
                    String[] stringHosts = specificSources.split("","");
                    Set<InetAddressAndPort> sources = new HashSet<>(stringHosts.length);
                    for (String stringHost : stringHosts)
                    {
                        try
                        {
                            InetAddressAndPort endpoint = InetAddressAndPort.getByName(stringHost);
                            if (FBUtilities.getBroadcastAddressAndPort().equals(endpoint))
                            {
                                throw new IllegalArgumentException(""This host was specified as a source for rebuilding. Sources for a rebuild can only be other nodes in the cluster."");
                            }
                            sources.add(endpoint);
                        }
                        catch (UnknownHostException ex)
                        {
                            throw new IllegalArgumentException(""Unknown host specified "" + stringHost, ex);
                        }
                    }
                    streamer.addSourceFilter(new RangeStreamer.AllowedSourcesFilter(sources));
                }

                streamer.addRanges(keyspace, streamRanges.build());
            }

            StreamResultFuture resultFuture = streamer.fetchAsync();
            // wait for result
            resultFuture.get();
        }
        catch (InterruptedException e)
        {
            throw new RuntimeException(""Interrupted while waiting on rebuild streaming"");
        }
        catch (ExecutionException e)
        {
            // This is used exclusively through JMX, so log the full trace but only throw a simple RTE
            logger.error(""Error while rebuilding node"", e.getCause());
            throw new RuntimeException(""Error while rebuilding node: "" + e.getCause().getMessage());
        }
        finally
        {
            // rebuild is done (successfully or not)
            isRebuilding.set(false);
        }
    }

    "
M:org.apache.cassandra.service.StorageService:isLocalDC(org.apache.cassandra.locator.InetAddressAndPort),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,isLocalDC,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"private boolean isLocalDC(InetAddressAndPort targetHost)
    {
        String remoteDC = DatabaseDescriptor.getEndpointSnitch().getDatacenter(targetHost);
        String localDC = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();
        return remoteDC.equals(localDC);
    }

    "
"M:org.apache.cassandra.service.StorageService:getNewSourceReplicas(java.lang.String,java.util.Set)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getNewSourceReplicas,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"/**
     * Finds living endpoints responsible for the given ranges
     *
     * @param keyspaceName the keyspace ranges belong to
     * @param leavingReplicas the ranges to find sources for
     * @return multimap of addresses to ranges the address is responsible for
     */
private Multimap<InetAddressAndPort, FetchReplica> getNewSourceReplicas(String keyspaceName, Set<LeavingReplica> leavingReplicas)
    {
        InetAddressAndPort myAddress = FBUtilities.getBroadcastAddressAndPort();
        EndpointsByRange rangeReplicas = Keyspace.open(keyspaceName).getReplicationStrategy().getRangeAddresses(tokenMetadata.cloneOnlyTokenMap());
        Multimap<InetAddressAndPort, FetchReplica> sourceRanges = HashMultimap.create();
        IFailureDetector failureDetector = FailureDetector.instance;

        logger.debug(""Getting new source replicas for {}"", leavingReplicas);

        // find alive sources for our new ranges
        for (LeavingReplica leaver : leavingReplicas)
        {
            //We need this to find the replicas from before leaving to supply the data
            Replica leavingReplica = leaver.leavingReplica;
            //We need this to know what to fetch and what the transient status is
            Replica ourReplica = leaver.ourReplica;
            //If we are going to be a full replica only consider full replicas
            Predicate<Replica> replicaFilter = ourReplica.isFull() ? Replica::isFull : Predicates.alwaysTrue();
            Predicate<Replica> notSelf = replica -> !replica.endpoint().equals(myAddress);
            EndpointsForRange possibleReplicas = rangeReplicas.get(leavingReplica.range());
            logger.info(""Possible replicas for newReplica {} are {}"", ourReplica, possibleReplicas);
            IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
            EndpointsForRange sortedPossibleReplicas = snitch.sortedByProximity(myAddress, possibleReplicas);
            logger.info(""Sorted possible replicas starts as {}"", sortedPossibleReplicas);
            Optional<Replica> myCurrentReplica = tryFind(possibleReplicas, replica -> replica.endpoint().equals(myAddress)).toJavaUtil();

            boolean transientToFull = myCurrentReplica.isPresent() && myCurrentReplica.get().isTransient() && ourReplica.isFull();
            assert !sortedPossibleReplicas.endpoints().contains(myAddress) || transientToFull : String.format(""My address %s, sortedPossibleReplicas %s, myCurrentReplica %s, myNewReplica %s"", myAddress, sortedPossibleReplicas, myCurrentReplica, ourReplica);

            //Originally this didn't log if it couldn't restore replication and that seems wrong
            boolean foundLiveReplica = false;
            for (Replica possibleReplica : sortedPossibleReplicas.filter(Predicates.and(replicaFilter, notSelf)))
            {
                if (failureDetector.isAlive(possibleReplica.endpoint()))
                {
                    foundLiveReplica = true;
                    sourceRanges.put(possibleReplica.endpoint(), new FetchReplica(ourReplica, possibleReplica));
                    break;
                }
                else
                {
                    logger.debug(""Skipping down replica {}"", possibleReplica);
                }
            }
            if (!foundLiveReplica)
            {
                logger.warn(""Didn't find live replica to restore replication for "" + ourReplica);
            }
        }
        return sourceRanges;
    }

    "
"M:org.apache.cassandra.service.StorageService:getPrimaryRangeForEndpointWithinDC(java.lang.String,org.apache.cassandra.locator.InetAddressAndPort)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getPrimaryRangeForEndpointWithinDC,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"/**
     * Get the ""primary ranges"" within local DC for the specified keyspace and endpoint.
     *
     * @see #getPrimaryRangesForEndpoint(String, InetAddressAndPort)
     * @param keyspace Keyspace name to check primary ranges
     * @param referenceEndpoint endpoint we are interested in.
     * @return primary ranges within local DC for the specified endpoint.
     */
public Collection<Range<Token>> getPrimaryRangeForEndpointWithinDC(String keyspace, InetAddressAndPort referenceEndpoint)
    {
        TokenMetadata metadata = tokenMetadata.cloneOnlyTokenMap();
        String localDC = DatabaseDescriptor.getEndpointSnitch().getDatacenter(referenceEndpoint);
        Collection<InetAddressAndPort> localDcNodes = metadata.getTopology().getDatacenterEndpoints().get(localDC);
        AbstractReplicationStrategy strategy = Keyspace.open(keyspace).getReplicationStrategy();

        Collection<Range<Token>> localDCPrimaryRanges = new HashSet<>();
        for (Token token : metadata.sortedTokens())
        {
            EndpointsForRange replicas = strategy.calculateNaturalReplicas(token, metadata);
            for (Replica replica : replicas)
            {
                if (localDcNodes.contains(replica.endpoint()))
                {
                    if (replica.endpoint().equals(referenceEndpoint))
                    {
                        localDCPrimaryRanges.add(new Range<>(metadata.getPredecessor(token), token));
                    }
                    break;
                }
            }
        }

        return localDCPrimaryRanges;
    }

    "
M:org.apache.cassandra.service.StorageService:getLocalPrimaryRangeForEndpoint(org.apache.cassandra.locator.InetAddressAndPort),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getLocalPrimaryRangeForEndpoint,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"public Collection<Range<Token>> getLocalPrimaryRangeForEndpoint(InetAddressAndPort referenceEndpoint)
    {
        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        TokenMetadata tokenMetadata = this.tokenMetadata.cloneOnlyTokenMap();
        if (!tokenMetadata.isMember(referenceEndpoint))
            return Collections.emptySet();
        String dc = snitch.getDatacenter(referenceEndpoint);
        Set<Token> tokens = new HashSet<>(tokenMetadata.getTokens(referenceEndpoint));

        // filter tokens to the single DC
        List<Token> filteredTokens = Lists.newArrayList();
        for (Token token : tokenMetadata.sortedTokens())
        {
            InetAddressAndPort endpoint = tokenMetadata.getEndpoint(token);
            if (dc.equals(snitch.getDatacenter(endpoint)))
                filteredTokens.add(token);
        }

        return getAllRanges(filteredTokens).stream()
                                           .filter(t -> tokens.contains(t.right))
                                           .collect(Collectors.toList());
    }

    "
M:org.apache.cassandra.service.StorageService:decommission(boolean),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,decommission,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"public void decommission(boolean force) throws InterruptedException
    {
        TokenMetadata metadata = tokenMetadata.cloneAfterAllLeft();
        if (operationMode != Mode.LEAVING)
        {
            if (!tokenMetadata.isMember(FBUtilities.getBroadcastAddressAndPort()))
                throw new UnsupportedOperationException(""local node is not a member of the token ring yet"");
            if (metadata.getAllEndpoints().size() < 2)
                    throw new UnsupportedOperationException(""no other normal nodes in the ring; decommission would be pointless"");
            if (operationMode != Mode.NORMAL)
                throw new UnsupportedOperationException(""Node in "" + operationMode + "" state; wait for status to become normal or restart"");
        }
        if (!isDecommissioning.compareAndSet(false, true))
            throw new IllegalStateException(""Node is still decommissioning. Check nodetool netstats."");

        if (logger.isDebugEnabled())
            logger.debug(""DECOMMISSIONING"");

        try
        {
            PendingRangeCalculatorService.instance.blockUntilFinished();

            String dc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();

            if (operationMode != Mode.LEAVING) // If we're already decommissioning there is no point checking RF/pending ranges
            {
                int rf, numNodes;
                for (String keyspaceName : Schema.instance.getNonLocalStrategyKeyspaces())
                {
                    if (!force)
                    {
                        Keyspace keyspace = Keyspace.open(keyspaceName);
                        if (keyspace.getReplicationStrategy() instanceof NetworkTopologyStrategy)
                        {
                            NetworkTopologyStrategy strategy = (NetworkTopologyStrategy) keyspace.getReplicationStrategy();
                            rf = strategy.getReplicationFactor(dc).allReplicas;
                            numNodes = metadata.getTopology().getDatacenterEndpoints().get(dc).size();
                        }
                        else
                        {
                            numNodes = metadata.getAllEndpoints().size();
                            rf = keyspace.getReplicationStrategy().getReplicationFactor().allReplicas;
                        }

                        if (numNodes <= rf)
                            throw new UnsupportedOperationException(""Not enough live nodes to maintain replication factor in keyspace ""
                                                                    + keyspaceName + "" (RF = "" + rf + "", N = "" + numNodes + "").""
                                                                    + "" Perform a forceful decommission to ignore."");
                    }
                    // TODO: do we care about fixing transient/full self-movements here? probably
                    if (tokenMetadata.getPendingRanges(keyspaceName, FBUtilities.getBroadcastAddressAndPort()).size() > 0)
                        throw new UnsupportedOperationException(""data is currently moving to this node; unable to leave the ring"");
                }
            }

            startLeaving();
            long timeout = Math.max(RING_DELAY, BatchlogManager.instance.getBatchlogTimeout());
            setMode(Mode.LEAVING, ""sleeping "" + timeout + "" ms for batch processing and pending range setup"", true);
            Thread.sleep(timeout);

            Runnable finishLeaving = new Runnable()
            {
                public void run()
                {
                    shutdownClientServers();
                    Gossiper.instance.stop();
                    try
                    {
                        MessagingService.instance().shutdown();
                    }
                    catch (IOError ioe)
                    {
                        logger.info(""failed to shutdown message service: {}"", ioe);
                    }

                    Stage.shutdownNow();
                    SystemKeyspace.setBootstrapState(SystemKeyspace.BootstrapState.DECOMMISSIONED);
                    setMode(Mode.DECOMMISSIONED, true);
                    // let op be responsible for killing the process
                }
            };
            unbootstrap(finishLeaving);
        }
        catch (InterruptedException e)
        {
            throw new RuntimeException(""Node interrupted while decommissioning"");
        }
        catch (ExecutionException e)
        {
            logger.error(""Error while decommissioning node "", e.getCause());
            throw new RuntimeException(""Error while decommissioning node: "" + e.getCause().getMessage());
        }
        finally
        {
            isDecommissioning.set(false);
        }
    }

    "
M:org.apache.cassandra.service.StorageService:getPreferredHintsStreamTarget(),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,getPreferredHintsStreamTarget,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"/**
     * Find the best target to stream hints to. Currently the closest peer according to the snitch
     */
private UUID getPreferredHintsStreamTarget()
    {
        Set<InetAddressAndPort> endpoints = StorageService.instance.getTokenMetadata().cloneAfterAllLeft().getAllEndpoints();

        EndpointsForRange candidates = getStreamCandidates(endpoints);
        if (candidates.isEmpty())
        {
            logger.warn(""Unable to stream hints since no live endpoints seen"");
            throw new RuntimeException(""Unable to stream hints since no live endpoints seen"");
        }
        else
        {
            // stream to the closest peer as chosen by the snitch
            candidates = DatabaseDescriptor.getEndpointSnitch().sortedByProximity(FBUtilities.getBroadcastAddressAndPort(), candidates);
            InetAddressAndPort hintsDestinationHost = candidates.get(0).endpoint();
            return tokenMetadata.getHostId(hintsDestinationHost);
        }
    }

    "
M:org.apache.cassandra.service.StorageService:setDynamicUpdateInterval(int),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,setDynamicUpdateInterval,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"public void setDynamicUpdateInterval(int dynamicUpdateInterval)
    {
        if (DatabaseDescriptor.getEndpointSnitch() instanceof DynamicEndpointSnitch)
        {

            try
            {
                updateSnitch(null, true, dynamicUpdateInterval, null, null);
            }
            catch (ClassNotFoundException e)
            {
                throw new RuntimeException(e);
            }
        }
    }

    "
"M:org.apache.cassandra.service.StorageService:updateSnitch(java.lang.String,java.lang.Boolean,java.lang.Integer,java.lang.Integer,java.lang.Double)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,updateSnitch,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/StorageService.java,StorageService,../data/xml/cassandra/StorageService.xml,"public void updateSnitch(String epSnitchClassName, Boolean dynamic, Integer dynamicUpdateInterval, Integer dynamicResetInterval, Double dynamicBadnessThreshold) throws ClassNotFoundException
    {
        // apply dynamic snitch configuration
        if (dynamicUpdateInterval != null)
            DatabaseDescriptor.setDynamicUpdateInterval(dynamicUpdateInterval);
        if (dynamicResetInterval != null)
            DatabaseDescriptor.setDynamicResetInterval(dynamicResetInterval);
        if (dynamicBadnessThreshold != null)
            DatabaseDescriptor.setDynamicBadnessThreshold(dynamicBadnessThreshold);

        IEndpointSnitch oldSnitch = DatabaseDescriptor.getEndpointSnitch();

        // new snitch registers mbean during construction
        if(epSnitchClassName != null)
        {

            // need to unregister the mbean _before_ the new dynamic snitch is instantiated (and implicitly initialized
            // and its mbean registered)
            if (oldSnitch instanceof DynamicEndpointSnitch)
                ((DynamicEndpointSnitch)oldSnitch).close();

            IEndpointSnitch newSnitch;
            try
            {
                newSnitch = DatabaseDescriptor.createEndpointSnitch(dynamic != null && dynamic, epSnitchClassName);
            }
            catch (ConfigurationException e)
            {
                throw new ClassNotFoundException(e.getMessage());
            }

            if (newSnitch instanceof DynamicEndpointSnitch)
            {
                logger.info(""Created new dynamic snitch {} with update-interval={}, reset-interval={}, badness-threshold={}"",
                            ((DynamicEndpointSnitch)newSnitch).subsnitch.getClass().getName(), DatabaseDescriptor.getDynamicUpdateInterval(),
                            DatabaseDescriptor.getDynamicResetInterval(), DatabaseDescriptor.getDynamicBadnessThreshold());
            }
            else
            {
                logger.info(""Created new non-dynamic snitch {}"", newSnitch.getClass().getName());
            }

            // point snitch references to the new instance
            DatabaseDescriptor.setEndpointSnitch(newSnitch);
            for (String ks : Schema.instance.getKeyspaces())
            {
                Keyspace.open(ks).getReplicationStrategy().snitch = newSnitch;
            }
        }
        else
        {
            if (oldSnitch instanceof DynamicEndpointSnitch)
            {
                logger.info(""Applying config change to dynamic snitch {} with update-interval={}, reset-interval={}, badness-threshold={}"",
                            ((DynamicEndpointSnitch)oldSnitch).subsnitch.getClass().getName(), DatabaseDescriptor.getDynamicUpdateInterval(),
                            DatabaseDescriptor.getDynamicResetInterval(), DatabaseDescriptor.getDynamicBadnessThreshold());

                DynamicEndpointSnitch snitch = (DynamicEndpointSnitch)oldSnitch;
                snitch.applyConfigChanges();
            }
        }

        updateTopology();
    }

    "
"M:org.apache.cassandra.service.TokenRange:create(org.apache.cassandra.dht.Token$TokenFactory,org.apache.cassandra.dht.Range,java.util.List,boolean)",(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,create,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/TokenRange.java,TokenRange,../data/xml/cassandra/TokenRange.xml,"public static TokenRange create(Token.TokenFactory tokenFactory, Range<Token> range, List<InetAddressAndPort> endpoints, boolean withPorts)
    {
        List<EndpointDetails> details = new ArrayList<>(endpoints.size());
        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
        for (InetAddressAndPort ep : endpoints)
            details.add(new EndpointDetails(ep,
                                            StorageService.instance.getNativeaddress(ep, withPorts),
                                            snitch.getDatacenter(ep),
                                            snitch.getRack(ep)));
        return new TokenRange(tokenFactory, range, details);
    }

    "
M:org.apache.cassandra.service.reads.ReadCallback:assertWaitingFor(org.apache.cassandra.locator.InetAddressAndPort),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,assertWaitingFor,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/service/reads/ReadCallback.java,ReadCallback,../data/xml/cassandra/ReadCallback.xml,"/**
     * Verify that a message doesn't come from an unexpected replica.
     */
private void assertWaitingFor(InetAddressAndPort from)
    {
        assert !replicaPlan().consistencyLevel().isDatacenterLocal()
               || DatabaseDescriptor.getLocalDataCenter().equals(DatabaseDescriptor.getEndpointSnitch().getDatacenter(from))
               : ""Received read response from unexpected replica: "" + from;
    }
}"
M:org.apache.cassandra.streaming.StreamManager$StreamRateLimiter:<init>(org.apache.cassandra.locator.InetAddressAndPort),(S)org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),org.apache.cassandra.config.DatabaseDescriptor:getEndpointSnitch(),endpoint_snitch,<init>,/Users/wang/Documents/project/configuration_code_understanding/code3/data/system/cassandra/src/java/org/apache/cassandra/streaming/StreamManager.java,StreamManager$StreamRateLimiter,../data/xml/cassandra/StreamManager.xml,"/**
 * StreamManager manages currently running {@link StreamResultFuture}s and provides status of all operation invoked.
 *
 * All stream operation should be created through this class to track streaming status and progress.
 */
public class StreamManager implements StreamManagerMBean
{
    public static final StreamManager instance = new StreamManager();

    /**
     * Gets streaming rate limiter.
     * When stream_throughput_outbound_megabits_per_sec is 0, this returns rate limiter
     * with the rate of Double.MAX_VALUE bytes per second.
     * Rate unit is bytes per sec.
     *
     * @return StreamRateLimiter with rate limit set based on peer location.
     */
    public static StreamRateLimiter getRateLimiter(InetAddressAndPort peer)
    {
        return new StreamRateLimiter(peer);
    }

    public static class StreamRateLimiter
    {
        public static final double BYTES_PER_MEGABIT = (1000 * 1000) / 8.0;
        private static final RateLimiter limiter = RateLimiter.create(calculateRateInBytes());
        private static final RateLimiter interDCLimiter = RateLimiter.create(calculateInterDCRateInBytes());
        private final boolean isLocalDC;

        public StreamRateLimiter(InetAddressAndPort peer)
        {
            if (DatabaseDescriptor.getLocalDataCenter() != null && DatabaseDescriptor.getEndpointSnitch() != null)
                isLocalDC = DatabaseDescriptor.getLocalDataCenter().equals(
                            DatabaseDescriptor.getEndpointSnitch().getDatacenter(peer));
            else
                isLocalDC = true;
        }

        public void acquire(int toTransfer)
        {
            limiter.acquire(toTransfer);
            if (!isLocalDC)
                interDCLimiter.acquire(toTransfer);
        }

        public static void updateThroughput()
        {
            limiter.setRate(calculateRateInBytes());
        }

        public static void updateInterDCThroughput()
        {
            interDCLimiter.setRate(calculateInterDCRateInBytes());
        }

        private static double calculateRateInBytes()
        {
            return DatabaseDescriptor.getStreamThroughputOutboundMegabitsPerSec() > 0
                   ? DatabaseDescriptor.getStreamThroughputOutboundMegabitsPerSec() * BYTES_PER_MEGABIT
                   : Double.MAX_VALUE; // if throughput is set to 0 or negative value, throttling is disabled
        }

        private static double calculateInterDCRateInBytes()
        {
            return DatabaseDescriptor.getInterDCStreamThroughputOutboundMegabitsPerSec() > 0
                   ? DatabaseDescriptor.getInterDCStreamThroughputOutboundMegabitsPerSec() * BYTES_PER_MEGABIT
                   : Double.MAX_VALUE; // if throughput is set to 0 or negative value, throttling is disabled
        }

        @VisibleForTesting
        public static double getRateLimiterRateInBytes()
        {
            return limiter.getRate();
        }

        @VisibleForTesting
        public static double getInterDCRateLimiterRateInBytes()
        {
            return interDCLimiter.getRate();
        }
    }

    private final StreamEventJMXNotifier notifier = new StreamEventJMXNotifier();

    /*
     * Currently running streams. Removed after completion/failure.
     * We manage them in two different maps to distinguish plan from initiated ones to
     * receiving ones withing the same JVM.
     */
    private final Map<UUID, StreamResultFuture> initiatorStreams = new NonBlockingHashMap<>();
    private final Map<UUID, StreamResultFuture> followerStreams = new NonBlockingHashMap<>();

    public Set<CompositeData> getCurrentStreams()
    {
        return Sets.newHashSet(Iterables.transform(Iterables.concat(initiatorStreams.values(), followerStreams.values()), new Function<StreamResultFuture, CompositeData>()
        {
            public CompositeData apply(StreamResultFuture input)
            {
                return StreamStateCompositeData.toCompositeData(input.getCurrentState());
            }
        }));
    }

    public void registerInitiator(final StreamResultFuture result)
    {
        result.addEventListener(notifier);
        // Make sure we remove the stream on completion (whether successful or not)
        result.addListener(new Runnable()
        {
            public void run()
            {
                initiatorStreams.remove(result.planId);
            }
        }, MoreExecutors.directExecutor());

        initiatorStreams.put(result.planId, result);
    }

    public StreamResultFuture registerFollower(final StreamResultFuture result)
    {
        result.addEventListener(notifier);
        // Make sure we remove the stream on completion (whether successful or not)
        result.addListener(new Runnable()
        {
            public void run()
            {
                followerStreams.remove(result.planId);
            }
        }, MoreExecutors.directExecutor());

        StreamResultFuture previous = followerStreams.putIfAbsent(result.planId, result);
        return previous ==  null ? result : previous;
    }

    public StreamResultFuture getReceivingStream(UUID planId)
    {
        return followerStreams.get(planId);
    }

    public void addNotificationListener(NotificationListener listener, NotificationFilter filter, Object handback)
    {
        notifier.addNotificationListener(listener, filter, handback);
    }

    public void removeNotificationListener(NotificationListener listener) throws ListenerNotFoundException
    {
        notifier.removeNotificationListener(listener);
    }

    public void removeNotificationListener(NotificationListener listener, NotificationFilter filter, Object handback) throws ListenerNotFoundException
    {
        notifier.removeNotificationListener(listener, filter, handback);
    }

    public MBeanNotificationInfo[] getNotificationInfo()
    {
        return notifier.getNotificationInfo();
    }

    public StreamSession findSession(InetAddressAndPort peer, UUID planId, int sessionIndex, boolean searchInitiatorSessions)
    {
        Map<UUID, StreamResultFuture> streams = searchInitiatorSessions ? initiatorStreams : followerStreams;
        return findSession(streams, peer, planId, sessionIndex);
    }

    private StreamSession findSession(Map<UUID, StreamResultFuture> streams, InetAddressAndPort peer, UUID planId, int sessionIndex)
    {
        StreamResultFuture streamResultFuture = streams.get(planId);
        if (streamResultFuture == null)
            return null;

        return streamResultFuture.getSession(peer, sessionIndex);
    }
}
"
