{
    "performance_sensitive": "Yes",
    "config_name": "partitioner",
    "messages": [
        {
            "sender": "System",
            "content": "Initialize the analysis for the target system and configuration."
        },
        {
            "sender": "Developer",
            "content": "public static void persistLocalMetadata()\n    {\n        String req = \"INSERT INTO system.%s (\" +\n                     \"key,\" +\n                     \"cluster_name,\" +\n                     \"release_version,\" +\n                     \"cql_version,\" +\n                     \"native_protocol_version,\" +\n                     \"data_center,\" +\n                     \"rack,\" +\n                     \"partitioner,\" +\n                     \"rpc_address,\" +\n                     \"rpc_port,\" +\n                     \"broadcast_address,\" +\n                     \"broadcast_port,\" +\n                     \"listen_address,\" +\n                     \"listen_port\" +\n                     \") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\";\n        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n        executeOnceInternal(format(req, LOCAL),\n                            LOCAL,\n                            DatabaseDescriptor.getClusterName(),\n                            FBUtilities.getReleaseVersionString(),\n                            QueryProcessor.CQL_VERSION.toString(),\n                            String.valueOf(ProtocolVersion.CURRENT.asInt()),\n                            snitch.getLocalDatacenter(),\n                            snitch.getLocalRack(),\n                            DatabaseDescriptor.getPartitioner().getClass().getName(),\n                            DatabaseDescriptor.getRpcAddress(),\n                            DatabaseDescriptor.getNativeTransportPort(),\n                            FBUtilities.getJustBroadcastAddress(),\n                            DatabaseDescriptor.getStoragePort(),\n                            FBUtilities.getJustLocalAddress(),\n                            DatabaseDescriptor.getStoragePort());\n    }\n\n    \npublic static AbstractType<?> getInstance(TypeParser parser)\n    {\n        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();\n        Iterator<String> argIterator = parser.getKeyValueParameters().keySet().iterator();\n        if (argIterator.hasNext())\n        {\n            partitioner = FBUtilities.newPartitioner(argIterator.next());\n            assert !argIterator.hasNext();\n        }\n        return partitioner.partitionOrdering();\n    }\n\n    \n/**\n * A thread-safe and atomic Partition implementation.\n *\n * Operations (in particular addAll) on this implementation are atomic and\n * isolated (in the sense of ACID). Typically a addAll is guaranteed that no\n * other thread can see the state where only parts but not all rows have\n * been added.\n */\npublic final class AtomicBTreePartition extends AbstractBTreePartition\n{\n    public static final long EMPTY_SIZE = ObjectSizes.measure(new AtomicBTreePartition(null,\n                                                                                       DatabaseDescriptor.getPartitioner().decorateKey(ByteBuffer.allocate(1)),\n                                                                                       null));\n\n    // Reserved values for wasteTracker field. These values must not be consecutive (see avoidReservedValues)\n    private static final int TRACKER_NEVER_WASTED = 0;\n    private static final int TRACKER_PESSIMISTIC_LOCKING = Integer.MAX_VALUE;\n\n    // The granularity with which we track wasted allocation/work; we round up\n    private static final int ALLOCATION_GRANULARITY_BYTES = 1024;\n    // The number of bytes we have to waste in excess of our acceptable realtime rate of waste (defined below)\n    private static final long EXCESS_WASTE_BYTES = 10 * 1024 * 1024L;\n    private static final int EXCESS_WASTE_OFFSET = (int) (EXCESS_WASTE_BYTES / ALLOCATION_GRANULARITY_BYTES);\n    // Note this is a shift, because dividing a long time and then picking the low 32 bits doesn't give correct rollover behavior\n    private static final int CLOCK_SHIFT = 17;\n    // CLOCK_GRANULARITY = 1^9ns >> CLOCK_SHIFT == 132us == (1/7.63)ms\n\n    private static final AtomicIntegerFieldUpdater<AtomicBTreePartition> wasteTrackerUpdater = AtomicIntegerFieldUpdater.newUpdater(AtomicBTreePartition.class, \"wasteTracker\");\n    private static final AtomicReferenceFieldUpdater<AtomicBTreePartition, Holder> refUpdater = AtomicReferenceFieldUpdater.newUpdater(AtomicBTreePartition.class, Holder.class, \"ref\");\n\n    /**\n     * (clock + allocation) granularity are combined to give us an acceptable (waste) allocation rate that is defined by\n     * the passage of real time of ALLOCATION_GRANULARITY_BYTES/CLOCK_GRANULARITY, or in this case 7.63Kb/ms, or 7.45Mb/s\n     *\n     * in wasteTracker we maintain within EXCESS_WASTE_OFFSET before the current time; whenever we waste bytes\n     * we increment the current value if it is within this window, and set it to the min of the window plus our waste\n     * otherwise.\n     */\n    private volatile int wasteTracker = TRACKER_NEVER_WASTED;\n\n    private final MemtableAllocator allocator;\n    private volatile Holder ref;\n\n    private final TableMetadataRef metadata;\n\n    public AtomicBTreePartition(TableMetadataRef metadata, DecoratedKey partitionKey, MemtableAllocator allocator)\n    {\n        // involved in potential bug? partition columns may be a subset if we alter columns while it's in memtable\n        super(partitionKey);\n        this.metadata = metadata;\n        this.allocator = allocator;\n        this.ref = EMPTY;\n    }\n\n    protected Holder holder()\n    {\n        return ref;\n    }\n\n    public TableMetadata metadata()\n    {\n        return metadata.get();\n    }\n\n    protected boolean canHaveShadowedData()\n    {\n        return true;\n    }\n\n    private long[] addAllWithSizeDeltaInternal(RowUpdater updater, PartitionUpdate update, UpdateTransaction indexer)\n    {\n        Holder current = ref;\n        updater.reset();\n\n        if (!update.deletionInfo().getPartitionDeletion().isLive())\n            indexer.onPartitionDeletion(update.deletionInfo().getPartitionDeletion());\n\n        if (update.deletionInfo().hasRanges())\n            update.deletionInfo().rangeIterator(false).forEachRemaining(indexer::onRangeTombstone);\n\n        DeletionInfo deletionInfo;\n        if (update.deletionInfo().mayModify(current.deletionInfo))\n        {\n            if (updater.inputDeletionInfoCopy == null)\n                updater.inputDeletionInfoCopy = update.deletionInfo().clone(HeapCloner.instance);\n\n            deletionInfo = current.deletionInfo.mutableCopy().add(updater.inputDeletionInfoCopy);\n            updater.onAllocatedOnHeap(deletionInfo.unsharedHeapSize() - current.deletionInfo.unsharedHeapSize());\n        }\n        else\n        {\n            deletionInfo = current.deletionInfo;\n        }\n\n        RegularAndStaticColumns columns = update.columns().mergeTo(current.columns);\n        updater.onAllocatedOnHeap(columns.unsharedHeapSize() - current.columns.unsharedHeapSize());\n        Row newStatic = update.staticRow();\n        Row staticRow = newStatic.isEmpty()\n                        ? current.staticRow\n                        : (current.staticRow.isEmpty() ? updater.insert(newStatic) : updater.merge(current.staticRow, newStatic));\n        Object[] tree = BTree.update(current.tree, update.holder().tree, update.metadata().comparator, updater);\n        EncodingStats newStats = current.stats.mergeWith(update.stats());\n        updater.onAllocatedOnHeap(newStats.unsharedHeapSize() - current.stats.unsharedHeapSize());\n\n        if (tree != null && refUpdater.compareAndSet(this, current, new Holder(columns, tree, deletionInfo, staticRow, newStats)))\n        {\n            updater.finish();\n            return new long[]{ updater.dataSize, updater.colUpdateTimeDelta };\n        }\n        else\n        {\n            return null;\n        }\n    }\n    /**\n     * Adds a given update to this in-memtable partition.\n     *\n     * @return an array containing first the difference in size seen after merging the updates, and second the minimum\n     * time detla between updates.\n     */\n    public long[] addAllWithSizeDelta(final PartitionUpdate update,\n                                      Cloner cloner,\n                                      OpOrder.Group writeOp,\n                                      UpdateTransaction indexer)\n    {\n        RowUpdater updater = new RowUpdater(allocator, cloner, writeOp, indexer);\n        try\n        {\n            boolean shouldLock = shouldLock(writeOp);\n            indexer.start();\n\n            while (true)\n            {\n                if (shouldLock)\n                {\n                    synchronized (this)\n                    {\n                        long[] result = addAllWithSizeDeltaInternal(updater, update, indexer);\n                        if (result != null)\n                            return result;\n                    }\n                }\n                else\n                {\n                    long[] result = addAllWithSizeDeltaInternal(updater, update, indexer);\n                    if (result != null)\n                        return result;\n\n                    shouldLock = shouldLock(updater.heapSize, writeOp);\n                }\n            }\n        }\n        finally\n        {\n            indexer.commit();\n        }\n    }\n\n    @Override\n    public DeletionInfo deletionInfo()\n    {\n        return allocator.ensureOnHeap().applyToDeletionInfo(super.deletionInfo());\n    }\n\n    @Override\n    public Row staticRow()\n    {\n        return allocator.ensureOnHeap().applyToStatic(super.staticRow());\n    }\n\n    @Override\n    public DecoratedKey partitionKey()\n    {\n        return allocator.ensureOnHeap().applyToPartitionKey(super.partitionKey());\n    }\n\n    @Override\n    public Row getRow(Clustering<?> clustering)\n    {\n        return allocator.ensureOnHeap().applyToRow(super.getRow(clustering));\n    }\n\n    @Override\n    public Row lastRow()\n    {\n        return allocator.ensureOnHeap().applyToRow(super.lastRow());\n    }\n\n    @Override\n    public UnfilteredRowIterator unfilteredIterator(ColumnFilter selection, Slices slices, boolean reversed)\n    {\n        return allocator.ensureOnHeap().applyToPartition(super.unfilteredIterator(selection, slices, reversed));\n    }\n\n    @Override\n    public UnfilteredRowIterator unfilteredIterator(ColumnFilter selection, NavigableSet<Clustering<?>> clusteringsInQueryOrder, boolean reversed)\n    {\n        return allocator.ensureOnHeap().applyToPartition(super.unfilteredIterator(selection, clusteringsInQueryOrder, reversed));\n    }\n\n    @Override\n    public UnfilteredRowIterator unfilteredIterator()\n    {\n        return allocator.ensureOnHeap().applyToPartition(super.unfilteredIterator());\n    }\n\n    @Override\n    public UnfilteredRowIterator unfilteredIterator(Holder current, ColumnFilter selection, Slices slices, boolean reversed)\n    {\n        return allocator.ensureOnHeap().applyToPartition(super.unfilteredIterator(current, selection, slices, reversed));\n    }\n\n    @Override\n    public Iterator<Row> iterator()\n    {\n        return allocator.ensureOnHeap().applyToPartition(super.iterator());\n    }\n\n    private boolean shouldLock(OpOrder.Group writeOp)\n    {\n        if (!useLock())\n            return false;\n\n        return lockIfOldest(writeOp);\n    }\n\n    private boolean shouldLock(long addWaste, OpOrder.Group writeOp)\n    {\n        if (!updateWastedAllocationTracker(addWaste))\n            return false;\n\n        return lockIfOldest(writeOp);\n    }\n\n    private boolean lockIfOldest(OpOrder.Group writeOp)\n    {\n        if (!writeOp.isOldestLiveGroup())\n        {\n            Thread.yield();\n            return writeOp.isOldestLiveGroup();\n        }\n\n        return true;\n    }\n\n    public boolean useLock()\n    {\n        return wasteTracker == TRACKER_PESSIMISTIC_LOCKING;\n    }\n\n    /**\n     * Update the wasted allocation tracker state based on newly wasted allocation information\n     *\n     * @param wastedBytes the number of bytes wasted by this thread\n     * @return true if the caller should now proceed with pessimistic locking because the waste limit has been reached\n     */\n    private boolean updateWastedAllocationTracker(long wastedBytes)\n    {\n        // Early check for huge allocation that exceeds the limit\n        if (wastedBytes < EXCESS_WASTE_BYTES)\n        {\n            // We round up to ensure work < granularity are still accounted for\n            int wastedAllocation = ((int) (wastedBytes + ALLOCATION_GRANULARITY_BYTES - 1)) / ALLOCATION_GRANULARITY_BYTES;\n\n            int oldTrackerValue;\n            while (TRACKER_PESSIMISTIC_LOCKING != (oldTrackerValue = wasteTracker))\n            {\n                // Note this time value has an arbitrary offset, but is a constant rate 32 bit counter (that may wrap)\n                int time = (int) (System.nanoTime() >>> CLOCK_SHIFT);\n                int delta = oldTrackerValue - time;\n                if (oldTrackerValue == TRACKER_NEVER_WASTED || delta >= 0 || delta < -EXCESS_WASTE_OFFSET)\n                    delta = -EXCESS_WASTE_OFFSET;\n                delta += wastedAllocation;\n                if (delta >= 0)\n                    break;\n                if (wasteTrackerUpdater.compareAndSet(this, oldTrackerValue, avoidReservedValues(time + delta)))\n                    return false;\n            }\n        }\n        // We have definitely reached our waste limit so set the state if it isn't already\n        wasteTrackerUpdater.set(this, TRACKER_PESSIMISTIC_LOCKING);\n        // And tell the caller to proceed with pessimistic locking\n        return true;\n    }\n\n    private static int avoidReservedValues(int wasteTracker)\n    {\n        if (wasteTracker == TRACKER_NEVER_WASTED || wasteTracker == TRACKER_PESSIMISTIC_LOCKING)\n            return wasteTracker + 1;\n        return wasteTracker;\n    }\n\n    @VisibleForTesting\n    public void unsafeSetHolder(Holder holder)\n    {\n        ref = holder;\n    }\n\n    @VisibleForTesting\n    public Holder unsafeGetHolder()\n    {\n        return ref;\n    }\n\n    // the function we provide to the btree utilities to perform any column replacements\n    private static final class RowUpdater implements UpdateFunction<Row, Row>, ColumnData.PostReconciliationFunction\n    {\n        final MemtableAllocator allocator;\n        final OpOrder.Group writeOp;\n        final UpdateTransaction indexer;\n        final Cloner cloner;\n        long dataSize;\n        long heapSize;\n        long colUpdateTimeDelta = Long.MAX_VALUE;\n        List<Row> inserted; // TODO: replace with walk of aborted BTree\n\n        DeletionInfo inputDeletionInfoCopy = null;\n\n        private RowUpdater(MemtableAllocator allocator, Cloner cloner, OpOrder.Group writeOp, UpdateTransaction indexer)\n        {\n            this.allocator = allocator;\n            this.writeOp = writeOp;\n            this.indexer = indexer;\n            this.cloner = cloner;\n        }\n\n        @Override\n        public Row insert(Row insert)\n        {\n            Row data = insert.clone(cloner); \n            indexer.onInserted(insert);\n\n            this.dataSize += data.dataSize();\n            onAllocatedOnHeap(data.unsharedHeapSizeExcludingData());\n            if (inserted == null)\n                inserted = new ArrayList<>();\n            inserted.add(data);\n            return data;\n        }\n\n        public Row merge(Row existing, Row update)\n        {\n            Row reconciled = Rows.merge(existing, update, this);\n            indexer.onUpdated(existing, reconciled);\n\n            if (inserted == null)\n                inserted = new ArrayList<>();\n            inserted.add(reconciled);\n\n            return reconciled;\n        }\n\n        public Row retain(Row existing)\n        {\n            return existing;\n        }\n\n        protected void reset()\n        {\n            this.dataSize = 0;\n            this.heapSize = 0;\n            if (inserted != null)\n                inserted.clear();\n        }\n\n        public Cell<?> merge(Cell<?> previous, Cell<?> insert)\n        {\n            if (insert != previous)\n            {\n                long timeDelta = Math.abs(insert.timestamp() - previous.timestamp());\n                if (timeDelta < colUpdateTimeDelta)\n                    colUpdateTimeDelta = timeDelta;\n            }\n            if (cloner != null)\n                insert = cloner.clone(insert);\n            dataSize += insert.dataSize() - previous.dataSize();\n            heapSize += insert.unsharedHeapSizeExcludingData() - previous.unsharedHeapSizeExcludingData();\n            return insert;\n        }\n\n        public ColumnData insert(ColumnData insert)\n        {\n            if (cloner != null)\n                insert = insert.clone(cloner);\n            dataSize += insert.dataSize();\n            heapSize += insert.unsharedHeapSizeExcludingData();\n            return insert;\n        }\n\n        @Override\n        public void delete(ColumnData existing)\n        {\n            dataSize -= existing.dataSize();\n            heapSize -= existing.unsharedHeapSizeExcludingData();\n        }\n\n        public void onAllocatedOnHeap(long heapSize)\n        {\n            this.heapSize += heapSize;\n        }\n\n        protected void finish()\n        {\n            allocator.onHeap().adjust(heapSize, writeOp);\n        }\n    }\n}\n\nprivate synchronized void build()\n    {\n        if (isStopped)\n        {\n            logger.debug(\"Stopped build for view({}.{}) after covering {} keys\", ksName, view.name, keysBuilt);\n            return;\n        }\n\n        // Get the local ranges for which the view hasn't already been built nor it's building\n        RangesAtEndpoint replicatedRanges = StorageService.instance.getLocalReplicas(ksName);\n        Replicas.temporaryAssertFull(replicatedRanges);\n        Set<Range<Token>> newRanges = replicatedRanges.ranges()\n                                                      .stream()\n                                                      .map(r -> r.subtractAll(builtRanges))\n                                                      .flatMap(Set::stream)\n                                                      .map(r -> r.subtractAll(pendingRanges.keySet()))\n                                                      .flatMap(Set::stream)\n                                                      .collect(Collectors.toSet());\n        // If there are no new nor pending ranges we should finish the build\n        if (newRanges.isEmpty() && pendingRanges.isEmpty())\n        {\n            finish();\n            return;\n        }\n\n        // Split the new local ranges and add them to the pending set\n        DatabaseDescriptor.getPartitioner()\n                          .splitter()\n                          .map(s -> s.split(newRanges, NUM_TASKS))\n                          .orElse(newRanges)\n                          .forEach(r -> pendingRanges.put(r, Pair.<Token, Long>create(null, 0L)));\n\n        \nstatic boolean isTrivial(Range<Token> range)\n    {\n        IPartitioner partitioner = DatabaseDescriptor.getPartitioner();\n        if (partitioner.splitter().isPresent())\n        {\n            BigInteger l = partitioner.splitter().get().valueForToken(range.left);\n            BigInteger r = partitioner.splitter().get().valueForToken(range.right);\n            if (r.compareTo(l) <= 0)\n                return false;\n            if (r.subtract(l).compareTo(BigInteger.valueOf(TRIVIAL_RANGE_LIMIT)) < 0)\n                return true;\n        }\n        return false;\n    }\n\n    \n/**\n * Utility to write SSTables.\n * <p>\n * Typical usage looks like:\n * <pre>\n *   String type = CREATE TYPE myKs.myType (a int, b int)\";\n *   String schema = \"CREATE TABLE myKs.myTable (\"\n *                 + \"  k int PRIMARY KEY,\"\n *                 + \"  v1 text,\"\n *                 + \"  v2 int,\"\n *                 + \"  v3 myType,\"\n *                 + \")\";\n *   String insert = \"INSERT INTO myKs.myTable (k, v1, v2, v3) VALUES (?, ?, ?, ?)\";\n *\n *   // Creates a new writer. You need to provide at least the directory where to write the created sstable,\n *   // the schema for the sstable to write and a (prepared) insert statement to use. If you do not use the\n *   // default partitioner (Murmur3Partitioner), you will also need to provide the partitioner in use, see\n *   // CQLSSTableWriter.Builder for more details on the available options.\n *   CQLSSTableWriter writer = CQLSSTableWriter.builder()\n *                                             .inDirectory(\"path/to/directory\")\n *                                             .withType(type)\n *                                             .forTable(schema)\n *                                             .using(insert).build();\n *\n *   UserType myType = writer.getUDType(\"myType\");\n *   // Adds a nember of rows to the resulting sstable\n *   writer.addRow(0, \"test1\", 24, myType.newValue().setInt(\"a\", 10).setInt(\"b\", 20));\n *   writer.addRow(1, \"test2\", null, null);\n *   writer.addRow(2, \"test3\", 42, myType.newValue().setInt(\"a\", 30).setInt(\"b\", 40));\n *\n *   // Close the writer, finalizing the sstable\n *   writer.close();\n * </pre>\n *\n * Please note that {@code CQLSSTableWriter} is <b>not</b> thread-safe (multiple threads cannot access the\n * same instance). It is however safe to use multiple instances in parallel (even if those instance write\n * sstables for the same table).\n */\npublic class CQLSSTableWriter implements Closeable\n{\n    public static final ByteBuffer UNSET_VALUE = ByteBufferUtil.UNSET_BYTE_BUFFER;\n\n    static\n    {\n        DatabaseDescriptor.clientInitialization(false);\n        // Partitioner is not set in client mode.\n        if (DatabaseDescriptor.getPartitioner() == null)\n            DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance);\n    }\n\n    private final AbstractSSTableSimpleWriter writer;\n    private final UpdateStatement insert;\n    private final List<ColumnSpecification> boundNames;\n    private final List<TypeCodec> typeCodecs;\n\n    private CQLSSTableWriter(AbstractSSTableSimpleWriter writer, UpdateStatement insert, List<ColumnSpecification> boundNames)\n    {\n        this.writer = writer;\n        this.insert = insert;\n        this.boundNames = boundNames;\n        this.typeCodecs = boundNames.stream().map(bn ->  UDHelper.codecFor(UDHelper.driverType(bn.type)))\n                                             .collect(Collectors.toList());\n    }\n\n    /**\n     * Returns a new builder for a CQLSSTableWriter.\n     *\n     * @return the new builder.\n     */\n    public static Builder builder()\n    {\n        return new Builder();\n    }\n\n    /**\n     * Adds a new row to the writer.\n     * <p>\n     * This is a shortcut for {@code addRow(Arrays.asList(values))}.\n     *\n     * @param values the row values (corresponding to the bind variables of the\n     * insertion statement used when creating by this writer).\n     * @return this writer.\n     */\n    public CQLSSTableWriter addRow(Object... values)\n    throws InvalidRequestException, IOException\n    {\n        return addRow(Arrays.asList(values));\n    }\n\n    /**\n     * Adds a new row to the writer.\n     * <p>\n     * Each provided value type should correspond to the types of the CQL column\n     * the value is for. The correspondance between java type and CQL type is the\n     * same one than the one documented at\n     * www.datastax.com/drivers/java/2.0/apidocs/com/datastax/driver/core/DataType.Name.html#asJavaClass().\n     * <p>\n     * If you prefer providing the values directly as binary, use\n     * {@link #rawAddRow} instead.\n     *\n     * @param values the row values (corresponding to the bind variables of the\n     * insertion statement used when creating by this writer).\n     * @return this writer.\n     */\n    public CQLSSTableWriter addRow(List<Object> values)\n    throws InvalidRequestException, IOException\n    {\n        int size = Math.min(values.size(), boundNames.size());\n        List<ByteBuffer> rawValues = new ArrayList<>(size);\n\n        for (int i = 0; i < size; i++)\n        {\n            Object value = values.get(i);\n            rawValues.add(serialize(value, typeCodecs.get(i), boundNames.get(i)));\n        }\n\n        return rawAddRow(rawValues);\n    }\n\n    /**\n     * Adds a new row to the writer.\n     * <p>\n     * This is equivalent to the other addRow methods, but takes a map whose\n     * keys are the names of the columns to add instead of taking a list of the\n     * values in the order of the insert statement used during construction of\n     * this write.\n     * <p>\n     * Please note that the column names in the map keys must be in lowercase unless\n     * the declared column name is a\n     * <a href=\"http://cassandra.apache.org/doc/cql3/CQL.html#identifiers\">case-sensitive quoted identifier</a>\n     * (in which case the map key must use the exact case of the column).\n     *\n     * @param values a map of colum name to column values representing the new\n     * row to add. Note that if a column is not part of the map, it's value will\n     * be {@code null}. If the map contains keys that does not correspond to one\n     * of the column of the insert statement used when creating this writer, the\n     * the corresponding value is ignored.\n     * @return this writer.\n     */\n    public CQLSSTableWriter addRow(Map<String, Object> values)\n    throws InvalidRequestException, IOException\n    {\n        int size = boundNames.size();\n        List<ByteBuffer> rawValues = new ArrayList<>(size);\n        for (int i = 0; i < size; i++)\n        {\n            ColumnSpecification spec = boundNames.get(i);\n            Object value = values.get(spec.name.toString());\n            rawValues.add(serialize(value, typeCodecs.get(i), boundNames.get(i)));\n        }\n        return rawAddRow(rawValues);\n    }\n\n    /**\n     * Adds a new row to the writer given already serialized values.\n     *\n     * @param values the row values (corresponding to the bind variables of the\n     * insertion statement used when creating by this writer) as binary.\n     * @return this writer.\n     */\n    public CQLSSTableWriter rawAddRow(ByteBuffer... values)\n    throws InvalidRequestException, IOException\n    {\n        return rawAddRow(Arrays.asList(values));\n    }\n\n    /**\n     * Adds a new row to the writer given already serialized values.\n     * <p>\n     * This is a shortcut for {@code rawAddRow(Arrays.asList(values))}.\n     *\n     * @param values the row values (corresponding to the bind variables of the\n     * insertion statement used when creating by this writer) as binary.\n     * @return this writer.\n     */\n    public CQLSSTableWriter rawAddRow(List<ByteBuffer> values)\n    throws InvalidRequestException, IOException\n    {\n        if (values.size() != boundNames.size())\n            throw new InvalidRequestException(String.format(\"Invalid number of arguments, expecting %d values but got %d\", boundNames.size(), values.size()));\n\n        QueryOptions options = QueryOptions.forInternalCalls(null, values);\n        List<ByteBuffer> keys = insert.buildPartitionKeyNames(options);\n        SortedSet<Clustering<?>> clusterings = insert.createClustering(options);\n\n        long now = System.currentTimeMillis();\n        // Note that we asks indexes to not validate values (the last 'false' arg below) because that triggers a 'Keyspace.open'\n        // and that forces a lot of initialization that we don't want.\n        UpdateParameters params = new UpdateParameters(insert.metadata,\n                                                       insert.updatedColumns(),\n                                                       options,\n                                                       insert.getTimestamp(TimeUnit.MILLISECONDS.toMicros(now), options),\n                                                       (int) TimeUnit.MILLISECONDS.toSeconds(now),\n                                                       insert.getTimeToLive(options),\n                                                       Collections.emptyMap());\n\n        try\n        {\n            for (ByteBuffer key : keys)\n            {\n                for (Clustering<?> clustering : clusterings)\n                    insert.addUpdateForKey(writer.getUpdateFor(key), clustering, params);\n            }\n            return this;\n        }\n        catch (SSTableSimpleUnsortedWriter.SyncException e)\n        {\n            // If we use a BufferedWriter and had a problem writing to disk, the IOException has been\n            // wrapped in a SyncException (see BufferedWriter below). We want to extract that IOE.\n            throw (IOException)e.getCause();\n        }\n    }\n\n    /**\n     * Adds a new row to the writer given already serialized values.\n     * <p>\n     * This is equivalent to the other rawAddRow methods, but takes a map whose\n     * keys are the names of the columns to add instead of taking a list of the\n     * values in the order of the insert statement used during construction of\n     * this write.\n     *\n     * @param values a map of colum name to column values representing the new\n     * row to add. Note that if a column is not part of the map, it's value will\n     * be {@code null}. If the map contains keys that does not correspond to one\n     * of the column of the insert statement used when creating this writer, the\n     * the corresponding value is ignored.\n     * @return this writer.\n     */\n    public CQLSSTableWriter rawAddRow(Map<String, ByteBuffer> values)\n    throws InvalidRequestException, IOException\n    {\n        int size = Math.min(values.size(), boundNames.size());\n        List<ByteBuffer> rawValues = new ArrayList<>(size);\n        for (int i = 0; i < size; i++)\n        {\n            ColumnSpecification spec = boundNames.get(i);\n            rawValues.add(values.get(spec.name.toString()));\n        }\n        return rawAddRow(rawValues);\n    }\n\n    /**\n     * Returns the User Defined type, used in this SSTable Writer, that can\n     * be used to create UDTValue instances.\n     *\n     * @param dataType name of the User Defined type\n     * @return user defined type\n     */\n    public UserType getUDType(String dataType)\n    {\n        KeyspaceMetadata ksm = Schema.instance.getKeyspaceMetadata(insert.keyspace());\n        org.apache.cassandra.db.marshal.UserType userType = ksm.types.getNullable(ByteBufferUtil.bytes(dataType));\n        return (UserType) UDHelper.driverType(userType);\n    }\n\n    /**\n     * Close this writer.\n     * <p>\n     * This method should be called, otherwise the produced sstables are not\n     * guaranteed to be complete (and won't be in practice).\n     */\n    public void close() throws IOException\n    {\n        writer.close();\n    }\n\n    private ByteBuffer serialize(Object value, TypeCodec codec, ColumnSpecification columnSpecification)\n    {\n        if (value == null || value == UNSET_VALUE)\n            return (ByteBuffer) value;\n\n        try\n        {\n            return codec.serialize(value, ProtocolVersion.CURRENT);\n        }\n        catch (ClassCastException cce)\n        {\n            // For backwards-compatibility with consumers that may be passing\n            // an Integer for a Date field, for example.\n            return ((AbstractType)columnSpecification.type).decompose(value);\n        }\n    }\n    /**\n     * A Builder for a CQLSSTableWriter object.\n     */\n    public static class Builder\n    {\n        private File directory;\n\n        protected SSTableFormat.Type formatType = null;\n\n        private CreateTableStatement.Raw schemaStatement;\n        private final List<CreateTypeStatement.Raw> typeStatements;\n        private ModificationStatement.Parsed insertStatement;\n        private IPartitioner partitioner;\n\n        private boolean sorted = false;\n        private long bufferSizeInMB = 128;\n\n        protected Builder() {\n            this.typeStatements = new ArrayList<>();\n        }\n\n        /**\n         * The directory where to write the sstables.\n         * <p>\n         * This is a mandatory option.\n         *\n         * @param directory the directory to use, which should exists and be writable.\n         * @return this builder.\n         *\n         * @throws IllegalArgumentException if {@code directory} doesn't exist or is not writable.\n         */\n        public Builder inDirectory(String directory)\n        {\n            return inDirectory(new File(directory));\n        }\n\n        /**\n         * The directory where to write the sstables (mandatory option).\n         * <p>\n         * This is a mandatory option.\n         *\n         * @param directory the directory to use, which should exists and be writable.\n         * @return this builder.\n         *\n         * @throws IllegalArgumentException if {@code directory} doesn't exist or is not writable.\n         */\n        public Builder inDirectory(File directory)\n        {\n            if (!directory.exists())\n                throw new IllegalArgumentException(directory + \" doesn't exists\");\n            if (!directory.canWrite())\n                throw new IllegalArgumentException(directory + \" exists but is not writable\");\n\n            this.directory = directory;\n            return this;\n        }\n\n        public Builder withType(String typeDefinition) throws SyntaxException\n        {\n            typeStatements.add(QueryProcessor.parseStatement(typeDefinition, CreateTypeStatement.Raw.class, \"CREATE TYPE\"));\n            return this;\n        }\n\n        /**\n         * The schema (CREATE TABLE statement) for the table for which sstable are to be created.\n         * <p>\n         * Please note that the provided CREATE TABLE statement <b>must</b> use a fully-qualified\n         * table name, one that include the keyspace name.\n         * <p>\n         * This is a mandatory option.\n         *\n         * @param schema the schema of the table for which sstables are to be created.\n         * @return this builder.\n         *\n         * @throws IllegalArgumentException if {@code schema} is not a valid CREATE TABLE statement\n         * or does not have a fully-qualified table name.\n         */\n        public Builder forTable(String schema)\n        {\n            this.schemaStatement = QueryProcessor.parseStatement(schema, CreateTableStatement.Raw.class, \"CREATE TABLE\");\n            return this;\n        }\n\n        /**\n         * The partitioner to use.\n         * <p>\n         * By default, {@code Murmur3Partitioner} will be used. If this is not the partitioner used\n         * by the cluster for which the SSTables are created, you need to use this method to\n         * provide the correct partitioner.\n         *\n         * @param partitioner the partitioner to use.\n         * @return this builder.\n         */\n        public Builder withPartitioner(IPartitioner partitioner)\n        {\n            this.partitioner = partitioner;\n            return this;\n        }\n\n        /**\n         * The INSERT or UPDATE statement defining the order of the values to add for a given CQL row.\n         * <p>\n         * Please note that the provided INSERT statement <b>must</b> use a fully-qualified\n         * table name, one that include the keyspace name. Moreover, said statement must use\n         * bind variables since these variables will be bound to values by the resulting writer.\n         * <p>\n         * This is a mandatory option.\n         *\n         * @param insert an insertion statement that defines the order\n         * of column values to use.\n         * @return this builder.\n         *\n         * @throws IllegalArgumentException if {@code insertStatement} is not a valid insertion\n         * statement, does not have a fully-qualified table name or have no bind variables.\n         */\n        public Builder using(String insert)\n        {\n            this.insertStatement = QueryProcessor.parseStatement(insert, ModificationStatement.Parsed.class, \"INSERT/UPDATE\");\n            return this;\n        }\n\n        /**\n         * The size of the buffer to use.\n         * <p>\n         * This defines how much data will be buffered before being written as\n         * a new SSTable. This correspond roughly to the data size that will have the created\n         * sstable.\n         * <p>\n         * The default is 128MB, which should be reasonable for a 1GB heap. If you experience\n         * OOM while using the writer, you should lower this value.\n         *\n         * @param size the size to use in MB.\n         * @return this builder.\n         */\n        public Builder withBufferSizeInMB(int size)\n        {\n            this.bufferSizeInMB = size;\n            return this;\n        }\n\n        /**\n         * Creates a CQLSSTableWriter that expects sorted inputs.\n         * <p>\n         * If this option is used, the resulting writer will expect rows to be\n         * added in SSTable sorted order (and an exception will be thrown if that\n         * is not the case during insertion). The SSTable sorted order means that\n         * rows are added such that their partition key respect the partitioner\n         * order.\n         * <p>\n         * You should thus only use this option is you know that you can provide\n         * the rows in order, which is rarely the case. If you can provide the\n         * rows in order however, using this sorted might be more efficient.\n         * <p>\n         * Note that if used, some option like withBufferSizeInMB will be ignored.\n         *\n         * @return this builder.\n         */\n        public Builder sorted()\n        {\n            this.sorted = true;\n            return this;\n        }\n\n        @SuppressWarnings(\"resource\")\n        public CQLSSTableWriter build()\n        {\n            if (directory == null)\n                throw new IllegalStateException(\"No ouptut directory specified, you should provide a directory with inDirectory()\");\n            if (schemaStatement == null)\n                throw new IllegalStateException(\"Missing schema, you should provide the schema for the SSTable to create with forTable()\");\n            if (insertStatement == null)\n                throw new IllegalStateException(\"No insert statement specified, you should provide an insert statement through using()\");\n\n            synchronized (CQLSSTableWriter.class)\n            {\n                if (Schema.instance.getKeyspaceMetadata(SchemaConstants.SCHEMA_KEYSPACE_NAME) == null)\n                    Schema.instance.load(Schema.getSystemKeyspaceMetadata());\n                if (Schema.instance.getKeyspaceMetadata(SchemaConstants.SYSTEM_KEYSPACE_NAME) == null)\n                    Schema.instance.load(SystemKeyspace.metadata());\n\n                String keyspaceName = schemaStatement.keyspace();\n\n                if (Schema.instance.getKeyspaceMetadata(keyspaceName) == null)\n                {\n                    Schema.instance.load(KeyspaceMetadata.create(keyspaceName,\n                                                                 KeyspaceParams.simple(1),\n                                                                 Tables.none(),\n                                                                 Views.none(),\n                                                                 Types.none(),\n                                                                 Functions.none()));\n                }\n\n                KeyspaceMetadata ksm = Schema.instance.getKeyspaceMetadata(keyspaceName);\n\n                TableMetadata tableMetadata = ksm.tables.getNullable(schemaStatement.table());\n                if (tableMetadata == null)\n                {\n                    Types types = createTypes(keyspaceName);\n                    tableMetadata = createTable(types);\n                    Schema.instance.load(ksm.withSwapped(ksm.tables.with(tableMetadata)).withSwapped(types));\n                }\n\n                UpdateStatement preparedInsert = prepareInsert();\n\n                TableMetadataRef ref = TableMetadataRef.forOfflineTools(tableMetadata);\n                AbstractSSTableSimpleWriter writer = sorted\n                                                   ? new SSTableSimpleWriter(directory, ref, preparedInsert.updatedColumns())\n                                                   : new SSTableSimpleUnsortedWriter(directory, ref, preparedInsert.updatedColumns(), bufferSizeInMB);\n\n                if (formatType != null)\n                    writer.setSSTableFormatType(formatType);\n\n                return new CQLSSTableWriter(writer, preparedInsert, preparedInsert.getBindVariables());\n            }\n        }\n\n        private Types createTypes(String keyspace)\n        {\n            Types.RawBuilder builder = Types.rawBuilder(keyspace);\n            for (CreateTypeStatement.Raw st : typeStatements)\n                st.addToRawBuilder(builder);\n            return builder.build();\n        }\n\n        /**\n         * Creates the table according to schema statement\n         *\n         * @param types types this table should be created with\n         */\n        private TableMetadata createTable(Types types)\n        {\n            ClientState state = ClientState.forInternalCalls();\n            CreateTableStatement statement = schemaStatement.prepare(state);\n            statement.validate(ClientState.forInternalCalls());\n\n            TableMetadata.Builder builder = statement.builder(types);\n            if (partitioner != null)\n                builder.partitioner(partitioner);\n\n            return builder.build();\n        }\n\n        /**\n         * Prepares insert statement for writing data to SSTable\n         *\n         * @return prepared Insert statement and it's bound names\n         */\n        private UpdateStatement prepareInsert()\n        {\n            ClientState state = ClientState.forInternalCalls();\n            UpdateStatement insert = (UpdateStatement) insertStatement.prepare(state);\n            insert.validate(state);\n\n            if (insert.hasConditions())\n                throw new IllegalArgumentException(\"Conditional statements are not supported\");\n            if (insert.isCounter())\n                throw new IllegalArgumentException(\"Counter update statements are not supported\");\n            if (insert.getBindVariables().isEmpty())\n                throw new IllegalArgumentException(\"Provided insert statement has no bind variables\");\n\n            return insert;\n        }\n    }\n}\n\npublic LocalStrategy(String keyspaceName, TokenMetadata tokenMetadata, IEndpointSnitch snitch, Map<String, String> configOptions)\n    {\n        super(keyspaceName, tokenMetadata, snitch, configOptions);\n        replicas = EndpointsForRange.of(\n                new Replica(FBUtilities.getBroadcastAddressAndPort(),\n                        DatabaseDescriptor.getPartitioner().getMinimumToken(),\n                        DatabaseDescriptor.getPartitioner().getMinimumToken(),\n                        true\n                )\n        );\n    }\n\n    \npublic static ReplicaPlan.ForTokenWrite forLocalBatchlogWrite()\n    {\n        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();\n        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);\n        Replica localSystemReplica = SystemReplicas.getSystemReplica(FBUtilities.getBroadcastAddressAndPort());\n\n        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(\n                systemKeypsace.getReplicationStrategy(),\n                EndpointsForToken.of(token, localSystemReplica),\n                EndpointsForToken.empty(token)\n        );\n        return forWrite(systemKeypsace, ConsistencyLevel.ONE, liveAndDown, liveAndDown, writeAll);\n    }\n\n    \n/**\n     * Requires that the provided endpoints are alive.  Converts them to their relevant system replicas.\n     * Note that the liveAndDown collection and live are equal to the provided endpoints.\n     *\n     * @param isAny if batch consistency level is ANY, in which case a local node will be picked\n     */\npublic static ReplicaPlan.ForTokenWrite forBatchlogWrite(boolean isAny) throws UnavailableException\n    {\n        // A single case we write not for range or token, but multiple mutations to many tokens\n        Token token = DatabaseDescriptor.getPartitioner().getMinimumToken();\n\n        TokenMetadata.Topology topology = StorageService.instance.getTokenMetadata().cachedOnlyTokenMap().getTopology();\n        IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();\n        Multimap<String, InetAddressAndPort> localEndpoints = HashMultimap.create(topology.getDatacenterRacks()\n                                                                                          .get(snitch.getLocalDatacenter()));\n        // Replicas are picked manually:\n        //  - replicas should be alive according to the failure detector\n        //  - replicas should be in the local datacenter\n        //  - choose min(2, number of qualifying candiates above)\n        //  - allow the local node to be the only replica only if it's a single-node DC\n        Collection<InetAddressAndPort> chosenEndpoints = filterBatchlogEndpoints(snitch.getLocalRack(), localEndpoints);\n\n        if (chosenEndpoints.isEmpty() && isAny)\n            chosenEndpoints = Collections.singleton(FBUtilities.getBroadcastAddressAndPort());\n\n        Keyspace systemKeypsace = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME);\n        ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWrite(\n                systemKeypsace.getReplicationStrategy(),\n                SystemReplicas.getSystemReplicas(chosenEndpoints).forToken(token),\n                EndpointsForToken.empty(token)\n        );\n        // Batchlog is hosted by either one node or two nodes from different racks.\n        ConsistencyLevel consistencyLevel = liveAndDown.all().size() == 1 ? ConsistencyLevel.ONE : ConsistencyLevel.TWO;\n        // assume that we have already been given live endpoints, and skip applying the failure detector\n        return forWrite(systemKeypsace, consistencyLevel, liveAndDown, liveAndDown, writeAll);\n    }\n\n    \npublic class SystemReplicas\n{\n    private static final Map<InetAddressAndPort, Replica> systemReplicas = new ConcurrentHashMap<>();\n    public static final Range<Token> FULL_RANGE = new Range<>(DatabaseDescriptor.getPartitioner().getMinimumToken(),\n                                                              DatabaseDescriptor.getPartitioner().getMinimumToken());\n\n    private static Replica createSystemReplica(InetAddressAndPort endpoint)\n    {\n        return new Replica(endpoint, FULL_RANGE, true);\n    }\n\n    /**\n     * There are a few places where a system function borrows write path functionality, but doesn't otherwise\n     * fit into normal replication strategies (ie: hints and batchlog). So here we provide a replica instance\n     */\n    public static Replica getSystemReplica(InetAddressAndPort endpoint)\n    {\n        return systemReplicas.computeIfAbsent(endpoint, SystemReplicas::createSystemReplica);\n    }\n\n    public static EndpointsForRange getSystemReplicas(Collection<InetAddressAndPort> endpoints)\n    {\n        if (endpoints.isEmpty())\n            return EndpointsForRange.empty(FULL_RANGE);\n\n        return EndpointsForRange.copyOf(Collections2.transform(endpoints, SystemReplicas::getSystemReplica));\n    }\n}\n\npublic TokenMetadata()\n    {\n        this(SortedBiMultiValMap.create(),\n             HashBiMap.create(),\n             Topology.empty(),\n             DatabaseDescriptor.getPartitioner());\n    }\n\n    \npublic TokenMetadata(IEndpointSnitch snitch)\n    {\n        this(SortedBiMultiValMap.create(),\n             HashBiMap.create(),\n             Topology.builder(() -> snitch).build(),\n             DatabaseDescriptor.getPartitioner());\n    }\n\n    \nprivate TokenMetadata(BiMultiValMap<Token, InetAddressAndPort> tokenToEndpointMap, BiMap<InetAddressAndPort, UUID> endpointsMap, Topology topology, IPartitioner partitioner)\n    {\n        this(tokenToEndpointMap, endpointsMap, topology, partitioner, 0);\n    }\n\n    \nprivate TokenMetadata(BiMultiValMap<Token, InetAddressAndPort> tokenToEndpointMap, BiMap<InetAddressAndPort, UUID> endpointsMap, Topology topology, IPartitioner partitioner, long ringVersion)\n    {\n        this.tokenToEndpointMap = tokenToEndpointMap;\n        this.topology = topology;\n        this.partitioner = partitioner;\n        endpointToHostIdMap = endpointsMap;\n        sortedTokens = sortTokens();\n        this.ringVersion = ringVersion;\n    }\n\n    \npublic TokenMetadata(IEndpointSnitch snitch)\n    {\n        this(SortedBiMultiValMap.create(),\n             HashBiMap.create(),\n             Topology.builder(() -> snitch).build(),\n             DatabaseDescriptor.getPartitioner());\n    }\n\n    \nprivate static Range<Token> deserializeRange(ByteBuffer bb)\n    {\n        try (DataInputBuffer in = new DataInputBuffer(bb, false))\n        {\n            IPartitioner partitioner = DatabaseDescriptor.getPartitioner();\n            Token left = Token.serializer.deserialize(in, partitioner, 0);\n            Token right = Token.serializer.deserialize(in, partitioner, 0);\n            return new Range<>(left, right);\n        }\n        catch (IOException e)\n        {\n            throw new RuntimeException(e);\n        }\n    }\n\n    \npublic TableMetadata build()\n        {\n            if (partitioner == null)\n                partitioner = DatabaseDescriptor.getPartitioner();\n\n            if (id == null)\n                id = TableId.generate();\n\n            if (Flag.isCQLTable(flags))\n                return new TableMetadata(this);\n            else\n                return new CompactTableMetadata(this);\n        }\n\n        \n@Override\n    public List<Map<String, String>> getSessions(boolean all, String rangesStr)\n    {\n        Set<Range<Token>> ranges = RepairOption.parseRanges(rangesStr, DatabaseDescriptor.getPartitioner());\n        return consistent.local.sessionInfo(all, ranges);\n    }\n\n    \npublic List<CompositeData> getRepairStats(List<String> schemaArgs, String rangeString)\n    {\n        List<CompositeData> stats = new ArrayList<>();\n        Collection<Range<Token>> userRanges = rangeString != null\n                                              ? RepairOption.parseRanges(rangeString, DatabaseDescriptor.getPartitioner())\n                                              : null;\n\n        for (ColumnFamilyStore cfs : SchemaArgsParser.parse(schemaArgs))\n        {\n            String keyspace = cfs.keyspace.getName();\n            Collection<Range<Token>> ranges = userRanges != null\n                                              ? userRanges\n                                              : StorageService.instance.getLocalReplicas(keyspace).ranges();\n            RepairedState.Stats cfStats = consistent.local.getRepairedStats(cfs.metadata().id, ranges);\n            stats.add(RepairStats.fromRepairState(keyspace, cfs.name, cfStats).toComposite());\n        }\n\n        return stats;\n    }\n\n    \n@Override\n    public List<CompositeData> getPendingStats(List<String> schemaArgs, String rangeString)\n    {\n        List<CompositeData> stats = new ArrayList<>();\n        Collection<Range<Token>> userRanges = rangeString != null\n                                              ? RepairOption.parseRanges(rangeString, DatabaseDescriptor.getPartitioner())\n                                              : null;\n        for (ColumnFamilyStore cfs : SchemaArgsParser.parse(schemaArgs))\n        {\n            String keyspace = cfs.keyspace.getName();\n            Collection<Range<Token>> ranges = userRanges != null\n                                              ? userRanges\n                                              : StorageService.instance.getLocalReplicas(keyspace).ranges();\n            PendingStats cfStats = consistent.local.getPendingStats(cfs.metadata().id, ranges);\n            stats.add(cfStats.toComposite());\n        }\n\n        return stats;\n    }\n\n    \n@Override\n    public List<CompositeData> cleanupPending(List<String> schemaArgs, String rangeString, boolean force)\n    {\n        List<CompositeData> stats = new ArrayList<>();\n        Collection<Range<Token>> userRanges = rangeString != null\n                                              ? RepairOption.parseRanges(rangeString, DatabaseDescriptor.getPartitioner())\n                                              : null;\n        for (ColumnFamilyStore cfs : SchemaArgsParser.parse(schemaArgs))\n        {\n            String keyspace = cfs.keyspace.getName();\n            Collection<Range<Token>> ranges = userRanges != null\n                                              ? userRanges\n                                              : StorageService.instance.getLocalReplicas(keyspace).ranges();\n            CleanupSummary summary = consistent.local.cleanup(cfs.metadata().id, ranges, force);\n            stats.add(summary.toComposite());\n        }\n        return stats;\n    }\n\n    "
        },
        {
            "sender": "PerformanceSensitivity",
            "content": "The configuration is Performance Sensitive: The partitioner is critical for determining how data is distributed across nodes in a distributed database system. Its configuration directly affects the performance of data retrieval and storage operations, as it influences data locality, load balancing, and the efficiency of queries. A poorly chosen partitioner can lead to uneven data distribution, increased latency, and memory overhead, significantly impacting overall system performance. Thus, any changes to the partitioner configuration can result in substantial performance variations due to its foundational role in the system's architecture."
        }
    ]
}