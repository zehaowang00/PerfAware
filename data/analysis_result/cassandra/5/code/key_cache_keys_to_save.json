{
    "performance_sensitive": "Yes",
    "config_name": "key_cache_keys_to_save",
    "messages": [
        {
            "sender": "System",
            "content": "Initialize the analysis for the target system and configuration."
        },
        {
            "sender": "Developer",
            "content": "/**\n     * @return auto saving cache object\n     */\nprivate AutoSavingCache<KeyCacheKey, RowIndexEntry> initKeyCache()\n    {\n        logger.info(\"Initializing key cache with capacity of {} MBs.\", DatabaseDescriptor.getKeyCacheSizeInMB());\n\n        long keyCacheInMemoryCapacity = DatabaseDescriptor.getKeyCacheSizeInMB() * 1024 * 1024;\n\n        // as values are constant size we can use singleton weigher\n        // where 48 = 40 bytes (average size of the key) + 8 bytes (size of value)\n        ICache<KeyCacheKey, RowIndexEntry> kc;\n        kc = CaffeineCache.create(keyCacheInMemoryCapacity);\n        AutoSavingCache<KeyCacheKey, RowIndexEntry> keyCache = new AutoSavingCache<>(kc, CacheType.KEY_CACHE, new KeyCacheSerializer());\n\n        int keyCacheKeysToSave = DatabaseDescriptor.getKeyCacheKeysToSave();\n\n        keyCache.scheduleSaving(DatabaseDescriptor.getKeyCacheSavePeriod(), keyCacheKeysToSave);\n\n        return keyCache;\n    }\n\n    \npublic void setKeyCacheSavePeriodInSeconds(int seconds)\n    {\n        if (seconds < 0)\n            throw new RuntimeException(\"KeyCacheSavePeriodInSeconds must be non-negative.\");\n\n        DatabaseDescriptor.setKeyCacheSavePeriod(seconds);\n        keyCache.scheduleSaving(seconds, DatabaseDescriptor.getKeyCacheKeysToSave());\n    }\n\n    \npublic int getKeyCacheKeysToSave()\n    {\n        return DatabaseDescriptor.getKeyCacheKeysToSave();\n    }\n\n    \npublic void saveCaches() throws ExecutionException, InterruptedException\n    {\n        List<Future<?>> futures = new ArrayList<>(3);\n        logger.debug(\"submitting cache saves\");\n\n        futures.add(keyCache.submitWrite(DatabaseDescriptor.getKeyCacheKeysToSave()));\n        futures.add(rowCache.submitWrite(DatabaseDescriptor.getRowCacheKeysToSave()));\n        futures.add(counterCache.submitWrite(DatabaseDescriptor.getCounterCacheKeysToSave()));\n\n        FBUtilities.waitOnFutures(futures);\n        logger.debug(\"cache saves completed\");\n    }\n\n    \nprivate static void logStatus()\n    {\n        // everything from o.a.c.concurrent\n        logger.info(String.format(\"%-28s%10s%10s%15s%10s%18s\", \"Pool Name\", \"Active\", \"Pending\", \"Completed\", \"Blocked\", \"All Time Blocked\"));\n\n        for (ThreadPoolMetrics tpool : CassandraMetricsRegistry.Metrics.allThreadPoolMetrics())\n        {\n            logger.info(String.format(\"%-28s%10s%10s%15s%10s%18s\",\n                                      tpool.poolName,\n                                      tpool.activeTasks.getValue(),\n                                      tpool.pendingTasks.getValue(),\n                                      tpool.completedTasks.getValue(),\n                                      tpool.currentBlocked.getCount(),\n                                      tpool.totalBlocked.getCount()));\n        }\n\n        // one offs\n        logger.info(String.format(\"%-25s%10s%10s\",\n                                  \"CompactionManager\", CompactionManager.instance.getActiveCompactions(), CompactionManager.instance.getPendingTasks()));\n        int pendingLargeMessages = 0;\n        for (int n : MessagingService.instance().getLargeMessagePendingTasks().values())\n        {\n            pendingLargeMessages += n;\n        }\n        int pendingSmallMessages = 0;\n        for (int n : MessagingService.instance().getSmallMessagePendingTasks().values())\n        {\n            pendingSmallMessages += n;\n        }\n        logger.info(String.format(\"%-25s%10s%10s\",\n                                  \"MessagingService\", \"n/a\", pendingLargeMessages + \"/\" + pendingSmallMessages));\n\n        // Global key/row cache information\n        AutoSavingCache<KeyCacheKey, RowIndexEntry> keyCache = CacheService.instance.keyCache;\n        AutoSavingCache<RowCacheKey, IRowCacheEntry> rowCache = CacheService.instance.rowCache;\n\n        int keyCacheKeysToSave = DatabaseDescriptor.getKeyCacheKeysToSave();\n        int rowCacheKeysToSave = DatabaseDescriptor.getRowCacheKeysToSave();\n\n        logger.info(String.format(\"%-25s%10s%25s%25s\",\n                                  \"Cache Type\", \"Size\", \"Capacity\", \"KeysToSave\"));\n        logger.info(String.format(\"%-25s%10s%25s%25s\",\n                                  \"KeyCache\",\n                                  keyCache.weightedSize(),\n                                  keyCache.getCapacity(),\n                                  keyCacheKeysToSave == Integer.MAX_VALUE ? \"all\" : keyCacheKeysToSave));\n\n        logger.info(String.format(\"%-25s%10s%25s%25s\",\n                                  \"RowCache\",\n                                  rowCache.weightedSize(),\n                                  rowCache.getCapacity(),\n                                  rowCacheKeysToSave == Integer.MAX_VALUE ? \"all\" : rowCacheKeysToSave));\n\n        // per-CF stats\n        logger.info(String.format(\"%-25s%20s\", \"Table\", \"Memtable ops,data\"));\n        for (ColumnFamilyStore cfs : ColumnFamilyStore.all())\n        {\n            logger.info(String.format(\"%-25s%20s\",\n                                      cfs.keyspace.getName() + \".\" + cfs.name,\n                                      cfs.metric.memtableColumnsCount.getValue() + \",\" + cfs.metric.memtableLiveDataSize.getValue()));\n        }\n    }\n}"
        },
        {
            "sender": "PerformanceSensitivity",
            "content": "The configuration is Performance Sensitive: The `key_cache_keys_to_save` directly influences the amount of data that is retained in the key cache, which can significantly impact memory usage and performance during cache save operations. A higher number of keys to save can lead to increased memory consumption and longer processing times during cache writes, especially in a system with high throughput or large datasets. This configuration can cause substantial variations in performance due to its effect on memory allocation and the time required for cache management operations."
        }
    ]
}