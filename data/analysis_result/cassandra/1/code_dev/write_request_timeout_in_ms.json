{
    "performance_sensitive": "Yes",
    "config_name": "write_request_timeout_in_ms",
    "messages": [
        {
            "sender": "System",
            "content": "Initialize the analysis for the target system and configuration."
        },
        {
            "sender": "Developer",
            "content": "public class BatchlogManager implements BatchlogManagerMBean\n{\n    public static final String MBEAN_NAME = \"org.apache.cassandra.db:type=BatchlogManager\";\n    private static final long REPLAY_INTERVAL = 10 * 1000; // milliseconds\n    static final int DEFAULT_PAGE_SIZE = 128;\n\n    private static final Logger logger = LoggerFactory.getLogger(BatchlogManager.class);\n    public static final BatchlogManager instance = new BatchlogManager();\n    public static final long BATCHLOG_REPLAY_TIMEOUT = Long.getLong(\"cassandra.batchlog.replay_timeout_in_ms\", DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS) * 2);\n\n    private volatile long totalBatchesReplayed = 0; // no concurrency protection necessary as only written by replay thread.\n    private volatile UUID lastReplayedUuid = UUIDGen.minTimeUUID(0);\n\n    // Single-thread executor service for scheduling and serializing log replay.\n    private final ScheduledExecutorService batchlogTasks;\n\n    private final RateLimiter rateLimiter = RateLimiter.create(Double.MAX_VALUE);\n\n    public BatchlogManager()\n    {\n        ScheduledThreadPoolExecutor executor = new DebuggableScheduledThreadPoolExecutor(\"BatchlogTasks\");\n        executor.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n        batchlogTasks = executor;\n    }\n\n    public void start()\n    {\n        MBeanWrapper.instance.registerMBean(this, MBEAN_NAME);\n\n        batchlogTasks.scheduleWithFixedDelay(this::replayFailedBatches,\n                                             StorageService.RING_DELAY,\n                                             REPLAY_INTERVAL,\n                                             MILLISECONDS);\n    }\n\n    public void shutdownAndWait(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException\n    {\n        ExecutorUtils.shutdownAndWait(timeout, unit, batchlogTasks);\n    }\n\n    public static void remove(UUID id)\n    {\n        new Mutation(PartitionUpdate.fullPartitionDelete(SystemKeyspace.Batches,\n                                                         UUIDType.instance.decompose(id),\n                                                         FBUtilities.timestampMicros(),\n                                                         FBUtilities.nowInSeconds()))\n            .apply();\n    }\n\n    public static void store(Batch batch)\n    {\n        store(batch, true);\n    }\n\n    public static void store(Batch batch, boolean durableWrites)\n    {\n        List<ByteBuffer> mutations = new ArrayList<>(batch.encodedMutations.size() + batch.decodedMutations.size());\n        mutations.addAll(batch.encodedMutations);\n\n        for (Mutation mutation : batch.decodedMutations)\n        {\n            try (DataOutputBuffer buffer = new DataOutputBuffer())\n            {\n                Mutation.serializer.serialize(mutation, buffer, MessagingService.current_version);\n                mutations.add(buffer.buffer());\n            }\n            catch (IOException e)\n            {\n                // shouldn't happen\n                throw new AssertionError(e);\n            }\n        }\n\n        PartitionUpdate.SimpleBuilder builder = PartitionUpdate.simpleBuilder(SystemKeyspace.Batches, batch.id);\n        builder.row()\n               .timestamp(batch.creationTime)\n               .add(\"version\", MessagingService.current_version)\n               .appendAll(\"mutations\", mutations);\n\n        builder.buildAsMutation().apply(durableWrites);\n    }\n\n    @VisibleForTesting\n    public int countAllBatches()\n    {\n        String query = String.format(\"SELECT count(*) FROM %s.%s\", SchemaConstants.SYSTEM_KEYSPACE_NAME, SystemKeyspace.BATCHES);\n        UntypedResultSet results = executeInternal(query);\n        if (results == null || results.isEmpty())\n            return 0;\n\n        return (int) results.one().getLong(\"count\");\n    }\n\n    public long getTotalBatchesReplayed()\n    {\n        return totalBatchesReplayed;\n    }\n\n    public void forceBatchlogReplay() throws Exception\n    {\n        startBatchlogReplay().get();\n    }\n\n    public Future<?> startBatchlogReplay()\n    {\n        // If a replay is already in progress this request will be executed after it completes.\n        return batchlogTasks.submit(this::replayFailedBatches);\n    }\n\n    void performInitialReplay() throws InterruptedException, ExecutionException\n    {\n        // Invokes initial replay. Used for testing only.\n        batchlogTasks.submit(this::replayFailedBatches).get();\n    }\n\n    private void replayFailedBatches()\n    {\n        logger.trace(\"Started replayFailedBatches\");\n\n        // rate limit is in bytes per second. Uses Double.MAX_VALUE if disabled (set to 0 in cassandra.yaml).\n        // max rate is scaled by the number of nodes in the cluster (same as for HHOM - see CASSANDRA-5272).\n        int endpointsCount = StorageService.instance.getTokenMetadata().getSizeOfAllEndpoints();\n        if (endpointsCount <= 0)\n        {\n            logger.trace(\"Replay cancelled as there are no peers in the ring.\");\n            return;\n        }\n        setRate(DatabaseDescriptor.getBatchlogReplayThrottleInKB());\n\n        UUID limitUuid = UUIDGen.maxTimeUUID(System.currentTimeMillis() - getBatchlogTimeout());\n        ColumnFamilyStore store = Keyspace.open(SchemaConstants.SYSTEM_KEYSPACE_NAME).getColumnFamilyStore(SystemKeyspace.BATCHES);\n        int pageSize = calculatePageSize(store);\n        // There cannot be any live content where token(id) <= token(lastReplayedUuid) as every processed batch is\n        // deleted, but the tombstoned content may still be present in the tables. To avoid walking over it we specify\n        // token(id) > token(lastReplayedUuid) as part of the query.\n        String query = String.format(\"SELECT id, mutations, version FROM %s.%s WHERE token(id) > token(?) AND token(id) <= token(?)\",\n                                     SchemaConstants.SYSTEM_KEYSPACE_NAME,\n                                     SystemKeyspace.BATCHES);\n        UntypedResultSet batches = executeInternalWithPaging(query, pageSize, lastReplayedUuid, limitUuid);\n        processBatchlogEntries(batches, pageSize, rateLimiter);\n        lastReplayedUuid = limitUuid;\n        logger.trace(\"Finished replayFailedBatches\");\n    }\n\n    /**\n     * Sets the rate for the current rate limiter. When {@code throttleInKB} is 0, this sets the rate to\n     * {@link Double#MAX_VALUE} bytes per second.\n     *\n     * @param throttleInKB throughput to set in KB per second\n     */\n    public void setRate(final int throttleInKB)\n    {\n        int endpointsCount = StorageService.instance.getTokenMetadata().getSizeOfAllEndpoints();\n        if (endpointsCount > 0)\n        {\n            int endpointThrottleInKB = throttleInKB / endpointsCount;\n            double throughput = endpointThrottleInKB == 0 ? Double.MAX_VALUE : endpointThrottleInKB * 1024.0;\n            if (rateLimiter.getRate() != throughput)\n            {\n                logger.debug(\"Updating batchlog replay throttle to {} KB/s, {} KB/s per endpoint\", throttleInKB, endpointThrottleInKB);\n                rateLimiter.setRate(throughput);\n            }\n        }\n    }\n\n    // read less rows (batches) per page if they are very large\n    static int calculatePageSize(ColumnFamilyStore store)\n    {\n        double averageRowSize = store.getMeanPartitionSize();\n        if (averageRowSize <= 0)\n            return DEFAULT_PAGE_SIZE;\n\n        return (int) Math.max(1, Math.min(DEFAULT_PAGE_SIZE, 4 * 1024 * 1024 / averageRowSize));\n    }\n\n    private void processBatchlogEntries(UntypedResultSet batches, int pageSize, RateLimiter rateLimiter)\n    {\n        int positionInPage = 0;\n        ArrayList<ReplayingBatch> unfinishedBatches = new ArrayList<>(pageSize);\n\n        Set<UUID> hintedNodes = new HashSet<>();\n        Set<UUID> replayedBatches = new HashSet<>();\n        Exception caughtException = null;\n        int skipped = 0;\n\n        // Sending out batches for replay without waiting for them, so that one stuck batch doesn't affect others\n        for (UntypedResultSet.Row row : batches)\n        {\n            UUID id = row.getUUID(\"id\");\n            int version = row.getInt(\"version\");\n            try\n            {\n                ReplayingBatch batch = new ReplayingBatch(id, version, row.getList(\"mutations\", BytesType.instance));\n                if (batch.replay(rateLimiter, hintedNodes) > 0)\n                {\n                    unfinishedBatches.add(batch);\n                }\n                else\n                {\n                    remove(id); // no write mutations were sent (either expired or all CFs involved truncated).\n                    ++totalBatchesReplayed;\n                }\n            }\n            catch (IOException e)\n            {\n                logger.warn(\"Skipped batch replay of {} due to {}\", id, e.getMessage());\n                caughtException = e;\n                remove(id);\n                ++skipped;\n            }\n\n            if (++positionInPage == pageSize)\n            {\n                // We have reached the end of a batch. To avoid keeping more than a page of mutations in memory,\n                // finish processing the page before requesting the next row.\n                finishAndClearBatches(unfinishedBatches, hintedNodes, replayedBatches);\n                positionInPage = 0;\n            }\n        }\n\n        // finalize the incomplete last page of batches\n        if (positionInPage > 0)\n            finishAndClearBatches(unfinishedBatches, hintedNodes, replayedBatches);\n\n        if (caughtException != null)\n            logger.warn(String.format(\"Encountered %d unexpected exceptions while sending out batches\", skipped), caughtException);\n\n        // to preserve batch guarantees, we must ensure that hints (if any) have made it to disk, before deleting the batches\n        HintsService.instance.flushAndFsyncBlockingly(hintedNodes);\n\n        // once all generated hints are fsynced, actually delete the batches\n        replayedBatches.forEach(BatchlogManager::remove);\n    }\n\n    private void finishAndClearBatches(ArrayList<ReplayingBatch> batches, Set<UUID> hintedNodes, Set<UUID> replayedBatches)\n    {\n        // schedule hints for timed out deliveries\n        for (ReplayingBatch batch : batches)\n        {\n            batch.finish(hintedNodes);\n            replayedBatches.add(batch.id);\n        }\n\n        totalBatchesReplayed += batches.size();\n        batches.clear();\n    }\n\n    public static long getBatchlogTimeout()\n    {\n        return BATCHLOG_REPLAY_TIMEOUT; // enough time for the actual write + BM removal mutation\n    }\n\n    private static class ReplayingBatch\n    {\n        private final UUID id;\n        private final long writtenAt;\n        private final List<Mutation> mutations;\n        private final int replayedBytes;\n\n        private List<ReplayWriteResponseHandler<Mutation>> replayHandlers;\n\n        ReplayingBatch(UUID id, int version, List<ByteBuffer> serializedMutations) throws IOException\n        {\n            this.id = id;\n            this.writtenAt = UUIDGen.unixTimestamp(id);\n            this.mutations = new ArrayList<>(serializedMutations.size());\n            this.replayedBytes = addMutations(version, serializedMutations);\n        }\n\n        public int replay(RateLimiter rateLimiter, Set<UUID> hintedNodes) throws IOException\n        {\n            logger.trace(\"Replaying batch {}\", id);\n\n            if (mutations.isEmpty())\n                return 0;\n\n            int gcgs = gcgs(mutations);\n            if (MILLISECONDS.toSeconds(writtenAt) + gcgs <= FBUtilities.nowInSeconds())\n                return 0;\n\n            replayHandlers = sendReplays(mutations, writtenAt, hintedNodes);\n\n            rateLimiter.acquire(replayedBytes); // acquire afterwards, to not mess up ttl calculation.\n\n            return replayHandlers.size();\n        }\n\n        public void finish(Set<UUID> hintedNodes)\n        {\n            for (int i = 0; i < replayHandlers.size(); i++)\n            {\n                ReplayWriteResponseHandler<Mutation> handler = replayHandlers.get(i);\n                try\n                {\n                    handler.get();\n                }\n                catch (WriteTimeoutException|WriteFailureException e)\n                {\n                    logger.trace(\"Failed replaying a batched mutation to a node, will write a hint\");\n                    logger.trace(\"Failure was : {}\", e.getMessage());\n                    // writing hints for the rest to hints, starting from i\n                    writeHintsForUndeliveredEndpoints(i, hintedNodes);\n                    return;\n                }\n            }\n        }\n\n        private int addMutations(int version, List<ByteBuffer> serializedMutations) throws IOException\n        {\n            int ret = 0;\n            for (ByteBuffer serializedMutation : serializedMutations)\n            {\n                ret += serializedMutation.remaining();\n                try (DataInputBuffer in = new DataInputBuffer(serializedMutation, true))\n                {\n                    addMutation(Mutation.serializer.deserialize(in, version));\n                }\n            }\n\n            return ret;\n        }\n\n        // Remove CFs that have been truncated since. writtenAt and SystemTable#getTruncatedAt() both return millis.\n        // We don't abort the replay entirely b/c this can be considered a success (truncated is same as delivered then\n        // truncated.\n        private void addMutation(Mutation mutation)\n        {\n            for (TableId tableId : mutation.getTableIds())\n                if (writtenAt <= SystemKeyspace.getTruncatedAt(tableId))\n                    mutation = mutation.without(tableId);\n\n            if (!mutation.isEmpty())\n                mutations.add(mutation);\n        }\n\n        private void writeHintsForUndeliveredEndpoints(int startFrom, Set<UUID> hintedNodes)\n        {\n            int gcgs = gcgs(mutations);\n\n            // expired\n            if (MILLISECONDS.toSeconds(writtenAt) + gcgs <= FBUtilities.nowInSeconds())\n                return;\n\n            Set<UUID> nodesToHint = new HashSet<>();\n            for (int i = startFrom; i < replayHandlers.size(); i++)\n            {\n                ReplayWriteResponseHandler<Mutation> handler = replayHandlers.get(i);\n                Mutation undeliveredMutation = mutations.get(i);\n\n                if (handler != null)\n                {\n                    for (InetAddressAndPort address : handler.undelivered)\n                    {\n                        UUID hostId = StorageService.instance.getHostIdForEndpoint(address);\n                        if (null != hostId)\n                            nodesToHint.add(hostId);\n                    }\n                    if (!nodesToHint.isEmpty())\n                        HintsService.instance.write(nodesToHint, Hint.create(undeliveredMutation, writtenAt));\n                    hintedNodes.addAll(nodesToHint);\n                    nodesToHint.clear();\n                }\n            }\n        }\n\n        private static List<ReplayWriteResponseHandler<Mutation>> sendReplays(List<Mutation> mutations,\n                                                                              long writtenAt,\n                                                                              Set<UUID> hintedNodes)\n        {\n            List<ReplayWriteResponseHandler<Mutation>> handlers = new ArrayList<>(mutations.size());\n            for (Mutation mutation : mutations)\n            {\n                ReplayWriteResponseHandler<Mutation> handler = sendSingleReplayMutation(mutation, writtenAt, hintedNodes);\n                handlers.add(handler);\n            }\n            return handlers;\n        }\n\n        /**\n         * We try to deliver the mutations to the replicas ourselves if they are alive and only resort to writing hints\n         * when a replica is down or a write request times out.\n         *\n         * @return direct delivery handler to wait on\n         */\n        private static ReplayWriteResponseHandler<Mutation> sendSingleReplayMutation(final Mutation mutation,\n                                                                                     long writtenAt,\n                                                                                     Set<UUID> hintedNodes)\n        {\n            String ks = mutation.getKeyspaceName();\n            Keyspace keyspace = Keyspace.open(ks);\n            Token tk = mutation.key().getToken();\n\n            // TODO: this logic could do with revisiting at some point, as it is unclear what its rationale is\n            // we perform a local write, ignoring errors and inline in this thread (potentially slowing replay down)\n            // effectively bumping CL for locally owned writes and also potentially stalling log replay if an error occurs\n            // once we decide how it should work, it can also probably be simplified, and avoid constructing a ReplicaPlan directly\n            ReplicaLayout.ForTokenWrite liveAndDown = ReplicaLayout.forTokenWriteLiveAndDown(keyspace, tk);\n            Replicas.temporaryAssertFull(liveAndDown.all()); // TODO in CASSANDRA-14549\n\n            Replica selfReplica = liveAndDown.all().selfIfPresent();\n            if (selfReplica != null)\n                mutation.apply();\n\n            ReplicaLayout.ForTokenWrite liveRemoteOnly = liveAndDown.filter(\n                    r -> FailureDetector.isReplicaAlive.test(r) && r != selfReplica);\n\n            for (Replica replica : liveAndDown.all())\n            {\n                if (replica == selfReplica || liveRemoteOnly.all().contains(replica))\n                    continue;\n\n                UUID hostId = StorageService.instance.getHostIdForEndpoint(replica.endpoint());\n                if (null != hostId)\n                {\n                    HintsService.instance.write(hostId, Hint.create(mutation, writtenAt));\n                    hintedNodes.add(hostId);\n                }\n            }\n\n            ReplicaPlan.ForTokenWrite replicaPlan = new ReplicaPlan.ForTokenWrite(keyspace, liveAndDown.replicationStrategy(),\n                    ConsistencyLevel.ONE, liveRemoteOnly.pending(), liveRemoteOnly.all(), liveRemoteOnly.all(), liveRemoteOnly.all());\n            ReplayWriteResponseHandler<Mutation> handler = new ReplayWriteResponseHandler<>(replicaPlan, System.nanoTime());\n            Message<Mutation> message = Message.outWithFlag(MUTATION_REQ, mutation, MessageFlag.CALL_BACK_ON_FAILURE);\n            for (Replica replica : liveRemoteOnly.all())\n                MessagingService.instance().sendWriteWithCallback(message, replica, handler, false);\n            return handler;\n        }\n\n        private static int gcgs(Collection<Mutation> mutations)\n        {\n            int gcgs = Integer.MAX_VALUE;\n            for (Mutation mutation : mutations)\n                gcgs = Math.min(gcgs, mutation.smallestGCGS());\n            return gcgs;\n        }\n\n        /**\n         * A wrapper of WriteResponseHandler that stores the addresses of the endpoints from\n         * which we did not receive a successful response.\n         */\n        private static class ReplayWriteResponseHandler<T> extends WriteResponseHandler<T>\n        {\n            private final Set<InetAddressAndPort> undelivered = Collections.newSetFromMap(new ConcurrentHashMap<>());\n\n            ReplayWriteResponseHandler(ReplicaPlan.ForTokenWrite replicaPlan, long queryStartNanoTime)\n            {\n                super(replicaPlan, null, WriteType.UNLOGGED_BATCH, queryStartNanoTime);\n                Iterables.addAll(undelivered, replicaPlan.contacts().endpoints());\n            }\n\n            @Override\n            protected int blockFor()\n            {\n                return this.replicaPlan.contacts().size();\n            }\n\n            @Override\n            public void onResponse(Message<T> m)\n            {\n                boolean removed = undelivered.remove(m == null ? FBUtilities.getBroadcastAddressAndPort() : m.from());\n                assert removed;\n                super.onResponse(m);\n            }\n        }\n    }\n}\n\n/**\n     * @return the minimum configured {read, write, range, truncate, misc} timeout\n     */\npublic static long getMinRpcTimeout(TimeUnit unit)\n    {\n        return Longs.min(getRpcTimeout(unit),\n                         getReadRpcTimeout(unit),\n                         getRangeRpcTimeout(unit),\n                         getWriteRpcTimeout(unit),\n                         getCounterWriteRpcTimeout(unit),\n                         getTruncateRpcTimeout(unit));\n    }\n\n    \npublic boolean useNewPreparedStatementBehaviour()\n    {\n        if (newPreparedStatementBehaviour || DatabaseDescriptor.getForceNewPreparedStatementBehaviour())\n            return true;\n\n        synchronized (this)\n        {\n            CassandraVersion minVersion = Gossiper.instance.getMinVersion(DatabaseDescriptor.getWriteRpcTimeout(TimeUnit.MILLISECONDS), TimeUnit.MILLISECONDS);\n            if (minVersion != null &&\n                ((minVersion.major == 3 && minVersion.minor == 0 && minVersion.compareTo(NEW_PREPARED_STATEMENT_BEHAVIOUR_SINCE_30) >= 0) ||\n                 (minVersion.major == 3 && minVersion.minor > 0 && minVersion.compareTo(NEW_PREPARED_STATEMENT_BEHAVIOUR_SINCE_3X) >= 0) ||\n                 (minVersion.compareTo(NEW_PREPARED_STATEMENT_BEHAVIOUR_SINCE_40, true) >= 0)))\n            {\n                logger.info(\"Fully upgraded to at least {}\", minVersion);\n                newPreparedStatementBehaviour = true;\n            }\n\n            return newPreparedStatementBehaviour;\n        }\n    }\n\n    \n@VisibleForTesting\n    public ColumnFamilyStore(Keyspace keyspace,\n                             String columnFamilyName,\n                             int generation,\n                             TableMetadataRef metadata,\n                             Directories directories,\n                             boolean loadSSTables,\n                             boolean registerBookeeping,\n                             boolean offline)\n    {\n        assert directories != null;\n        assert metadata != null : \"null metadata for \" + keyspace + ':' + columnFamilyName;\n\n        this.keyspace = keyspace;\n        this.metadata = metadata;\n        this.directories = directories;\n        name = columnFamilyName;\n        minCompactionThreshold = new DefaultValue<>(metadata.get().params.compaction.minCompactionThreshold());\n        maxCompactionThreshold = new DefaultValue<>(metadata.get().params.compaction.maxCompactionThreshold());\n        crcCheckChance = new DefaultValue<>(metadata.get().params.crcCheckChance);\n        viewManager = keyspace.viewManager.forTable(metadata.id);\n        fileIndexGenerator.set(generation);\n        sampleReadLatencyNanos = DatabaseDescriptor.getReadRpcTimeout(NANOSECONDS) / 2;\n        additionalWriteLatencyNanos = DatabaseDescriptor.getWriteRpcTimeout(NANOSECONDS) / 2;\n\n        logger.info(\"Initializing {}.{}\", keyspace.getName(), name);\n\n        // Create Memtable only on online\n        Memtable initialMemtable = null;\n        if (DatabaseDescriptor.isDaemonInitialized())\n            initialMemtable = new Memtable(new AtomicReference<>(CommitLog.instance.getCurrentPosition()), this);\n        data = new Tracker(initialMemtable, loadSSTables);\n\n        // Note that this needs to happen before we load the first sstables, or the global sstable tracker will not\n        // be notified on the initial loading.\n        data.subscribe(StorageService.instance.sstablesTracker);\n\n        Collection<SSTableReader> sstables = null;\n        // scan for sstables corresponding to this cf and load them\n        if (data.loadsstables)\n        {\n            Directories.SSTableLister sstableFiles = directories.sstableLister(Directories.OnTxnErr.IGNORE).skipTemporary(true);\n            sstables = SSTableReader.openAll(sstableFiles.list().entrySet(), metadata);\n            data.addInitialSSTablesWithoutUpdatingSize(sstables);\n        }\n\n        // compaction strategy should be created after the CFS has been prepared\n        compactionStrategyManager = new CompactionStrategyManager(this);\n\n        if (maxCompactionThreshold.value() <= 0 || minCompactionThreshold.value() <=0)\n        {\n            logger.warn(\"Disabling compaction strategy by setting compaction thresholds to 0 is deprecated, set the compaction option 'enabled' to 'false' instead.\");\n            this.compactionStrategyManager.disable();\n        }\n\n        // create the private ColumnFamilyStores for the secondary column indexes\n        indexManager = new SecondaryIndexManager(this);\n        for (IndexMetadata info : metadata.get().indexes)\n        {\n            indexManager.addIndex(info, true);\n        }\n\n        metric = new TableMetrics(this);\n\n        if (data.loadsstables)\n        {\n            data.updateInitialSSTableSize(sstables);\n        }\n\n        if (registerBookeeping)\n        {\n            // register the mbean\n            mbeanName = getTableMBeanName(keyspace.getName(), name, isIndex());\n            oldMBeanName = getColumnFamilieMBeanName(keyspace.getName(), name, isIndex());\n\n            String[] objectNames = {mbeanName, oldMBeanName};\n            for (String objectName : objectNames)\n                MBeanWrapper.instance.registerMBean(this, objectName);\n        }\n        else\n        {\n            mbeanName = null;\n            oldMBeanName= null;\n        }\n        writeHandler = new CassandraTableWriteHandler(this);\n        streamManager = new CassandraStreamManager(this);\n        repairManager = new CassandraTableRepairManager(this);\n        sstableImporter = new SSTableImporter(this);\n    }\n\n    \n/**\n     * This method appends a row to the global CommitLog, then updates memtables and indexes.\n     *\n     * @param mutation       the row to write.  Must not be modified after calling apply, since commitlog append\n     *                       may happen concurrently, depending on the CL Executor type.\n     * @param makeDurable    if true, don't return unless write has been made durable\n     * @param updateIndexes  false to disable index updates (used by CollationController \"defragmenting\")\n     * @param isDroppable    true if this should throw WriteTimeoutException if it does not acquire lock within write_request_timeout_in_ms\n     * @param isDeferrable   true if caller is not waiting for future to complete, so that future may be deferred\n     */\nprivate CompletableFuture<?> applyInternal(final Mutation mutation,\n                                               final boolean makeDurable,\n                                               boolean updateIndexes,\n                                               boolean isDroppable,\n                                               boolean isDeferrable,\n                                               CompletableFuture<?> future)\n    {\n        if (TEST_FAIL_WRITES && metadata.name.equals(TEST_FAIL_WRITES_KS))\n            throw new RuntimeException(\"Testing write failures\");\n\n        Lock[] locks = null;\n\n        boolean requiresViewUpdate = updateIndexes && viewManager.updatesAffectView(Collections.singleton(mutation), false);\n\n        if (requiresViewUpdate)\n        {\n            mutation.viewLockAcquireStart.compareAndSet(0L, System.currentTimeMillis());\n\n            // the order of lock acquisition doesn't matter (from a deadlock perspective) because we only use tryLock()\n            Collection<TableId> tableIds = mutation.getTableIds();\n            Iterator<TableId> idIterator = tableIds.iterator();\n\n            locks = new Lock[tableIds.size()];\n            for (int i = 0; i < tableIds.size(); i++)\n            {\n                TableId tableId = idIterator.next();\n                int lockKey = Objects.hash(mutation.key().getKey(), tableId);\n                while (true)\n                {\n                    Lock lock = null;\n\n                    if (TEST_FAIL_MV_LOCKS_COUNT == 0)\n                        lock = ViewManager.acquireLockFor(lockKey);\n                    else\n                        TEST_FAIL_MV_LOCKS_COUNT--;\n\n                    if (lock == null)\n                    {\n                        //throw WTE only if request is droppable\n                        if (isDroppable && (approxTime.isAfter(mutation.approxCreatedAtNanos + DatabaseDescriptor.getWriteRpcTimeout(NANOSECONDS))))\n                        {\n                            for (int j = 0; j < i; j++)\n                                locks[j].unlock();\n\n                            if (logger.isTraceEnabled())\n                                logger.trace(\"Could not acquire lock for {} and table {}\", ByteBufferUtil.bytesToHex(mutation.key().getKey()), columnFamilyStores.get(tableId).name);\n                            Tracing.trace(\"Could not acquire MV lock\");\n                            if (future != null)\n                            {\n                                future.completeExceptionally(new WriteTimeoutException(WriteType.VIEW, ConsistencyLevel.LOCAL_ONE, 0, 1));\n                                return future;\n                            }\n                            else\n                                throw new WriteTimeoutException(WriteType.VIEW, ConsistencyLevel.LOCAL_ONE, 0, 1);\n                        }\n                        else if (isDeferrable)\n                        {\n                            for (int j = 0; j < i; j++)\n                                locks[j].unlock();\n\n                            // This view update can't happen right now. so rather than keep this thread busy\n                            // we will re-apply ourself to the queue and try again later\n                            final CompletableFuture<?> mark = future;\n                            Stage.MUTATION.execute(() ->\n                                                   applyInternal(mutation, makeDurable, true, isDroppable, true, mark)\n                            );\n                            return future;\n                        }\n                        else\n                        {\n                            // Retry lock on same thread, if mutation is not deferrable.\n                            // Mutation is not deferrable, if applied from MutationStage and caller is waiting for future to finish\n                            // If blocking caller defers future, this may lead to deadlock situation with all MutationStage workers\n                            // being blocked by waiting for futures which will never be processed as all workers are blocked\n                            try\n                            {\n                                // Wait a little bit before retrying to lock\n                                Thread.sleep(10);\n                            }\n                            catch (InterruptedException e)\n                            {\n                                // Just continue\n                            }\n                            continue;\n                        }\n                    }\n                    else\n                    {\n                        locks[i] = lock;\n                    }\n                    break;\n                }\n            }\n\n            long acquireTime = System.currentTimeMillis() - mutation.viewLockAcquireStart.get();\n            // Metrics are only collected for droppable write operations\n            // Bulk non-droppable operations (e.g. commitlog replay, hint delivery) are not measured\n            if (isDroppable)\n            {\n                for(TableId tableId : tableIds)\n                    columnFamilyStores.get(tableId).metric.viewLockAcquireTime.update(acquireTime, MILLISECONDS);\n            }\n        }\n        int nowInSec = FBUtilities.nowInSeconds();\n        try (WriteContext ctx = getWriteHandler().beginWrite(mutation, makeDurable))\n        {\n            for (PartitionUpdate upd : mutation.getPartitionUpdates())\n            {\n                ColumnFamilyStore cfs = columnFamilyStores.get(upd.metadata().id);\n                if (cfs == null)\n                {\n                    logger.error(\"Attempting to mutate non-existant table {} ({}.{})\", upd.metadata().id, upd.metadata().keyspace, upd.metadata().name);\n                    continue;\n                }\n                AtomicLong baseComplete = new AtomicLong(Long.MAX_VALUE);\n\n                if (requiresViewUpdate)\n                {\n                    try\n                    {\n                        Tracing.trace(\"Creating materialized view mutations from base table replica\");\n                        viewManager.forTable(upd.metadata().id).pushViewReplicaUpdates(upd, makeDurable, baseComplete);\n                    }\n                    catch (Throwable t)\n                    {\n                        JVMStabilityInspector.inspectThrowable(t);\n                        logger.error(String.format(\"Unknown exception caught while attempting to update MaterializedView! %s\",\n                                                   upd.metadata().toString()), t);\n                        throw t;\n                    }\n                }\n\n                UpdateTransaction indexTransaction = updateIndexes\n                                                     ? cfs.indexManager.newUpdateTransaction(upd, ctx, nowInSec)\n                                                     : UpdateTransaction.NO_OP;\n                cfs.getWriteHandler().write(upd, ctx, indexTransaction);\n\n                if (requiresViewUpdate)\n                    baseComplete.set(System.currentTimeMillis());\n            }\n\n            if (future != null) {\n                future.complete(null);\n            }\n            return future;\n        }\n        finally\n        {\n            if (locks != null)\n            {\n                for (Lock lock : locks)\n                    if (lock != null)\n                        lock.unlock();\n            }\n        }\n    }\n\n    \npublic long getTimeout(TimeUnit unit)\n    {\n        return DatabaseDescriptor.getWriteRpcTimeout(unit);\n    }\n\n    \n@Override\n    public long getTimeout(TimeUnit unit)\n    {\n        return DatabaseDescriptor.getWriteRpcTimeout(unit);\n    }\n\n    \npublic final long currentTimeoutNanos()\n    {\n        long requestTimeout = writeType == WriteType.COUNTER\n                              ? DatabaseDescriptor.getCounterWriteRpcTimeout(NANOSECONDS)\n                              : DatabaseDescriptor.getWriteRpcTimeout(NANOSECONDS);\n        return requestTimeout - (System.nanoTime() - queryStartNanoTime);\n    }\n\n    \npublic Long getWriteRpcTimeout() { return DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS); }\n    \npublic long getWriteRpcTimeout()\n    {\n        return DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS);\n    }\n\n    \nprivate void excise(Collection<Token> tokens, InetAddressAndPort endpoint)\n    {\n        logger.info(\"Removing tokens {} for {}\", tokens, endpoint);\n\n        UUID hostId = tokenMetadata.getHostId(endpoint);\n        if (hostId != null && tokenMetadata.isMember(endpoint))\n        {\n            // enough time for writes to expire and MessagingService timeout reporter callback to fire, which is where\n            // hints are mostly written from - using getMinRpcTimeout() / 2 for the interval.\n            long delay = DatabaseDescriptor.getMinRpcTimeout(MILLISECONDS) + DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS);\n            ScheduledExecutors.optionalTasks.schedule(() -> HintsService.instance.excise(hostId), delay, MILLISECONDS);\n        }\n\n        removeEndpoint(endpoint);\n        tokenMetadata.removeEndpoint(endpoint);\n        if (!tokens.isEmpty())\n            tokenMetadata.removeBootstrapTokens(tokens);\n        notifyLeft(endpoint);\n        PendingRangeCalculatorService.instance.update();\n    }\n\n    \nprivate void excise(Collection<Token> tokens, InetAddressAndPort endpoint, long expireTime)\n    {\n        addExpireTimeIfFound(endpoint, expireTime);\n        excise(tokens, endpoint);\n    }\n\n    \npublic void await() throws WriteTimeoutException\n    {\n        try\n        {\n            long timeout = DatabaseDescriptor.getWriteRpcTimeout(NANOSECONDS) - (System.nanoTime() - queryStartNanoTime);\n            if (!latch.await(timeout, NANOSECONDS))\n                throw new WriteTimeoutException(WriteType.CAS, consistency, getResponseCount(), targets);\n        }\n        catch (InterruptedException ex)\n        {\n            throw new AssertionError(\"This latch shouldn't have been interrupted.\");\n        }\n    }\n}"
        },
        {
            "sender": "Developer Filter",
            "content": "Filter of the code context that is related to configuration: ```java\npublic static final long BATCHLOG_REPLAY_TIMEOUT = Long.getLong(\"cassandra.batchlog.replay_timeout_in_ms\", DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS) * 2);\n\npublic long getTimeout(TimeUnit unit)\n{\n    return DatabaseDescriptor.getWriteRpcTimeout(unit);\n}\n\n@Override\npublic long getTimeout(TimeUnit unit)\n{\n    return DatabaseDescriptor.getWriteRpcTimeout(unit);\n}\n\npublic Long getWriteRpcTimeout() { return DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS); }\n\npublic long getWriteRpcTimeout()\n{\n    return DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS);\n}\n\npublic final long currentTimeoutNanos()\n{\n    long requestTimeout = writeType == WriteType.COUNTER\n                          ? DatabaseDescriptor.getCounterWriteRpcTimeout(NANOSECONDS)\n                          : DatabaseDescriptor.getWriteRpcTimeout(NANOSECONDS);\n    return requestTimeout - (System.nanoTime() - queryStartNanoTime);\n}\n```"
        },
        {
            "sender": "Developer summary",
            "content": "Summary of the extracted code context: ### Configuration Summary: `write_request_timeout_in_ms`\n\nThe `write_request_timeout_in_ms` configuration parameter plays a crucial role in managing the timeout settings for write operations within the system. It is specifically used to define the maximum allowable duration for write requests to complete before they are considered to have failed. This timeout is essential for ensuring that the system can handle write operations efficiently and can respond appropriately to potential issues that may arise during data writing processes.\n\n#### Key Components and Dependencies:\n1. **Batchlog Replay Timeout**: The configuration is utilized to set the `BATCHLOG_REPLAY_TIMEOUT`, which is derived from the `write_request_timeout_in_ms` value. This establishes a baseline for how long the system will wait for batch log replays to complete, effectively doubling the write timeout to accommodate potential delays.\n\n2. **DatabaseDescriptor**: The `DatabaseDescriptor` class serves as a central point for retrieving various configuration settings, including the write request timeout. Methods such as `getWriteRpcTimeout` are invoked to fetch the current timeout values in different time units (milliseconds, nanoseconds).\n\n3. **TimeUnit**: The configuration interacts with the `TimeUnit` class to provide flexibility in specifying and converting timeout values into various time formats, enhancing the usability of the timeout settings across different components of the system.\n\n4. **Mutation Handling**: The timeout configuration is integral to the mutation handling process, particularly in methods that apply mutations. It ensures that the system can track how long a write operation has been ongoing and can take appropriate actions if the operation exceeds the defined timeout.\n\n#### Interaction with Other Parts of the System:\n- The `getTimeout` methods provide a standardized way to retrieve the configured timeout value, ensuring that various components of the system can access this critical configuration consistently.\n- The `currentTimeoutNanos` method calculates the remaining time for a write operation based on the elapsed time since the operation started, leveraging the `write_request_timeout_in_ms` configuration to determine the maximum allowable duration.\n- By defining the timeout in milliseconds and providing mechanisms to retrieve and utilize this value, the configuration ensures that the system maintains a robust and responsive approach to handling write requests, contributing to overall operational integrity.\n\nIn summary, the `write_request_timeout_in_ms` configuration is a foundational element that governs the timeout behavior for write operations, ensuring that the system can manage data integrity and responsiveness effectively."
        },
        {
            "sender": "performance agent sensitivity",
            "content": "{\"Role\": \"You are a Performance Sensitivity Agent responsible for determining if a configuration is performance-sensitive within a software system.\", \"Requirement\": \"Using developer-provided information as a secondary reference, analyze the configuration-related code to determine if it is performance-sensitive. Focus exclusively on configurations with substantial system-level performance impact due to time-intensive or memory-intensive operations. Only configurations that directly cause significant performance variations through extremely time-consuming or memory-heavy operations should be classified as performance-sensitive. Minor or moderate impacts should not be considered.\", \"Output Format\": \"Respond strictly in one of the following formats: 'The configuration is Performance Sensitive: reason' or 'The configuration is Performance Insensitive: reason'. No other response format is acceptable. Provide a concise yet clear and detailed reason for your classification.\", \"configuration_name\": \"write_request_timeout_in_ms\", \"code_context\": \"```java\\npublic static final long BATCHLOG_REPLAY_TIMEOUT = Long.getLong(\\\"cassandra.batchlog.replay_timeout_in_ms\\\", DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS) * 2);\\n\\npublic long getTimeout(TimeUnit unit)\\n{\\n    return DatabaseDescriptor.getWriteRpcTimeout(unit);\\n}\\n\\n@Override\\npublic long getTimeout(TimeUnit unit)\\n{\\n    return DatabaseDescriptor.getWriteRpcTimeout(unit);\\n}\\n\\npublic Long getWriteRpcTimeout() { return DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS); }\\n\\npublic long getWriteRpcTimeout()\\n{\\n    return DatabaseDescriptor.getWriteRpcTimeout(MILLISECONDS);\\n}\\n\\npublic final long currentTimeoutNanos()\\n{\\n    long requestTimeout = writeType == WriteType.COUNTER\\n                          ? DatabaseDescriptor.getCounterWriteRpcTimeout(NANOSECONDS)\\n                          : DatabaseDescriptor.getWriteRpcTimeout(NANOSECONDS);\\n    return requestTimeout - (System.nanoTime() - queryStartNanoTime);\\n}\\n```\\n\\n--- Summary by Developer ---\\n### Configuration Summary: `write_request_timeout_in_ms`\\n\\nThe `write_request_timeout_in_ms` configuration parameter plays a crucial role in managing the timeout settings for write operations within the system. It is specifically used to define the maximum allowable duration for write requests to complete before they are considered to have failed. This timeout is essential for ensuring that the system can handle write operations efficiently and can respond appropriately to potential issues that may arise during data writing processes.\\n\\n#### Key Components and Dependencies:\\n1. **Batchlog Replay Timeout**: The configuration is utilized to set the `BATCHLOG_REPLAY_TIMEOUT`, which is derived from the `write_request_timeout_in_ms` value. This establishes a baseline for how long the system will wait for batch log replays to complete, effectively doubling the write timeout to accommodate potential delays.\\n\\n2. **DatabaseDescriptor**: The `DatabaseDescriptor` class serves as a central point for retrieving various configuration settings, including the write request timeout. Methods such as `getWriteRpcTimeout` are invoked to fetch the current timeout values in different time units (milliseconds, nanoseconds).\\n\\n3. **TimeUnit**: The configuration interacts with the `TimeUnit` class to provide flexibility in specifying and converting timeout values into various time formats, enhancing the usability of the timeout settings across different components of the system.\\n\\n4. **Mutation Handling**: The timeout configuration is integral to the mutation handling process, particularly in methods that apply mutations. It ensures that the system can track how long a write operation has been ongoing and can take appropriate actions if the operation exceeds the defined timeout.\\n\\n#### Interaction with Other Parts of the System:\\n- The `getTimeout` methods provide a standardized way to retrieve the configured timeout value, ensuring that various components of the system can access this critical configuration consistently.\\n- The `currentTimeoutNanos` method calculates the remaining time for a write operation based on the elapsed time since the operation started, leveraging the `write_request_timeout_in_ms` configuration to determine the maximum allowable duration.\\n- By defining the timeout in milliseconds and providing mechanisms to retrieve and utilize this value, the configuration ensures that the system maintains a robust and responsive approach to handling write requests, contributing to overall operational integrity.\\n\\nIn summary, the `write_request_timeout_in_ms` configuration is a foundational element that governs the timeout behavior for write operations, ensuring that the system can manage data integrity and responsiveness effectively.\"}"
        },
        {
            "sender": "PerformanceSensitivity",
            "content": "The configuration is Performance Sensitive: The `write_request_timeout_in_ms` directly impacts the duration of write operations, which can be time-intensive. If the timeout is set too high, it may lead to prolonged waits for failed operations, affecting overall system responsiveness. Conversely, if set too low, it could result in premature timeouts, causing legitimate write operations to fail. This configuration significantly influences the performance of write-heavy applications, making it critical for maintaining operational efficiency and data integrity."
        }
    ]
}