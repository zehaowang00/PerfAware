{
    "performance_sensitive": "Yes",
    "config_name": "hints_directory",
    "messages": [
        {
            "sender": "System",
            "content": "Initialize the analysis for the target system and configuration."
        },
        {
            "sender": "Developer",
            "content": "@VisibleForTesting\n    HintsService(IFailureDetector failureDetector)\n    {\n        File hintsDirectory = DatabaseDescriptor.getHintsDirectory();\n        int maxDeliveryThreads = DatabaseDescriptor.getMaxHintsDeliveryThreads();\n\n        catalog = HintsCatalog.load(hintsDirectory, createDescriptorParams());\n        writeExecutor = new HintsWriteExecutor(catalog);\n\n        int bufferSize = Math.max(DatabaseDescriptor.getMaxMutationSize() * 2, MIN_BUFFER_SIZE);\n        bufferPool = new HintsBufferPool(bufferSize, writeExecutor::flushBuffer);\n\n        isDispatchPaused = new AtomicBoolean(true);\n        dispatchExecutor = new HintsDispatchExecutor(hintsDirectory, maxDeliveryThreads, isDispatchPaused, failureDetector::isAlive);\n\n        // periodically empty the current content of the buffers\n        int flushPeriod = DatabaseDescriptor.getHintsFlushPeriodInMS();\n        triggerFlushingFuture = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(() -> writeExecutor.flushBufferPool(bufferPool),\n                                                                                        flushPeriod,\n                                                                                        flushPeriod,\n                                                                                        TimeUnit.MILLISECONDS);\n        metrics = new HintedHandoffMetrics();\n    }\n\n    \npublic void execute()\n        {\n            if (FBUtilities.isWindows)\n                return;\n            String jemalloc = System.getProperty(\"cassandra.libjemalloc\");\n            if (jemalloc == null)\n                logger.warn(\"jemalloc shared library could not be preloaded to speed up memory allocations\");\n            else if (\"-\".equals(jemalloc))\n                logger.info(\"jemalloc preload explicitly disabled\");\n            else\n                logger.info(\"jemalloc seems to be preloaded from {}\", jemalloc);\n        }\n    }\npublic void execute() throws StartupException\n        {\n            long now = System.currentTimeMillis();\n            if (now < EARLIEST_LAUNCH_DATE)\n                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE,\n                                           String.format(\"current machine time is %s, but that is seemingly incorrect. exiting now.\",\n                                                         new Date(now).toString()));\n        }\n    }\npublic void execute()\n        {\n            String jmxPort = System.getProperty(\"cassandra.jmx.remote.port\");\n            if (jmxPort == null)\n            {\n                logger.warn(\"JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.\");\n                jmxPort = System.getProperty(\"cassandra.jmx.local.port\");\n                if (jmxPort == null)\n                    logger.error(\"cassandra.jmx.local.port missing from cassandra-env.sh, unable to start local JMX service.\");\n            }\n            else\n            {\n                logger.info(\"JMX is enabled to receive remote connections on port: {}\", jmxPort);\n            }\n        }\n    }\npublic void execute()\n        {\n            if (COM_SUN_MANAGEMENT_JMXREMOTE_PORT.isPresent())\n            {\n                logger.warn(\"Use of com.sun.management.jmxremote.port at startup is deprecated. \" +\n                            \"Please use cassandra.jmx.remote.port instead.\");\n            }\n        }\n    }\npublic void execute()\n        {\n            // log warnings for different kinds of sub-optimal JVMs.  tldr use 64-bit Oracle >= 1.6u32\n            if (!DatabaseDescriptor.hasLargeAddressSpace())\n                logger.warn(\"32bit JVM detected.  It is recommended to run Cassandra on a 64bit JVM for better performance.\");\n\n            String javaVmName = JAVA_VM_NAME.getString();\n            if (!(javaVmName.contains(\"HotSpot\") || javaVmName.contains(\"OpenJDK\")))\n            {\n                logger.warn(\"Non-Oracle JVM detected.  Some features, such as immediate unmap of compacted SSTables, may not work as intended\");\n            }\n            else\n            {\n                checkOutOfMemoryHandling();\n            }\n        }\n\n        \npublic void execute() throws StartupException\n        {\n            // Fail-fast if the native library could not be linked.\n            if (!NativeLibrary.isAvailable())\n                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE, \"The native library could not be initialized properly. \");\n        }\n    }\npublic void execute()\n        {\n            SigarLibrary.instance.warnIfRunningInDegradedMode();\n        }\n    }\npublic void execute()\n        {\n            if (!FBUtilities.isLinux)\n                return;\n\n            if (DatabaseDescriptor.getDiskAccessMode() == Config.DiskAccessMode.standard &&\n                DatabaseDescriptor.getIndexAccessMode() == Config.DiskAccessMode.standard)\n                return; // no need to check if disk access mode is only standard and not mmap\n\n            long maxMapCount = getMaxMapCount();\n            if (maxMapCount < EXPECTED_MAX_MAP_COUNT)\n                logger.warn(\"Maximum number of memory map areas per process (vm.max_map_count) {} \" +\n                            \"is too low, recommended value: {}, you can change it with sysctl.\",\n                            maxMapCount, EXPECTED_MAX_MAP_COUNT);\n        }\n    }\npublic void execute() throws StartupException\n        {\n            final Set<String> invalid = new HashSet<>();\n            final Set<String> nonSSTablePaths = new HashSet<>();\n            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getCommitLogLocation()));\n            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getSavedCachesLocation()));\n            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getHintsDirectory()));\n\n            FileVisitor<Path> sstableVisitor = new SimpleFileVisitor<Path>()\n            {\n                public FileVisitResult visitFile(Path path, BasicFileAttributes attrs)\n                {\n                    File file = path.toFile();\n                    if (!Descriptor.isValidFile(file))\n                        return FileVisitResult.CONTINUE;\n\n                    try\n                    {\n                        if (!Descriptor.fromFilename(file).isCompatible())\n                            invalid.add(file.toString());\n                    }\n                    catch (Exception e)\n                    {\n                        invalid.add(file.toString());\n                    }\n                    return FileVisitResult.CONTINUE;\n                }\n\n                public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException\n                {\n                    String name = dir.getFileName().toString();\n                    return (name.equals(Directories.SNAPSHOT_SUBDIR)\n                            || name.equals(Directories.BACKUPS_SUBDIR)\n                            || nonSSTablePaths.contains(dir.toFile().getCanonicalPath()))\n                           ? FileVisitResult.SKIP_SUBTREE\n                           : FileVisitResult.CONTINUE;\n                }\n            };\n\n            for (String dataDir : DatabaseDescriptor.getAllDataFileLocations())\n            {\n                try\n                {\n                    Files.walkFileTree(Paths.get(dataDir), sstableVisitor);\n                }\n                catch (IOException e)\n                {\n                    throw new StartupException(3, \"Unable to verify sstable files on disk\", e);\n                }\n            }\n\n            if (!invalid.isEmpty())\n                throw new StartupException(StartupException.ERR_WRONG_DISK_STATE,\n                                           String.format(\"Detected unreadable sstables %s, please check \" +\n                                                         \"NEWS.txt and ensure that you have upgraded through \" +\n                                                         \"all required intermediate versions, running \" +\n                                                         \"upgradesstables\",\n                                                         Joiner.on(\",\").join(invalid)));\n\n        }\n    }\npublic void execute() throws StartupException\n        {\n            // check the system keyspace to keep user from shooting self in foot by changing partitioner, cluster name, etc.\n            // we do a one-off scrub of the system keyspace first; we can't load the list of the rest of the keyspaces,\n            // until system keyspace is opened.\n\n            for (TableMetadata cfm : Schema.instance.getTablesAndViews(SchemaConstants.SYSTEM_KEYSPACE_NAME))\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n\n            try\n            {\n                SystemKeyspace.checkHealth();\n            }\n            catch (ConfigurationException e)\n            {\n                throw new StartupException(StartupException.ERR_WRONG_CONFIG, \"Fatal exception during initialization\", e);\n            }\n        }\n    }\npublic void execute() throws StartupException\n        {\n            if (!Boolean.getBoolean(\"cassandra.ignore_dc\"))\n            {\n                String storedDc = SystemKeyspace.getDatacenter();\n                if (storedDc != null)\n                {\n                    String currentDc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();\n                    if (!storedDc.equals(currentDc))\n                    {\n                        String formatMessage = \"Cannot start node if snitch's data center (%s) differs from previous data center (%s). \" +\n                                               \"Please fix the snitch configuration, decommission and rebootstrap this node or use the flag -Dcassandra.ignore_dc=true.\";\n\n                        throw new StartupException(StartupException.ERR_WRONG_CONFIG, String.format(formatMessage, currentDc, storedDc));\n                    }\n                }\n            }\n        }\n    }\npublic void execute() throws StartupException\n        {\n            if (!Boolean.getBoolean(\"cassandra.ignore_rack\"))\n            {\n                String storedRack = SystemKeyspace.getRack();\n                if (storedRack != null)\n                {\n                    String currentRack = DatabaseDescriptor.getEndpointSnitch().getLocalRack();\n                    if (!storedRack.equals(currentRack))\n                    {\n                        String formatMessage = \"Cannot start node if snitch's rack (%s) differs from previous rack (%s). \" +\n                                               \"Please fix the snitch configuration, decommission and rebootstrap this node or use the flag -Dcassandra.ignore_rack=true.\";\n\n                        throw new StartupException(StartupException.ERR_WRONG_CONFIG, String.format(formatMessage, currentRack, storedRack));\n                    }\n                }\n            }\n        }\n    }\n/**\n * Verifies that the system and environment is in a fit state to be started.\n * Used in CassandraDaemon#setup() to check various settings and invariants.\n *\n * Each individual test is modelled as an implementation of StartupCheck, these are run\n * at the start of CassandraDaemon#setup() before any local state is mutated. The default\n * checks are a mix of informational tests (inspectJvmOptions), initialization\n * (initSigarLibrary, checkCacheServiceInitialization) and invariant checking\n * (checkValidLaunchDate, checkSystemKeyspaceState, checkSSTablesFormat).\n *\n * In addition, if checkSystemKeyspaceState determines that the release version has\n * changed since last startup (i.e. the node has been upgraded) it snapshots the system\n * keyspace to make it easier to back out if necessary.\n *\n * If any check reports a failure, then the setup method exits with an error (after\n * logging any output from the tests). If all tests report success, setup can continue.\n * We should be careful in future to ensure anything which mutates local state (such as\n * writing new sstables etc) only happens after we've verified the initial setup.\n */\npublic class StartupChecks\n{\n    private static final Logger logger = LoggerFactory.getLogger(StartupChecks.class);\n    // List of checks to run before starting up. If any test reports failure, startup will be halted.\n    private final List<StartupCheck> preFlightChecks = new ArrayList<>();\n\n    // The default set of pre-flight checks to run. Order is somewhat significant in that we probably\n    // always want the system keyspace check run last, as this actually loads the schema for that\n    // keyspace. All other checks should not require any schema initialization.\n    private final List<StartupCheck> DEFAULT_TESTS = ImmutableList.of(checkJemalloc,\n                                                                      checkLz4Native,\n                                                                      checkValidLaunchDate,\n                                                                      checkJMXPorts,\n                                                                      checkJMXProperties,\n                                                                      inspectJvmOptions,\n                                                                      checkNativeLibraryInitialization,\n                                                                      initSigarLibrary,\n                                                                      checkMaxMapCount,\n                                                                      checkDataDirs,\n                                                                      checkSSTablesFormat,\n                                                                      checkSystemKeyspaceState,\n                                                                      checkDatacenter,\n                                                                      checkRack,\n                                                                      checkLegacyAuthTables);\n\n    public StartupChecks withDefaultTests()\n    {\n        preFlightChecks.addAll(DEFAULT_TESTS);\n        return this;\n    }\n\n    /**\n     * Add system test to be run before schema is loaded during startup\n     * @param test the system test to include\n     */\n    public StartupChecks withTest(StartupCheck test)\n    {\n        preFlightChecks.add(test);\n        return this;\n    }\n\n    /**\n     * Run the configured tests and return a report detailing the results.\n     * @throws org.apache.cassandra.exceptions.StartupException if any test determines that the\n     * system is not in an valid state to startup\n     */\n    public void verify() throws StartupException\n    {\n        for (StartupCheck test : preFlightChecks)\n            test.execute();\n    }\n\n    public static final StartupCheck checkJemalloc = new StartupCheck()\n    {\n        public void execute()\n        {\n            if (FBUtilities.isWindows)\n                return;\n            String jemalloc = System.getProperty(\"cassandra.libjemalloc\");\n            if (jemalloc == null)\n                logger.warn(\"jemalloc shared library could not be preloaded to speed up memory allocations\");\n            else if (\"-\".equals(jemalloc))\n                logger.info(\"jemalloc preload explicitly disabled\");\n            else\n                logger.info(\"jemalloc seems to be preloaded from {}\", jemalloc);\n        }\n    };\n\n    public static final StartupCheck checkLz4Native = () -> {\n        try\n        {\n            LZ4Factory.nativeInstance(); // make sure native loads\n        }\n        catch (AssertionError | LinkageError e)\n        {\n            logger.warn(\"lz4-java was unable to load native libraries; this will lower the performance of lz4 (network/sstables/etc.): {}\", Throwables.getRootCause(e).getMessage());\n        }\n    };\n\n    public static final StartupCheck checkValidLaunchDate = new StartupCheck()\n    {\n        /**\n         * The earliest legit timestamp a casandra instance could have ever launched.\n         * Date roughly taken from http://perspectives.mvdirona.com/2008/07/12/FacebookReleasesCassandraAsOpenSource.aspx\n         * We use this to ensure the system clock is at least somewhat correct at startup.\n         */\n        private static final long EARLIEST_LAUNCH_DATE = 1215820800000L;\n        public void execute() throws StartupException\n        {\n            long now = System.currentTimeMillis();\n            if (now < EARLIEST_LAUNCH_DATE)\n                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE,\n                                           String.format(\"current machine time is %s, but that is seemingly incorrect. exiting now.\",\n                                                         new Date(now).toString()));\n        }\n    };\n\n    public static final StartupCheck checkJMXPorts = new StartupCheck()\n    {\n        public void execute()\n        {\n            String jmxPort = System.getProperty(\"cassandra.jmx.remote.port\");\n            if (jmxPort == null)\n            {\n                logger.warn(\"JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.\");\n                jmxPort = System.getProperty(\"cassandra.jmx.local.port\");\n                if (jmxPort == null)\n                    logger.error(\"cassandra.jmx.local.port missing from cassandra-env.sh, unable to start local JMX service.\");\n            }\n            else\n            {\n                logger.info(\"JMX is enabled to receive remote connections on port: {}\", jmxPort);\n            }\n        }\n    };\n\n    public static final StartupCheck checkJMXProperties = new StartupCheck()\n    {\n        public void execute()\n        {\n            if (COM_SUN_MANAGEMENT_JMXREMOTE_PORT.isPresent())\n            {\n                logger.warn(\"Use of com.sun.management.jmxremote.port at startup is deprecated. \" +\n                            \"Please use cassandra.jmx.remote.port instead.\");\n            }\n        }\n    };\n\n    public static final StartupCheck inspectJvmOptions = new StartupCheck()\n    {\n        public void execute()\n        {\n            // log warnings for different kinds of sub-optimal JVMs.  tldr use 64-bit Oracle >= 1.6u32\n            if (!DatabaseDescriptor.hasLargeAddressSpace())\n                logger.warn(\"32bit JVM detected.  It is recommended to run Cassandra on a 64bit JVM for better performance.\");\n\n            String javaVmName = JAVA_VM_NAME.getString();\n            if (!(javaVmName.contains(\"HotSpot\") || javaVmName.contains(\"OpenJDK\")))\n            {\n                logger.warn(\"Non-Oracle JVM detected.  Some features, such as immediate unmap of compacted SSTables, may not work as intended\");\n            }\n            else\n            {\n                checkOutOfMemoryHandling();\n            }\n        }\n\n        /**\n         * Checks that the JVM is configured to handle OutOfMemoryError\n         */\n        private void checkOutOfMemoryHandling()\n        {\n            if (JavaUtils.supportExitOnOutOfMemory(JAVA_VERSION.getString()))\n            {\n                if (!jvmOptionsContainsOneOf(\"-XX:OnOutOfMemoryError=\", \"-XX:+ExitOnOutOfMemoryError\", \"-XX:+CrashOnOutOfMemoryError\"))\n                    logger.warn(\"The JVM is not configured to stop on OutOfMemoryError which can cause data corruption.\"\n                                + \" Use one of the following JVM options to configure the behavior on OutOfMemoryError: \"\n                                + \" -XX:+ExitOnOutOfMemoryError, -XX:+CrashOnOutOfMemoryError, or -XX:OnOutOfMemoryError=\\\"<cmd args>;<cmd args>\\\"\");\n            }\n            else\n            {\n                if (!jvmOptionsContainsOneOf(\"-XX:OnOutOfMemoryError=\"))\n                    logger.warn(\"The JVM is not configured to stop on OutOfMemoryError which can cause data corruption.\"\n                            + \" Either upgrade your JRE to a version greater or equal to 8u92 and use -XX:+ExitOnOutOfMemoryError/-XX:+CrashOnOutOfMemoryError\"\n                            + \" or use -XX:OnOutOfMemoryError=\\\"<cmd args>;<cmd args>\\\" on your current JRE.\");\n            }\n        }\n\n        /**\n         * Checks if one of the specified options is being used.\n         * @param optionNames The name of the options to check\n         * @return {@code true} if one of the specified options is being used, {@code false} otherwise.\n         */\n        private boolean jvmOptionsContainsOneOf(String... optionNames)\n        {\n            RuntimeMXBean runtimeMxBean = ManagementFactory.getRuntimeMXBean();\n            List<String> inputArguments = runtimeMxBean.getInputArguments();\n            for (String argument : inputArguments)\n            {\n                for (String optionName : optionNames)\n                    if (argument.startsWith(optionName))\n                        return true;\n            }\n            return false;\n        }\n    };\n\n    public static final StartupCheck checkNativeLibraryInitialization = new StartupCheck()\n    {\n        public void execute() throws StartupException\n        {\n            // Fail-fast if the native library could not be linked.\n            if (!NativeLibrary.isAvailable())\n                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE, \"The native library could not be initialized properly. \");\n        }\n    };\n\n    public static final StartupCheck initSigarLibrary = new StartupCheck()\n    {\n        public void execute()\n        {\n            SigarLibrary.instance.warnIfRunningInDegradedMode();\n        }\n    };\n\n    public static final StartupCheck checkMaxMapCount = new StartupCheck()\n    {\n        private final long EXPECTED_MAX_MAP_COUNT = 1048575;\n        private final String MAX_MAP_COUNT_PATH = \"/proc/sys/vm/max_map_count\";\n\n        private long getMaxMapCount()\n        {\n            final Path path = Paths.get(MAX_MAP_COUNT_PATH);\n            try (final BufferedReader bufferedReader = Files.newBufferedReader(path))\n            {\n                final String data = bufferedReader.readLine();\n                if (data != null)\n                {\n                    try\n                    {\n                        return Long.parseLong(data);\n                    }\n                    catch (final NumberFormatException e)\n                    {\n                        logger.warn(\"Unable to parse {}.\", path, e);\n                    }\n                }\n            }\n            catch (final IOException e)\n            {\n                logger.warn(\"IO exception while reading file {}.\", path, e);\n            }\n            return -1;\n        }\n\n        public void execute()\n        {\n            if (!FBUtilities.isLinux)\n                return;\n\n            if (DatabaseDescriptor.getDiskAccessMode() == Config.DiskAccessMode.standard &&\n                DatabaseDescriptor.getIndexAccessMode() == Config.DiskAccessMode.standard)\n                return; // no need to check if disk access mode is only standard and not mmap\n\n            long maxMapCount = getMaxMapCount();\n            if (maxMapCount < EXPECTED_MAX_MAP_COUNT)\n                logger.warn(\"Maximum number of memory map areas per process (vm.max_map_count) {} \" +\n                            \"is too low, recommended value: {}, you can change it with sysctl.\",\n                            maxMapCount, EXPECTED_MAX_MAP_COUNT);\n        }\n    };\n\n    public static final StartupCheck checkDataDirs = () ->\n    {\n        // check all directories(data, commitlog, saved cache) for existence and permission\n        Iterable<String> dirs = Iterables.concat(Arrays.asList(DatabaseDescriptor.getAllDataFileLocations()),\n                                                 Arrays.asList(DatabaseDescriptor.getCommitLogLocation(),\n                                                               DatabaseDescriptor.getSavedCachesLocation(),\n                                                               DatabaseDescriptor.getHintsDirectory().getAbsolutePath()));\n\n        for (String dataDir : dirs)\n        {\n            logger.debug(\"Checking directory {}\", dataDir);\n            File dir = new File(dataDir);\n\n            // check that directories exist.\n            if (!dir.exists())\n            {\n                logger.warn(\"Directory {} doesn't exist\", dataDir);\n                // if they don't, failing their creation, stop cassandra.\n                if (!dir.mkdirs())\n                    throw new StartupException(StartupException.ERR_WRONG_DISK_STATE,\n                                               \"Has no permission to create directory \"+ dataDir);\n            }\n\n            // if directories exist verify their permissions\n            if (!Directories.verifyFullPermissions(dir, dataDir))\n                throw new StartupException(StartupException.ERR_WRONG_DISK_STATE,\n                                           \"Insufficient permissions on directory \" + dataDir);\n        }\n    };\n\n    public static final StartupCheck checkSSTablesFormat = new StartupCheck()\n    {\n        public void execute() throws StartupException\n        {\n            final Set<String> invalid = new HashSet<>();\n            final Set<String> nonSSTablePaths = new HashSet<>();\n            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getCommitLogLocation()));\n            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getSavedCachesLocation()));\n            nonSSTablePaths.add(FileUtils.getCanonicalPath(DatabaseDescriptor.getHintsDirectory()));\n\n            FileVisitor<Path> sstableVisitor = new SimpleFileVisitor<Path>()\n            {\n                public FileVisitResult visitFile(Path path, BasicFileAttributes attrs)\n                {\n                    File file = path.toFile();\n                    if (!Descriptor.isValidFile(file))\n                        return FileVisitResult.CONTINUE;\n\n                    try\n                    {\n                        if (!Descriptor.fromFilename(file).isCompatible())\n                            invalid.add(file.toString());\n                    }\n                    catch (Exception e)\n                    {\n                        invalid.add(file.toString());\n                    }\n                    return FileVisitResult.CONTINUE;\n                }\n\n                public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException\n                {\n                    String name = dir.getFileName().toString();\n                    return (name.equals(Directories.SNAPSHOT_SUBDIR)\n                            || name.equals(Directories.BACKUPS_SUBDIR)\n                            || nonSSTablePaths.contains(dir.toFile().getCanonicalPath()))\n                           ? FileVisitResult.SKIP_SUBTREE\n                           : FileVisitResult.CONTINUE;\n                }\n            };\n\n            for (String dataDir : DatabaseDescriptor.getAllDataFileLocations())\n            {\n                try\n                {\n                    Files.walkFileTree(Paths.get(dataDir), sstableVisitor);\n                }\n                catch (IOException e)\n                {\n                    throw new StartupException(3, \"Unable to verify sstable files on disk\", e);\n                }\n            }\n\n            if (!invalid.isEmpty())\n                throw new StartupException(StartupException.ERR_WRONG_DISK_STATE,\n                                           String.format(\"Detected unreadable sstables %s, please check \" +\n                                                         \"NEWS.txt and ensure that you have upgraded through \" +\n                                                         \"all required intermediate versions, running \" +\n                                                         \"upgradesstables\",\n                                                         Joiner.on(\",\").join(invalid)));\n\n        }\n    };\n\n    public static final StartupCheck checkSystemKeyspaceState = new StartupCheck()\n    {\n        public void execute() throws StartupException\n        {\n            // check the system keyspace to keep user from shooting self in foot by changing partitioner, cluster name, etc.\n            // we do a one-off scrub of the system keyspace first; we can't load the list of the rest of the keyspaces,\n            // until system keyspace is opened.\n\n            for (TableMetadata cfm : Schema.instance.getTablesAndViews(SchemaConstants.SYSTEM_KEYSPACE_NAME))\n                ColumnFamilyStore.scrubDataDirectories(cfm);\n\n            try\n            {\n                SystemKeyspace.checkHealth();\n            }\n            catch (ConfigurationException e)\n            {\n                throw new StartupException(StartupException.ERR_WRONG_CONFIG, \"Fatal exception during initialization\", e);\n            }\n        }\n    };\n\n    public static final StartupCheck checkDatacenter = new StartupCheck()\n    {\n        public void execute() throws StartupException\n        {\n            if (!Boolean.getBoolean(\"cassandra.ignore_dc\"))\n            {\n                String storedDc = SystemKeyspace.getDatacenter();\n                if (storedDc != null)\n                {\n                    String currentDc = DatabaseDescriptor.getEndpointSnitch().getLocalDatacenter();\n                    if (!storedDc.equals(currentDc))\n                    {\n                        String formatMessage = \"Cannot start node if snitch's data center (%s) differs from previous data center (%s). \" +\n                                               \"Please fix the snitch configuration, decommission and rebootstrap this node or use the flag -Dcassandra.ignore_dc=true.\";\n\n                        throw new StartupException(StartupException.ERR_WRONG_CONFIG, String.format(formatMessage, currentDc, storedDc));\n                    }\n                }\n            }\n        }\n    };\n\n    public static final StartupCheck checkRack = new StartupCheck()\n    {\n        public void execute() throws StartupException\n        {\n            if (!Boolean.getBoolean(\"cassandra.ignore_rack\"))\n            {\n                String storedRack = SystemKeyspace.getRack();\n                if (storedRack != null)\n                {\n                    String currentRack = DatabaseDescriptor.getEndpointSnitch().getLocalRack();\n                    if (!storedRack.equals(currentRack))\n                    {\n                        String formatMessage = \"Cannot start node if snitch's rack (%s) differs from previous rack (%s). \" +\n                                               \"Please fix the snitch configuration, decommission and rebootstrap this node or use the flag -Dcassandra.ignore_rack=true.\";\n\n                        throw new StartupException(StartupException.ERR_WRONG_CONFIG, String.format(formatMessage, currentRack, storedRack));\n                    }\n                }\n            }\n        }\n    };\n\n    public static final StartupCheck checkLegacyAuthTables = () ->\n    {\n        Optional<String> errMsg = checkLegacyAuthTablesMessage();\n        if (errMsg.isPresent())\n            throw new StartupException(StartupException.ERR_WRONG_CONFIG, errMsg.get());\n    };\n\n    @VisibleForTesting\n    static Optional<String> checkLegacyAuthTablesMessage()\n    {\n        List<String> existing = new ArrayList<>(SchemaConstants.LEGACY_AUTH_TABLES).stream().filter((legacyAuthTable) ->\n            {\n                UntypedResultSet result = QueryProcessor.executeOnceInternal(String.format(\"SELECT table_name FROM %s.%s WHERE keyspace_name='%s' AND table_name='%s'\",\n                                                                                           SchemaConstants.SCHEMA_KEYSPACE_NAME,\n                                                                                           \"tables\",\n                                                                                           SchemaConstants.AUTH_KEYSPACE_NAME,\n                                                                                           legacyAuthTable));\n                return result != null && !result.isEmpty();\n            }).collect(Collectors.toList());\n\n        if (!existing.isEmpty())\n            return Optional.of(String.format(\"Legacy auth tables %s in keyspace %s still exist and have not been properly migrated.\",\n                        Joiner.on(\", \").join(existing), SchemaConstants.AUTH_KEYSPACE_NAME));\n        else\n            return Optional.empty();\n    };\n}\n"
        },
        {
            "sender": "Developer Filter",
            "content": "Filter of the code context that is related to configuration: ```java\n@VisibleForTesting\nHintsService(IFailureDetector failureDetector)\n{\n    File hintsDirectory = DatabaseDescriptor.getHintsDirectory();\n    int maxDeliveryThreads = DatabaseDescriptor.getMaxHintsDeliveryThreads();\n\n    catalog = HintsCatalog.load(hintsDirectory, createDescriptorParams());\n    writeExecutor = new HintsWriteExecutor(catalog);\n\n    isDispatchPaused = new AtomicBoolean(true);\n    dispatchExecutor = new HintsDispatchExecutor(hintsDirectory, maxDeliveryThreads, isDispatchPaused, failureDetector::isAlive);\n\n    // periodically empty the current content of the buffers\n    int flushPeriod = DatabaseDescriptor.getHintsFlushPeriodInMS();\n    triggerFlushingFuture = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(() -> writeExecutor.flushBufferPool(bufferPool),\n                                                                                    flushPeriod,\n                                                                                    flushPeriod,\n                                                                                    TimeUnit.MILLISECONDS);\n}\n```"
        },
        {
            "sender": "Developer summary",
            "content": "Summary of the extracted code context: ### Summary of the `hints_directory` Configuration\n\nThe `hints_directory` configuration plays a critical role in the `HintsService` class, which is responsible for managing hint delivery in a distributed system. Hints are used to ensure that write operations are not lost when a node is temporarily unavailable, allowing for eventual consistency.\n\n#### Purpose and Functionality\n- **Storage Location**: The `hints_directory` specifies the file path where hints are stored. This directory is crucial for persisting hints that need to be delivered to nodes that were down during write operations.\n- **Initialization**: During the initialization of `HintsService`, the `hintsDirectory` is retrieved from `DatabaseDescriptor.getHintsDirectory()`, establishing the location for hint storage.\n- **Interaction with Other Components**:\n  - **HintsCatalog**: The `hintsDirectory` is used to load the `HintsCatalog`, which manages the collection of hints. This catalog is essential for tracking which hints need to be delivered.\n  - **HintsWriteExecutor**: This component is responsible for writing hints to the `hintsDirectory`. It interacts with the catalog to ensure that hints are processed correctly.\n  - **HintsDispatchExecutor**: This executor utilizes the `hintsDirectory` to manage the delivery of hints. It operates with a specified number of delivery threads, which are determined by the configuration value retrieved from `DatabaseDescriptor.getMaxHintsDeliveryThreads()`.\n  \n#### Dependencies\n- The `hints_directory` relies on the `DatabaseDescriptor` class, which centralizes configuration management for various system parameters. This dependency ensures that the hints service can dynamically adapt to configuration changes.\n- The `IFailureDetector` dependency allows the `HintsDispatchExecutor` to check the health of nodes, ensuring that hints are only sent to nodes that are alive and reachable.\n\n#### Interaction with System Components\n- The `hints_directory` interacts with the `ScheduledExecutors` to periodically flush the buffer pool of hints, ensuring that hints are regularly processed and delivered. The flush period is also configurable through `DatabaseDescriptor.getHintsFlushPeriodInMS()`, allowing for flexible management of hint delivery timing.\n\nIn summary, the `hints_directory` configuration is integral to the operation of the `HintsService`, facilitating the storage, management, and delivery of hints in a distributed environment. Its interactions with various components ensure that the system can maintain consistency and recover from temporary node failures."
        },
        {
            "sender": "performance agent sensitivity",
            "content": "{\"Role\": \"You are a Performance Sensitivity Agent responsible for determining if a configuration is performance-sensitive within a software system.\", \"Requirement\": \"Using developer-provided information as a secondary reference, analyze the configuration-related code to determine if it is performance-sensitive. Focus exclusively on configurations with substantial system-level performance impact due to time-intensive or memory-intensive operations. Only configurations that directly cause significant performance variations through extremely time-consuming or memory-heavy operations should be classified as performance-sensitive. Minor or moderate impacts should not be considered.\", \"Output Format\": \"Respond strictly in one of the following formats: 'The configuration is Performance Sensitive: reason' or 'The configuration is Performance Insensitive: reason'. No other response format is acceptable. Provide a concise yet clear and detailed reason for your classification.\", \"configuration_name\": \"hints_directory\", \"code_context\": \"```java\\n@VisibleForTesting\\nHintsService(IFailureDetector failureDetector)\\n{\\n    File hintsDirectory = DatabaseDescriptor.getHintsDirectory();\\n    int maxDeliveryThreads = DatabaseDescriptor.getMaxHintsDeliveryThreads();\\n\\n    catalog = HintsCatalog.load(hintsDirectory, createDescriptorParams());\\n    writeExecutor = new HintsWriteExecutor(catalog);\\n\\n    isDispatchPaused = new AtomicBoolean(true);\\n    dispatchExecutor = new HintsDispatchExecutor(hintsDirectory, maxDeliveryThreads, isDispatchPaused, failureDetector::isAlive);\\n\\n    // periodically empty the current content of the buffers\\n    int flushPeriod = DatabaseDescriptor.getHintsFlushPeriodInMS();\\n    triggerFlushingFuture = ScheduledExecutors.optionalTasks.scheduleWithFixedDelay(() -> writeExecutor.flushBufferPool(bufferPool),\\n                                                                                    flushPeriod,\\n                                                                                    flushPeriod,\\n                                                                                    TimeUnit.MILLISECONDS);\\n}\\n```\\n\\n--- Summary by Developer ---\\n### Summary of the `hints_directory` Configuration\\n\\nThe `hints_directory` configuration plays a critical role in the `HintsService` class, which is responsible for managing hint delivery in a distributed system. Hints are used to ensure that write operations are not lost when a node is temporarily unavailable, allowing for eventual consistency.\\n\\n#### Purpose and Functionality\\n- **Storage Location**: The `hints_directory` specifies the file path where hints are stored. This directory is crucial for persisting hints that need to be delivered to nodes that were down during write operations.\\n- **Initialization**: During the initialization of `HintsService`, the `hintsDirectory` is retrieved from `DatabaseDescriptor.getHintsDirectory()`, establishing the location for hint storage.\\n- **Interaction with Other Components**:\\n  - **HintsCatalog**: The `hintsDirectory` is used to load the `HintsCatalog`, which manages the collection of hints. This catalog is essential for tracking which hints need to be delivered.\\n  - **HintsWriteExecutor**: This component is responsible for writing hints to the `hintsDirectory`. It interacts with the catalog to ensure that hints are processed correctly.\\n  - **HintsDispatchExecutor**: This executor utilizes the `hintsDirectory` to manage the delivery of hints. It operates with a specified number of delivery threads, which are determined by the configuration value retrieved from `DatabaseDescriptor.getMaxHintsDeliveryThreads()`.\\n  \\n#### Dependencies\\n- The `hints_directory` relies on the `DatabaseDescriptor` class, which centralizes configuration management for various system parameters. This dependency ensures that the hints service can dynamically adapt to configuration changes.\\n- The `IFailureDetector` dependency allows the `HintsDispatchExecutor` to check the health of nodes, ensuring that hints are only sent to nodes that are alive and reachable.\\n\\n#### Interaction with System Components\\n- The `hints_directory` interacts with the `ScheduledExecutors` to periodically flush the buffer pool of hints, ensuring that hints are regularly processed and delivered. The flush period is also configurable through `DatabaseDescriptor.getHintsFlushPeriodInMS()`, allowing for flexible management of hint delivery timing.\\n\\nIn summary, the `hints_directory` configuration is integral to the operation of the `HintsService`, facilitating the storage, management, and delivery of hints in a distributed environment. Its interactions with various components ensure that the system can maintain consistency and recover from temporary node failures.\"}"
        },
        {
            "sender": "PerformanceSensitivity",
            "content": "The configuration is Performance Sensitive: The `hints_directory` is critical for managing hint delivery in a distributed system, which directly impacts system performance during write operations. The configuration influences the efficiency of hint storage and retrieval, and the number of delivery threads can significantly affect the throughput of hint processing. Additionally, the periodic flushing of the buffer pool, controlled by the `hints_flush_period`, can lead to performance variations based on how frequently hints are processed and delivered. Overall, these factors can lead to substantial performance impacts during high-load scenarios, making this configuration performance sensitive."
        }
    ]
}