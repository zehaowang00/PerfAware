{
    "performance_sensitive": "Yes",
    "config_name": "concurrent_materialized_view_builders",
    "messages": [
        {
            "sender": "System",
            "content": "Initialize the analysis for the target system and configuration."
        },
        {
            "sender": "Developer",
            "content": "/**\n * <p>\n * A singleton which manages a private executor of ongoing compactions.\n * </p>\n * Scheduling for compaction is accomplished by swapping sstables to be compacted into\n * a set via Tracker. New scheduling attempts will ignore currently compacting\n * sstables.\n */\npublic class CompactionManager implements CompactionManagerMBean\n{\n    public static final String MBEAN_OBJECT_NAME = \"org.apache.cassandra.db:type=CompactionManager\";\n    private static final Logger logger = LoggerFactory.getLogger(CompactionManager.class);\n    public static final CompactionManager instance;\n\n    @VisibleForTesting\n    public final AtomicInteger currentlyBackgroundUpgrading = new AtomicInteger(0);\n\n    public static final int NO_GC = Integer.MIN_VALUE;\n    public static final int GC_ALL = Integer.MAX_VALUE;\n\n    // A thread local that tells us if the current thread is owned by the compaction manager. Used\n    // by CounterContext to figure out if it should log a warning for invalid counter shards.\n    public static final FastThreadLocal<Boolean> isCompactionManager = new FastThreadLocal<Boolean>()\n    {\n        @Override\n        protected Boolean initialValue()\n        {\n            return false;\n        }\n    };\n\n    static\n    {\n        instance = new CompactionManager();\n\n        MBeanWrapper.instance.registerMBean(instance, MBEAN_OBJECT_NAME);\n    }\n\n    private final CompactionExecutor executor = new CompactionExecutor();\n    private final ValidationExecutor validationExecutor = new ValidationExecutor();\n    private final CompactionExecutor cacheCleanupExecutor = new CacheCleanupExecutor();\n    private final CompactionExecutor viewBuildExecutor = new ViewBuildExecutor();\n\n    private final CompactionMetrics metrics = new CompactionMetrics(executor, validationExecutor, viewBuildExecutor);\n    @VisibleForTesting\n    final Multiset<ColumnFamilyStore> compactingCF = ConcurrentHashMultiset.create();\n\n    public final ActiveCompactions active = new ActiveCompactions();\n\n    // used to temporarily pause non-strategy managed compactions (like index summary redistribution)\n    private final AtomicInteger globalCompactionPauseCount = new AtomicInteger(0);\n\n    private final RateLimiter compactionRateLimiter = RateLimiter.create(Double.MAX_VALUE);\n\n    public CompactionMetrics getMetrics()\n    {\n        return metrics;\n    }\n\n    /**\n     * Gets compaction rate limiter.\n     * Rate unit is bytes per sec.\n     *\n     * @return RateLimiter with rate limit set\n     */\n    public RateLimiter getRateLimiter()\n    {\n        setRate(DatabaseDescriptor.getCompactionThroughputMbPerSec());\n        return compactionRateLimiter;\n    }\n\n    /**\n     * Sets the rate for the rate limiter. When compaction_throughput_mb_per_sec is 0 or node is bootstrapping,\n     * this sets the rate to Double.MAX_VALUE bytes per second.\n     * @param throughPutMbPerSec throughput to set in mb per second\n     */\n    public void setRate(final double throughPutMbPerSec)\n    {\n        double throughput = throughPutMbPerSec * 1024.0 * 1024.0;\n        // if throughput is set to 0, throttling is disabled\n        if (throughput == 0 || StorageService.instance.isBootstrapMode())\n            throughput = Double.MAX_VALUE;\n        if (compactionRateLimiter.getRate() != throughput)\n            compactionRateLimiter.setRate(throughput);\n    }\n\n    /**\n     * Call this whenever a compaction might be needed on the given columnfamily.\n     * It's okay to over-call (within reason) if a call is unnecessary, it will\n     * turn into a no-op in the bucketing/candidate-scan phase.\n     */\n    public List<Future<?>> submitBackground(final ColumnFamilyStore cfs)\n    {\n        if (cfs.isAutoCompactionDisabled())\n        {\n            logger.trace(\"Autocompaction is disabled\");\n            return Collections.emptyList();\n        }\n\n        /**\n         * If a CF is currently being compacted, and there are no idle threads, submitBackground should be a no-op;\n         * we can wait for the current compaction to finish and re-submit when more information is available.\n         * Otherwise, we should submit at least one task to prevent starvation by busier CFs, and more if there\n         * are idle threads stil. (CASSANDRA-4310)\n         */\n        int count = compactingCF.count(cfs);\n        if (count > 0 && executor.getActiveCount() >= executor.getMaximumPoolSize())\n        {\n            logger.trace(\"Background compaction is still running for {}.{} ({} remaining). Skipping\",\n                         cfs.keyspace.getName(), cfs.name, count);\n            return Collections.emptyList();\n        }\n\n        logger.trace(\"Scheduling a background task check for {}.{} with {}\",\n                     cfs.keyspace.getName(),\n                     cfs.name,\n                     cfs.getCompactionStrategyManager().getName());\n\n        List<Future<?>> futures = new ArrayList<>(1);\n        Future<?> fut = executor.submitIfRunning(new BackgroundCompactionCandidate(cfs), \"background task\");\n        if (!fut.isCancelled())\n            futures.add(fut);\n        else\n            compactingCF.remove(cfs);\n        return futures;\n    }\n\n    public boolean isCompacting(Iterable<ColumnFamilyStore> cfses, Predicate<SSTableReader> sstablePredicate)\n    {\n        for (ColumnFamilyStore cfs : cfses)\n            if (cfs.getTracker().getCompacting().stream().anyMatch(sstablePredicate))\n                return true;\n        return false;\n    }\n\n    /**\n     * Shutdowns both compaction and validation executors, cancels running compaction / validation,\n     * and waits for tasks to complete if tasks were not cancelable.\n     */\n    public void forceShutdown()\n    {\n        // shutdown executors to prevent further submission\n        executor.shutdown();\n        validationExecutor.shutdown();\n        viewBuildExecutor.shutdown();\n        cacheCleanupExecutor.shutdown();\n\n        // interrupt compactions and validations\n        for (Holder compactionHolder : active.getCompactions())\n        {\n            compactionHolder.stop();\n        }\n\n        // wait for tasks to terminate\n        // compaction tasks are interrupted above, so it shuold be fairy quick\n        // until not interrupted tasks to complete.\n        for (ExecutorService exec : Arrays.asList(executor, validationExecutor, viewBuildExecutor, cacheCleanupExecutor))\n        {\n            try\n            {\n                if (!exec.awaitTermination(1, TimeUnit.MINUTES))\n                    logger.warn(\"Failed to wait for compaction executors shutdown\");\n            }\n            catch (InterruptedException e)\n            {\n                logger.error(\"Interrupted while waiting for tasks to be terminated\", e);\n            }\n        }\n    }\n\n    public void finishCompactionsAndShutdown(long timeout, TimeUnit unit) throws InterruptedException\n    {\n        executor.shutdown();\n        executor.awaitTermination(timeout, unit);\n    }\n\n    // the actual sstables to compact are not determined until we run the BCT; that way, if new sstables\n    // are created between task submission and execution, we execute against the most up-to-date information\n    @VisibleForTesting\n    class BackgroundCompactionCandidate implements Runnable\n    {\n        private final ColumnFamilyStore cfs;\n\n        BackgroundCompactionCandidate(ColumnFamilyStore cfs)\n        {\n            compactingCF.add(cfs);\n            this.cfs = cfs;\n        }\n\n        public void run()\n        {\n            boolean ranCompaction = false;\n            try\n            {\n                logger.trace(\"Checking {}.{}\", cfs.keyspace.getName(), cfs.name);\n                if (!cfs.isValid())\n                {\n                    logger.trace(\"Aborting compaction for dropped CF\");\n                    return;\n                }\n\n                CompactionStrategyManager strategy = cfs.getCompactionStrategyManager();\n                AbstractCompactionTask task = strategy.getNextBackgroundTask(getDefaultGcBefore(cfs, FBUtilities.nowInSeconds()));\n                if (task == null)\n                {\n                    if (DatabaseDescriptor.automaticSSTableUpgrade())\n                        ranCompaction = maybeRunUpgradeTask(strategy);\n                }\n                else\n                {\n                    task.execute(active);\n                    ranCompaction = true;\n                }\n            }\n            finally\n            {\n                compactingCF.remove(cfs);\n            }\n            if (ranCompaction) // only submit background if we actually ran a compaction - otherwise we end up in an infinite loop submitting noop background tasks\n                submitBackground(cfs);\n        }\n\n        boolean maybeRunUpgradeTask(CompactionStrategyManager strategy)\n        {\n            logger.debug(\"Checking for upgrade tasks {}.{}\", cfs.keyspace.getName(), cfs.getTableName());\n            try\n            {\n                if (currentlyBackgroundUpgrading.incrementAndGet() <= DatabaseDescriptor.maxConcurrentAutoUpgradeTasks())\n                {\n                    AbstractCompactionTask upgradeTask = strategy.findUpgradeSSTableTask();\n                    if (upgradeTask != null)\n                    {\n                        upgradeTask.execute(active);\n                        return true;\n                    }\n                }\n            }\n            finally\n            {\n                currentlyBackgroundUpgrading.decrementAndGet();\n            }\n            logger.trace(\"No tasks available\");\n            return false;\n        }\n    }\n\n    @VisibleForTesting\n    public BackgroundCompactionCandidate getBackgroundCompactionCandidate(ColumnFamilyStore cfs)\n    {\n        return new BackgroundCompactionCandidate(cfs);\n    }\n\n    /**\n     * Run an operation over all sstables using jobs threads\n     *\n     * @param cfs the column family store to run the operation on\n     * @param operation the operation to run\n     * @param jobs the number of threads to use - 0 means use all available. It never uses more than concurrent_compactors threads\n     * @return status of the operation\n     * @throws ExecutionException\n     * @throws InterruptedException\n     */\n    @SuppressWarnings(\"resource\")\n    private AllSSTableOpStatus parallelAllSSTableOperation(final ColumnFamilyStore cfs, final OneSSTableOperation operation, int jobs, OperationType operationType) throws ExecutionException, InterruptedException\n    {\n        logger.info(\"Starting {} for {}.{}\", operationType, cfs.keyspace.getName(), cfs.getTableName());\n        List<LifecycleTransaction> transactions = new ArrayList<>();\n        List<Future<?>> futures = new ArrayList<>();\n        try (LifecycleTransaction compacting = cfs.markAllCompacting(operationType))\n        {\n            if (compacting == null)\n                return AllSSTableOpStatus.UNABLE_TO_CANCEL;\n\n            Iterable<SSTableReader> sstables = Lists.newArrayList(operation.filterSSTables(compacting));\n            if (Iterables.isEmpty(sstables))\n            {\n                logger.info(\"No sstables to {} for {}.{}\", operationType.name(), cfs.keyspace.getName(), cfs.name);\n                return AllSSTableOpStatus.SUCCESSFUL;\n            }\n\n            for (final SSTableReader sstable : sstables)\n            {\n                final LifecycleTransaction txn = compacting.split(singleton(sstable));\n                transactions.add(txn);\n                Callable<Object> callable = new Callable<Object>()\n                {\n                    @Override\n                    public Object call() throws Exception\n                    {\n                        operation.execute(txn);\n                        return this;\n                    }\n                };\n                Future<?> fut = executor.submitIfRunning(callable, \"paralell sstable operation\");\n                if (!fut.isCancelled())\n                    futures.add(fut);\n                else\n                    return AllSSTableOpStatus.ABORTED;\n\n                if (jobs > 0 && futures.size() == jobs)\n                {\n                    Future<?> f = FBUtilities.waitOnFirstFuture(futures);\n                    futures.remove(f);\n                }\n            }\n            FBUtilities.waitOnFutures(futures);\n            assert compacting.originals().isEmpty();\n            logger.info(\"Finished {} for {}.{} successfully\", operationType, cfs.keyspace.getName(), cfs.getTableName());\n            return AllSSTableOpStatus.SUCCESSFUL;\n        }\n        finally\n        {\n            // wait on any unfinished futures to make sure we don't close an ongoing transaction\n            try\n            {\n                FBUtilities.waitOnFutures(futures);\n            }\n            catch (Throwable t)\n            {\n               // these are handled/logged in CompactionExecutor#afterExecute\n            }\n            Throwable fail = Throwables.close(null, transactions);\n            if (fail != null)\n                logger.error(\"Failed to cleanup lifecycle transactions ({} for {}.{})\", operationType, cfs.keyspace.getName(), cfs.getTableName(), fail);\n        }\n    }\n\n    private static interface OneSSTableOperation\n    {\n        Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction);\n        void execute(LifecycleTransaction input) throws IOException;\n    }\n\n    public enum AllSSTableOpStatus\n    {\n        SUCCESSFUL(0),\n        ABORTED(1),\n        UNABLE_TO_CANCEL(2);\n\n        public final int statusCode;\n\n        AllSSTableOpStatus(int statusCode)\n        {\n            this.statusCode = statusCode;\n        }\n    }\n\n    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData,\n                                           int jobs)\n    throws InterruptedException, ExecutionException\n    {\n        return performScrub(cfs, skipCorrupted, checkData, false, jobs);\n    }\n\n    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData,\n                                           final boolean reinsertOverflowedTTL, int jobs)\n    throws InterruptedException, ExecutionException\n    {\n        return parallelAllSSTableOperation(cfs, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction input)\n            {\n                return input.originals();\n            }\n\n            @Override\n            public void execute(LifecycleTransaction input)\n            {\n                scrubOne(cfs, input, skipCorrupted, checkData, reinsertOverflowedTTL, active);\n            }\n        }, jobs, OperationType.SCRUB);\n    }\n\n    public AllSSTableOpStatus performVerify(ColumnFamilyStore cfs, Verifier.Options options) throws InterruptedException, ExecutionException\n    {\n        assert !cfs.isIndex();\n        return parallelAllSSTableOperation(cfs, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction input)\n            {\n                return input.originals();\n            }\n\n            @Override\n            public void execute(LifecycleTransaction input)\n            {\n                verifyOne(cfs, input.onlyOne(), options, active);\n            }\n        }, 0, OperationType.VERIFY);\n    }\n\n    public AllSSTableOpStatus performSSTableRewrite(final ColumnFamilyStore cfs, final boolean excludeCurrentVersion, int jobs) throws InterruptedException, ExecutionException\n    {\n        return parallelAllSSTableOperation(cfs, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                List<SSTableReader> sortedSSTables = Lists.newArrayList(transaction.originals());\n                Collections.sort(sortedSSTables, SSTableReader.sizeComparator.reversed());\n                Iterator<SSTableReader> iter = sortedSSTables.iterator();\n                while (iter.hasNext())\n                {\n                    SSTableReader sstable = iter.next();\n                    if (excludeCurrentVersion && sstable.descriptor.version.equals(sstable.descriptor.getFormat().getLatestVersion()))\n                    {\n                        transaction.cancel(sstable);\n                        iter.remove();\n                    }\n                }\n                return sortedSSTables;\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn)\n            {\n                AbstractCompactionTask task = cfs.getCompactionStrategyManager().getCompactionTask(txn, NO_GC, Long.MAX_VALUE);\n                task.setUserDefined(true);\n                task.setCompactionType(OperationType.UPGRADE_SSTABLES);\n                task.execute(active);\n            }\n        }, jobs, OperationType.UPGRADE_SSTABLES);\n    }\n\n    public AllSSTableOpStatus performCleanup(final ColumnFamilyStore cfStore, int jobs) throws InterruptedException, ExecutionException\n    {\n        assert !cfStore.isIndex();\n        Keyspace keyspace = cfStore.keyspace;\n        if (!StorageService.instance.isJoined())\n        {\n            logger.info(\"Cleanup cannot run before a node has joined the ring\");\n            return AllSSTableOpStatus.ABORTED;\n        }\n        // if local ranges is empty, it means no data should remain\n        final RangesAtEndpoint replicas = StorageService.instance.getLocalReplicas(keyspace.getName());\n        final Set<Range<Token>> allRanges = replicas.ranges();\n        final Set<Range<Token>> transientRanges = replicas.onlyTransient().ranges();\n        final Set<Range<Token>> fullRanges = replicas.onlyFull().ranges();\n        final boolean hasIndexes = cfStore.indexManager.hasIndexes();\n\n        return parallelAllSSTableOperation(cfStore, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                List<SSTableReader> sortedSSTables = Lists.newArrayList(transaction.originals());\n                Iterator<SSTableReader> sstableIter = sortedSSTables.iterator();\n                int totalSSTables = 0;\n                int skippedSStables = 0;\n                while (sstableIter.hasNext())\n                {\n                    SSTableReader sstable = sstableIter.next();\n                    boolean needsCleanupFull = needsCleanup(sstable, fullRanges);\n                    boolean needsCleanupTransient = !transientRanges.isEmpty() && sstable.isRepaired() && needsCleanup(sstable, transientRanges);\n                    //If there are no ranges for which the table needs cleanup either due to lack of intersection or lack\n                    //of the table being repaired.\n                    totalSSTables++;\n                    if (!needsCleanupFull && !needsCleanupTransient)\n                    {\n                        logger.debug(\"Skipping {} ([{}, {}]) for cleanup; all rows should be kept. Needs cleanup full ranges: {} Needs cleanup transient ranges: {} Repaired: {}\",\n                                    sstable,\n                                    sstable.first.getToken(),\n                                    sstable.last.getToken(),\n                                    needsCleanupFull,\n                                    needsCleanupTransient,\n                                    sstable.isRepaired());\n                        sstableIter.remove();\n                        transaction.cancel(sstable);\n                        skippedSStables++;\n                    }\n                }\n                logger.info(\"Skipping cleanup for {}/{} sstables for {}.{} since they are fully contained in owned ranges (full ranges: {}, transient ranges: {})\",\n                            skippedSStables, totalSSTables, cfStore.keyspace.getName(), cfStore.getTableName(), fullRanges, transientRanges);\n                sortedSSTables.sort(SSTableReader.sizeComparator);\n                return sortedSSTables;\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn) throws IOException\n            {\n                CleanupStrategy cleanupStrategy = CleanupStrategy.get(cfStore, allRanges, transientRanges, txn.onlyOne().isRepaired(), FBUtilities.nowInSeconds());\n                doCleanupOne(cfStore, txn, cleanupStrategy, replicas.ranges(), hasIndexes);\n            }\n        }, jobs, OperationType.CLEANUP);\n    }\n\n    public AllSSTableOpStatus performGarbageCollection(final ColumnFamilyStore cfStore, TombstoneOption tombstoneOption, int jobs) throws InterruptedException, ExecutionException\n    {\n        assert !cfStore.isIndex();\n\n        return parallelAllSSTableOperation(cfStore, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                Iterable<SSTableReader> originals = transaction.originals();\n                if (cfStore.getCompactionStrategyManager().onlyPurgeRepairedTombstones())\n                    originals = Iterables.filter(originals, SSTableReader::isRepaired);\n                List<SSTableReader> sortedSSTables = Lists.newArrayList(originals);\n                Collections.sort(sortedSSTables, SSTableReader.maxTimestampAscending);\n                return sortedSSTables;\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn) throws IOException\n            {\n                logger.debug(\"Garbage collecting {}\", txn.originals());\n                CompactionTask task = new CompactionTask(cfStore, txn, getDefaultGcBefore(cfStore, FBUtilities.nowInSeconds()))\n                {\n                    @Override\n                    protected CompactionController getCompactionController(Set<SSTableReader> toCompact)\n                    {\n                        return new CompactionController(cfStore, toCompact, gcBefore, null, tombstoneOption);\n                    }\n\n                    @Override\n                    protected int getLevel()\n                    {\n                        return txn.onlyOne().getSSTableLevel();\n                    }\n                };\n                task.setUserDefined(true);\n                task.setCompactionType(OperationType.GARBAGE_COLLECT);\n                task.execute(active);\n            }\n        }, jobs, OperationType.GARBAGE_COLLECT);\n    }\n\n    public AllSSTableOpStatus relocateSSTables(final ColumnFamilyStore cfs, int jobs) throws ExecutionException, InterruptedException\n    {\n        if (!cfs.getPartitioner().splitter().isPresent())\n        {\n            logger.info(\"Partitioner does not support splitting\");\n            return AllSSTableOpStatus.ABORTED;\n        }\n\n        if (StorageService.instance.getLocalReplicas(cfs.keyspace.getName()).isEmpty())\n        {\n            logger.info(\"Relocate cannot run before a node has joined the ring\");\n            return AllSSTableOpStatus.ABORTED;\n        }\n\n        final DiskBoundaries diskBoundaries = cfs.getDiskBoundaries();\n\n        return parallelAllSSTableOperation(cfs, new OneSSTableOperation()\n        {\n            @Override\n            public Iterable<SSTableReader> filterSSTables(LifecycleTransaction transaction)\n            {\n                Set<SSTableReader> originals = Sets.newHashSet(transaction.originals());\n                Set<SSTableReader> needsRelocation = originals.stream().filter(s -> !inCorrectLocation(s)).collect(Collectors.toSet());\n                transaction.cancel(Sets.difference(originals, needsRelocation));\n\n                Map<Integer, List<SSTableReader>> groupedByDisk = groupByDiskIndex(needsRelocation);\n\n                int maxSize = 0;\n                for (List<SSTableReader> diskSSTables : groupedByDisk.values())\n                    maxSize = Math.max(maxSize, diskSSTables.size());\n\n                List<SSTableReader> mixedSSTables = new ArrayList<>();\n\n                for (int i = 0; i < maxSize; i++)\n                    for (List<SSTableReader> diskSSTables : groupedByDisk.values())\n                        if (i < diskSSTables.size())\n                            mixedSSTables.add(diskSSTables.get(i));\n\n                return mixedSSTables;\n            }\n\n            public Map<Integer, List<SSTableReader>> groupByDiskIndex(Set<SSTableReader> needsRelocation)\n            {\n                return needsRelocation.stream().collect(Collectors.groupingBy((s) -> diskBoundaries.getDiskIndex(s)));\n            }\n\n            private boolean inCorrectLocation(SSTableReader sstable)\n            {\n                if (!cfs.getPartitioner().splitter().isPresent())\n                    return true;\n\n                // Compare the expected data directory for the sstable with its current data directory\n                Directories.DataDirectory currentDirectory = cfs.getDirectories().getDataDirectoryForFile(sstable.descriptor);\n                return diskBoundaries.isInCorrectLocation(sstable, currentDirectory);\n            }\n\n            @Override\n            public void execute(LifecycleTransaction txn)\n            {\n                logger.debug(\"Relocating {}\", txn.originals());\n                AbstractCompactionTask task = cfs.getCompactionStrategyManager().getCompactionTask(txn, NO_GC, Long.MAX_VALUE);\n                task.setUserDefined(true);\n                task.setCompactionType(OperationType.RELOCATE);\n                task.execute(active);\n            }\n        }, jobs, OperationType.RELOCATE);\n    }\n\n    /**\n     * Splits the given token ranges of the given sstables into a pending repair silo\n     */\n    public ListenableFuture<?> submitPendingAntiCompaction(ColumnFamilyStore cfs,\n                                                           RangesAtEndpoint tokenRanges,\n                                                           Refs<SSTableReader> sstables,\n                                                           LifecycleTransaction txn,\n                                                           UUID sessionId,\n                                                           BooleanSupplier isCancelled)\n    {\n        Runnable runnable = new WrappedRunnable()\n        {\n            protected void runMayThrow() throws Exception\n            {\n                try (TableMetrics.TableTimer.Context ctx = cfs.metric.anticompactionTime.time())\n                {\n                    performAnticompaction(cfs, tokenRanges, sstables, txn, sessionId, isCancelled);\n                }\n            }\n        };\n\n        ListenableFuture<?> task = null;\n        try\n        {\n            task = executor.submitIfRunning(runnable, \"pending anticompaction\");\n            return task;\n        }\n        finally\n        {\n            if (task == null || task.isCancelled())\n            {\n                sstables.release();\n                txn.abort();\n            }\n        }\n    }\n\n    /**\n     * for sstables that are fully contained in the given ranges, just rewrite their metadata with\n     * the pending repair id and remove them from the transaction\n     */\n    private static void mutateFullyContainedSSTables(ColumnFamilyStore cfs,\n                                                     Refs<SSTableReader> refs,\n                                                     Iterator<SSTableReader> sstableIterator,\n                                                     Collection<Range<Token>> ranges,\n                                                     LifecycleTransaction txn,\n                                                     UUID sessionID,\n                                                     boolean isTransient) throws IOException\n    {\n        if (ranges.isEmpty())\n            return;\n\n        List<Range<Token>> normalizedRanges = Range.normalize(ranges);\n\n        Set<SSTableReader> fullyContainedSSTables = findSSTablesToAnticompact(sstableIterator, normalizedRanges, sessionID);\n\n        cfs.metric.bytesMutatedAnticompaction.inc(SSTableReader.getTotalBytes(fullyContainedSSTables));\n        cfs.getCompactionStrategyManager().mutateRepaired(fullyContainedSSTables, UNREPAIRED_SSTABLE, sessionID, isTransient);\n        // since we're just re-writing the sstable metdata for the fully contained sstables, we don't want\n        // them obsoleted when the anti-compaction is complete. So they're removed from the transaction here\n        txn.cancel(fullyContainedSSTables);\n        refs.release(fullyContainedSSTables);\n    }\n\n    /**\n     * Make sure the {validatedForRepair} are marked for compaction before calling this.\n     *\n     * Caller must reference the validatedForRepair sstables (via ParentRepairSession.getActiveRepairedSSTableRefs(..)).\n     *\n     * @param cfs\n     * @param replicas token ranges to be repaired\n     * @param validatedForRepair SSTables containing the repaired ranges. Should be referenced before passing them.\n     * @param sessionID the repair session we're anti-compacting for\n     * @param isCancelled function that indicates if active anti-compaction should be canceled\n     * @throws InterruptedException\n     * @throws IOException\n     */\n    public void performAnticompaction(ColumnFamilyStore cfs,\n                                      RangesAtEndpoint replicas,\n                                      Refs<SSTableReader> validatedForRepair,\n                                      LifecycleTransaction txn,\n                                      UUID sessionID,\n                                      BooleanSupplier isCancelled) throws IOException\n    {\n        try\n        {\n            ActiveRepairService.ParentRepairSession prs = ActiveRepairService.instance.getParentRepairSession(sessionID);\n            Preconditions.checkArgument(!prs.isPreview(), \"Cannot anticompact for previews\");\n            Preconditions.checkArgument(!replicas.isEmpty(), \"No ranges to anti-compact\");\n\n            if (logger.isInfoEnabled())\n                logger.info(\"{} Starting anticompaction for {}.{} on {}/{} sstables\", PreviewKind.NONE.logPrefix(sessionID), cfs.keyspace.getName(), cfs.getTableName(), validatedForRepair.size(), cfs.getLiveSSTables().size());\n            if (logger.isTraceEnabled())\n                logger.trace(\"{} Starting anticompaction for ranges {}\", PreviewKind.NONE.logPrefix(sessionID), replicas);\n\n            Set<SSTableReader> sstables = new HashSet<>(validatedForRepair);\n            validateSSTableBoundsForAnticompaction(sessionID, sstables, replicas);\n            mutateFullyContainedSSTables(cfs, validatedForRepair, sstables.iterator(), replicas.onlyFull().ranges(), txn, sessionID, false);\n            mutateFullyContainedSSTables(cfs, validatedForRepair, sstables.iterator(), replicas.onlyTransient().ranges(), txn, sessionID, true);\n\n            assert txn.originals().equals(sstables);\n            if (!sstables.isEmpty())\n                doAntiCompaction(cfs, replicas, txn, sessionID, isCancelled);\n            txn.finish();\n        }\n        finally\n        {\n            validatedForRepair.release();\n            txn.close();\n        }\n\n        logger.info(\"{} Completed anticompaction successfully\", PreviewKind.NONE.logPrefix(sessionID));\n    }\n\n    static void validateSSTableBoundsForAnticompaction(UUID sessionID,\n                                                       Collection<SSTableReader> sstables,\n                                                       RangesAtEndpoint ranges)\n    {\n        List<Range<Token>> normalizedRanges = Range.normalize(ranges.ranges());\n        for (SSTableReader sstable : sstables)\n        {\n            Bounds<Token> bounds = new Bounds<>(sstable.first.getToken(), sstable.last.getToken());\n\n            if (!Iterables.any(normalizedRanges, r -> (r.contains(bounds.left) && r.contains(bounds.right)) || r.intersects(bounds)))\n            {\n                // this should never happen - in PendingAntiCompaction#getSSTables we select all sstables that intersect the repaired ranges, that can't have changed here\n                String message = String.format(\"%s SSTable %s (%s) does not intersect repaired ranges %s, this sstable should not have been included.\",\n                                               PreviewKind.NONE.logPrefix(sessionID), sstable, bounds, normalizedRanges);\n                logger.error(message);\n                throw new IllegalStateException(message);\n            }\n        }\n\n    }\n\n    \nCompactionManager.class\npublic static final CompactionManager \nCompactionManager()\nCompactionManager.createWriterForAntiCompaction\nCompactionManager.createWriterForAntiCompaction\nCompactionManager.createWriterForAntiCompaction\nCompactionManager.instance.isCompacting\npublic int getConcurrentViewBuilders()\n    {\n        return DatabaseDescriptor.getConcurrentViewBuilders();\n    }\n\n    \npublic void setConcurrentViewBuilders(int value)\n    {\n        if (value <= 0)\n            throw new IllegalArgumentException(\"Number of concurrent view builders should be greater than 0.\");\n        DatabaseDescriptor.setConcurrentViewBuilders(value);\n        CompactionManager.instance.setConcurrentViewBuilders(DatabaseDescriptor.getConcurrentViewBuilders());\n    }\n\n    "
        },
        {
            "sender": "Developer Filter",
            "content": "Filter of the code context that is related to configuration: ```java\npublic int getConcurrentViewBuilders()\n{\n    return DatabaseDescriptor.getConcurrentViewBuilders();\n}\n\npublic void setConcurrentViewBuilders(int value)\n{\n    if (value <= 0)\n        throw new IllegalArgumentException(\"Number of concurrent view builders should be greater than 0.\");\n    DatabaseDescriptor.setConcurrentViewBuilders(value);\n    CompactionManager.instance.setConcurrentViewBuilders(DatabaseDescriptor.getConcurrentViewBuilders());\n}\n```"
        },
        {
            "sender": "Developer summary",
            "content": "Summary of the extracted code context: The configuration for `concurrent_materialized_view_builders` is responsible for managing the number of concurrent view building processes within the system. This configuration is accessed and modified through two primary methods: `getConcurrentViewBuilders()` and `setConcurrentViewBuilders(int value)`.\n\n### Purpose\nThe primary purpose of this configuration is to define how many concurrent tasks can be executed for building materialized views in the database. Materialized views are used to provide a pre-computed view of data that can improve query performance and simplify data access patterns.\n\n### Functionality\n- **Getter Method**: The `getConcurrentViewBuilders()` method retrieves the current setting for the number of concurrent view builders from the `DatabaseDescriptor`, which acts as a centralized configuration management component.\n  \n- **Setter Method**: The `setConcurrentViewBuilders(int value)` method allows for updating the number of concurrent view builders. It includes validation to ensure that the value is greater than zero, throwing an `IllegalArgumentException` if the provided value is invalid. Upon successfully setting a new value, it updates the `DatabaseDescriptor` and also calls the `setConcurrentViewBuilders` method on the `CompactionManager` instance to ensure that the new configuration is applied consistently across the system.\n\n### Dependencies and Interactions\n- **DatabaseDescriptor**: This class serves as the main configuration repository for the system, holding various settings, including the number of concurrent view builders. The interaction with `DatabaseDescriptor` is crucial as it centralizes configuration management, ensuring that changes are reflected throughout the application.\n\n- **CompactionManager**: The `CompactionManager` is directly involved in managing the execution of compaction tasks, including those related to materialized views. By updating the concurrent view builders in the `CompactionManager`, the system ensures that the new configuration is utilized during the compaction process.\n\nOverall, the `concurrent_materialized_view_builders` configuration plays a vital role in controlling the concurrency of view building tasks, facilitating efficient data management and retrieval within the system."
        },
        {
            "sender": "performance agent sensitivity",
            "content": "{\"Role\": \"You are a Performance Sensitivity Agent responsible for determining if a configuration is performance-sensitive within a software system.\", \"Requirement\": \"Using developer-provided information as a secondary reference, analyze the configuration-related code to determine if it is performance-sensitive. Focus exclusively on configurations with substantial system-level performance impact due to time-intensive or memory-intensive operations. Only configurations that directly cause significant performance variations through extremely time-consuming or memory-heavy operations should be classified as performance-sensitive. Minor or moderate impacts should not be considered.\", \"Output Format\": \"Respond strictly in one of the following formats: 'The configuration is Performance Sensitive: reason' or 'The configuration is Performance Insensitive: reason'. No other response format is acceptable. Provide a concise yet clear and detailed reason for your classification.\", \"configuration_name\": \"concurrent_materialized_view_builders\", \"code_context\": \"```java\\npublic int getConcurrentViewBuilders()\\n{\\n    return DatabaseDescriptor.getConcurrentViewBuilders();\\n}\\n\\npublic void setConcurrentViewBuilders(int value)\\n{\\n    if (value <= 0)\\n        throw new IllegalArgumentException(\\\"Number of concurrent view builders should be greater than 0.\\\");\\n    DatabaseDescriptor.setConcurrentViewBuilders(value);\\n    CompactionManager.instance.setConcurrentViewBuilders(DatabaseDescriptor.getConcurrentViewBuilders());\\n}\\n```\\n\\n--- Summary by Developer ---\\nThe configuration for `concurrent_materialized_view_builders` is responsible for managing the number of concurrent view building processes within the system. This configuration is accessed and modified through two primary methods: `getConcurrentViewBuilders()` and `setConcurrentViewBuilders(int value)`.\\n\\n### Purpose\\nThe primary purpose of this configuration is to define how many concurrent tasks can be executed for building materialized views in the database. Materialized views are used to provide a pre-computed view of data that can improve query performance and simplify data access patterns.\\n\\n### Functionality\\n- **Getter Method**: The `getConcurrentViewBuilders()` method retrieves the current setting for the number of concurrent view builders from the `DatabaseDescriptor`, which acts as a centralized configuration management component.\\n  \\n- **Setter Method**: The `setConcurrentViewBuilders(int value)` method allows for updating the number of concurrent view builders. It includes validation to ensure that the value is greater than zero, throwing an `IllegalArgumentException` if the provided value is invalid. Upon successfully setting a new value, it updates the `DatabaseDescriptor` and also calls the `setConcurrentViewBuilders` method on the `CompactionManager` instance to ensure that the new configuration is applied consistently across the system.\\n\\n### Dependencies and Interactions\\n- **DatabaseDescriptor**: This class serves as the main configuration repository for the system, holding various settings, including the number of concurrent view builders. The interaction with `DatabaseDescriptor` is crucial as it centralizes configuration management, ensuring that changes are reflected throughout the application.\\n\\n- **CompactionManager**: The `CompactionManager` is directly involved in managing the execution of compaction tasks, including those related to materialized views. By updating the concurrent view builders in the `CompactionManager`, the system ensures that the new configuration is utilized during the compaction process.\\n\\nOverall, the `concurrent_materialized_view_builders` configuration plays a vital role in controlling the concurrency of view building tasks, facilitating efficient data management and retrieval within the system.\"}"
        },
        {
            "sender": "PerformanceSensitivity",
            "content": "The configuration is Performance Sensitive: The `concurrent_materialized_view_builders` directly impacts the number of concurrent tasks for building materialized views, which can be time-intensive operations. Increasing the number of concurrent view builders can lead to substantial variations in performance, especially during heavy data processing scenarios. If too many concurrent tasks are allowed, it could lead to resource contention, increased latency, and potential memory overhead, significantly affecting overall system performance. Conversely, setting this value too low could underutilize system resources, leading to slower data retrieval and processing times. Therefore, this configuration has a substantial impact on system-level performance."
        }
    ]
}