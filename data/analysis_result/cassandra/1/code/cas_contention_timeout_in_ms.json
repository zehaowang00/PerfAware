{
    "performance_sensitive": "Yes",
    "config_name": "cas_contention_timeout_in_ms",
    "messages": [
        {
            "sender": "System",
            "content": "Initialize the analysis for the target system and configuration."
        },
        {
            "sender": "Developer",
            "content": "/**\n     * Performs the Paxos rounds for a given proposal, retrying when preempted until the timeout.\n     *\n     * <p>The main 'configurable' of this method is the {@code createUpdateProposal} method: it is called by the method\n     * once a ballot has been successfully 'prepared' to generate the update to 'propose' (and commit if the proposal is\n     * successful). That method also generates the result that the whole method will return. Note that due to retrying,\n     * this method may be called multiple times and does not have to return the same results.\n     *\n     * @param metadata the table to update with Paxos.\n     * @param key the partition updated.\n     * @param consistencyForPaxos the serial consistency of the operation (either {@link ConsistencyLevel#SERIAL} or\n     *     {@link ConsistencyLevel#LOCAL_SERIAL}).\n     * @param consistencyForReplayCommits the consistency for the commit phase of \"replayed\" in-progress operations.\n     * @param consistencyForCommit the consistency for the commit phase of _this_ operation update.\n     * @param state the client state.\n     * @param queryStartNanoTime the nano time for the start of the query this is part of. This is the base time for\n     *     timeouts.\n     * @param casMetrics the metrics to update for this operation.\n     * @param createUpdateProposal method called after a successful 'prepare' phase to obtain 1) the actual update of\n     *     this operation and 2) the result that the whole method should return. This can return {@code null} in the\n     *     special where, after having \"prepared\" (and thus potentially replayed in-progress upgdates), we don't want\n     *     to propose anything (the whole method then return {@code null}).\n     * @return the second element of the pair returned by {@code createUpdateProposal} (for the last call of that method\n     *     if that method is called multiple times due to retries).\n     */\nprivate static RowIterator doPaxos(TableMetadata metadata,\n                                       DecoratedKey key,\n                                       ConsistencyLevel consistencyForPaxos,\n                                       ConsistencyLevel consistencyForReplayCommits,\n                                       ConsistencyLevel consistencyForCommit,\n                                       ClientState state,\n                                       long queryStartNanoTime,\n                                       CASClientRequestMetrics casMetrics,\n                                       Supplier<Pair<PartitionUpdate, RowIterator>> createUpdateProposal)\n    throws UnavailableException, IsBootstrappingException, RequestFailureException, RequestTimeoutException, InvalidRequestException\n    {\n        int contentions = 0;\n        Keyspace keyspace = Keyspace.open(metadata.keyspace);\n        AbstractReplicationStrategy latestRs = keyspace.getReplicationStrategy();\n        try\n        {\n            consistencyForPaxos.validateForCas();\n            consistencyForReplayCommits.validateForCasCommit(latestRs);\n            consistencyForCommit.validateForCasCommit(latestRs);\n\n            long timeoutNanos = DatabaseDescriptor.getCasContentionTimeout(NANOSECONDS);\n            while (System.nanoTime() - queryStartNanoTime < timeoutNanos)\n            {\n                // for simplicity, we'll do a single liveness check at the start of each attempt\n                ReplicaPlan.ForPaxosWrite replicaPlan = ReplicaPlans.forPaxos(keyspace, key, consistencyForPaxos);\n                latestRs = replicaPlan.replicationStrategy();\n                PaxosBallotAndContention pair = beginAndRepairPaxos(queryStartNanoTime,\n                                                                    key,\n                                                                    metadata,\n                                                                    replicaPlan,\n                                                                    consistencyForPaxos,\n                                                                    consistencyForReplayCommits,\n                                                                    casMetrics,\n                                                                    state);\n\n                final UUID ballot = pair.ballot;\n                contentions += pair.contentions;\n\n                Pair<PartitionUpdate, RowIterator> proposalPair = createUpdateProposal.get();\n                // See method javadoc: null here is code for \"stop here and return null\".\n                if (proposalPair == null)\n                    return null;\n\n                Commit proposal = Commit.newProposal(ballot, proposalPair.left);\n                Tracing.trace(\"CAS precondition is met; proposing client-requested updates for {}\", ballot);\n                if (proposePaxos(proposal, replicaPlan, true, queryStartNanoTime))\n                {\n                    // We skip committing accepted updates when they are empty. This is an optimization which works\n                    // because we also skip replaying those same empty update in beginAndRepairPaxos (see the longer\n                    // comment there). As empty update are somewhat common (serial reads and non-applying CAS propose\n                    // them), this is worth bothering.\n                    if (!proposal.update.isEmpty())\n                        commitPaxos(proposal, consistencyForCommit, true, queryStartNanoTime);\n                    RowIterator result = proposalPair.right;\n                    if (result != null)\n                        Tracing.trace(\"CAS did not apply\");\n                    else\n                        Tracing.trace(\"CAS applied successfully\");\n                    return result;\n                }\n\n                Tracing.trace(\"Paxos proposal not accepted (pre-empted by a higher ballot)\");\n                contentions++;\n                Uninterruptibles.sleepUninterruptibly(ThreadLocalRandom.current().nextInt(100), TimeUnit.MILLISECONDS);\n                // continue to retry\n            }\n        }\n        catch (CasWriteTimeoutException e)\n        {\n            // Might be thrown by beginRepairAndPaxos. In that case, any contention that happened within the method and\n            // led up to the timeout was not accounted in our local 'contentions' variable and we add it now so it the\n            // contention recorded in the finally is correct.\n            contentions += e.contentions;\n            throw e;\n        }\n        catch (WriteTimeoutException e)\n        {\n            // Might be thrown by proposePaxos or commitPaxos\n            throw new CasWriteTimeoutException(e.writeType, e.consistency, e.received, e.blockFor, contentions);\n        }\n        finally\n        {\n            recordCasContention(metadata, key, casMetrics, contentions);\n        }\n\n        throw new CasWriteTimeoutException(WriteType.CAS, consistencyForPaxos, 0, consistencyForPaxos.blockFor(latestRs), contentions);\n    }\n\n    \n/**\n     * begin a Paxos session by sending a prepare request and completing any in-progress requests seen in the replies\n     *\n     * @return the Paxos ballot promised by the replicas if no in-progress requests were seen and a quorum of\n     * nodes have seen the mostRecentCommit.  Otherwise, return null.\n     */\nprivate static PaxosBallotAndContention beginAndRepairPaxos(long queryStartNanoTime,\n                                                                DecoratedKey key,\n                                                                TableMetadata metadata,\n                                                                ReplicaPlan.ForPaxosWrite paxosPlan,\n                                                                ConsistencyLevel consistencyForPaxos,\n                                                                ConsistencyLevel consistencyForCommit,\n                                                                CASClientRequestMetrics casMetrics,\n                                                                ClientState state)\n    throws WriteTimeoutException, WriteFailureException\n    {\n        long timeoutNanos = DatabaseDescriptor.getCasContentionTimeout(NANOSECONDS);\n\n        PrepareCallback summary = null;\n        int contentions = 0;\n        while (System.nanoTime() - queryStartNanoTime < timeoutNanos)\n        {\n            // We want a timestamp that is guaranteed to be unique for that node (so that the ballot is globally unique), but if we've got a prepare rejected\n            // already we also want to make sure we pick a timestamp that has a chance to be promised, i.e. one that is greater that the most recently known\n            // in progress (#5667). Lastly, we don't want to use a timestamp that is older than the last one assigned by ClientState or operations may appear\n            // out-of-order (#7801).\n            long minTimestampMicrosToUse = summary == null ? Long.MIN_VALUE : 1 + UUIDGen.microsTimestamp(summary.mostRecentInProgressCommit.ballot);\n            long ballotMicros = state.getTimestampForPaxos(minTimestampMicrosToUse);\n            // Note that ballotMicros is not guaranteed to be unique if two proposal are being handled concurrently by the same coordinator. But we still\n            // need ballots to be unique for each proposal so we have to use getRandomTimeUUIDFromMicros.\n            UUID ballot = UUIDGen.getRandomTimeUUIDFromMicros(ballotMicros);\n\n            // prepare\n            try\n            {\n                Tracing.trace(\"Preparing {}\", ballot);\n                Commit toPrepare = Commit.newPrepare(key, metadata, ballot);\n                summary = preparePaxos(toPrepare, paxosPlan, queryStartNanoTime);\n                if (!summary.promised)\n                {\n                    Tracing.trace(\"Some replicas have already promised a higher ballot than ours; aborting\");\n                    contentions++;\n                    // sleep a random amount to give the other proposer a chance to finish\n                    Uninterruptibles.sleepUninterruptibly(ThreadLocalRandom.current().nextInt(100), MILLISECONDS);\n                    continue;\n                }\n\n                Commit inProgress = summary.mostRecentInProgressCommit;\n                Commit mostRecent = summary.mostRecentCommit;\n\n                // If we have an in-progress ballot greater than the MRC we know, then it's an in-progress round that\n                // needs to be completed, so do it.\n                // One special case we make is for update that are empty (which are proposed by serial reads and\n                // non-applying CAS). While we could handle those as any other updates, we can optimize this somewhat by\n                // neither committing those empty updates, nor replaying in-progress ones. The reasoning is this: as the\n                // update is empty, we have nothing to apply to storage in the commit phase, so the only reason to commit\n                // would be to update the MRC. However, if we skip replaying those empty updates, then we don't need to\n                // update the MRC for following updates to make progress (that is, if we didn't had the empty update skip\n                // below _but_ skipped updating the MRC on empty updates, then we'd be stuck always proposing that same\n                // empty update). And the reason skipping that replay is safe is that when an operation tries to propose\n                // an empty value, there can be only 2 cases:\n                //  1) the propose succeed, meaning a quorum of nodes accept it, in which case we are guaranteed no earlier\n                //     pending operation can ever be replayed (which is what we want to guarantee with the empty update).\n                //  2) the propose does not succeed. But then the operation proposing the empty update will not succeed\n                //     either (it will retry or ultimately timeout), and we're actually ok if earlier pending operation gets\n                //     replayed in that case.\n                // Tl;dr, it is safe to skip committing empty updates _as long as_ we also skip replying them below. And\n                // doing is more efficient, so we do so.\n                if (!inProgress.update.isEmpty() && inProgress.isAfter(mostRecent))\n                {\n                    Tracing.trace(\"Finishing incomplete paxos round {}\", inProgress);\n                    casMetrics.unfinishedCommit.inc();\n                    Commit refreshedInProgress = Commit.newProposal(ballot, inProgress.update);\n                    if (proposePaxos(refreshedInProgress, paxosPlan, false, queryStartNanoTime))\n                    {\n                        commitPaxos(refreshedInProgress, consistencyForCommit, false, queryStartNanoTime);\n                    }\n                    else\n                    {\n                        Tracing.trace(\"Some replicas have already promised a higher ballot than ours; aborting\");\n                        // sleep a random amount to give the other proposer a chance to finish\n                        contentions++;\n                        Uninterruptibles.sleepUninterruptibly(ThreadLocalRandom.current().nextInt(100), MILLISECONDS);\n                    }\n                    continue;\n                }\n\n                // To be able to propose our value on a new round, we need a quorum of replica to have learn the previous one. Why is explained at:\n                // https://issues.apache.org/jira/browse/CASSANDRA-5062?focusedCommentId=13619810&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13619810)\n                // Since we waited for quorum nodes, if some of them haven't seen the last commit (which may just be a timing issue, but may also\n                // mean we lost messages), we pro-actively \"repair\" those nodes, and retry.\n                int nowInSec = Ints.checkedCast(TimeUnit.MICROSECONDS.toSeconds(ballotMicros));\n                Iterable<InetAddressAndPort> missingMRC = summary.replicasMissingMostRecentCommit(metadata, nowInSec);\n                if (Iterables.size(missingMRC) > 0)\n                {\n                    Tracing.trace(\"Repairing replicas that missed the most recent commit\");\n                    sendCommit(mostRecent, missingMRC);\n                    // TODO: provided commits don't invalid the prepare we just did above (which they don't), we could just wait\n                    // for all the missingMRC to acknowledge this commit and then move on with proposing our value. But that means\n                    // adding the ability to have commitPaxos block, which is exactly CASSANDRA-5442 will do. So once we have that\n                    // latter ticket, we can pass CL.ALL to the commit above and remove the 'continue'.\n                    continue;\n                }\n\n                return new PaxosBallotAndContention(ballot, contentions);\n            }\n            catch (WriteTimeoutException e)\n            {\n                // We're still doing preparation for the paxos rounds, so we want to use the CAS (see CASSANDRA-8672)\n                throw new CasWriteTimeoutException(WriteType.CAS, e.consistency, e.received, e.blockFor, contentions);\n            }\n        }\n\n        throw new CasWriteTimeoutException(WriteType.CAS, consistencyForPaxos, 0, consistencyForPaxos.blockFor(paxosPlan.replicationStrategy()), contentions);\n    }\n\n    \npublic Long getCasContentionTimeout() { return DatabaseDescriptor.getCasContentionTimeout(MILLISECONDS); }\n    \npublic long getCasContentionTimeout()\n    {\n        return DatabaseDescriptor.getCasContentionTimeout(MILLISECONDS);\n    }\n\n    "
        },
        {
            "sender": "PerformanceSensitivity",
            "content": "The configuration is Performance Sensitive: The `cas_contention_timeout_in_ms` directly influences the duration of the Paxos rounds, which are crucial for achieving consensus in distributed systems. A longer timeout can lead to increased waiting times during contention, potentially causing significant delays in operations and affecting overall system responsiveness. Since the Paxos protocol is inherently time-sensitive and can involve retries and contention, variations in this timeout can lead to substantial performance impacts, making it a critical configuration for system performance."
        }
    ]
}