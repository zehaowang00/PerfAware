{
    "performance_sensitive": "Yes",
    "config_name": "memtable_flush_writers",
    "messages": [
        {
            "sender": "System",
            "content": "Initialize the analysis for the target system and configuration."
        },
        {
            "sender": "Developer",
            "content": "public class ColumnFamilyStore implements ColumnFamilyStoreMBean\n{\n    private static final Logger logger = LoggerFactory.getLogger(ColumnFamilyStore.class);\n\n    /*\n    We keep a pool of threads for each data directory, size of each pool is memtable_flush_writers.\n    When flushing we start a Flush runnable in the flushExecutor. Flush calculates how to split the\n    memtable ranges over the existing data directories and creates a FlushRunnable for each of the directories.\n    The FlushRunnables are executed in the perDiskflushExecutors and the Flush will block until all FlushRunnables\n    are finished. By having flushExecutor size the same size as each of the perDiskflushExecutors we make sure we can\n    have that many flushes going at the same time.\n    */\n    private static final ThreadPoolExecutor flushExecutor = new JMXEnabledThreadPoolExecutor(DatabaseDescriptor.getFlushWriters(),\n                                                                                             Stage.KEEP_ALIVE_SECONDS,\n                                                                                             TimeUnit.SECONDS,\n                                                                                             new LinkedBlockingQueue<>(),\n                                                                                             new NamedThreadFactory(\"MemtableFlushWriter\"),\n                                                                                             \"internal\");\n\n    private static final PerDiskFlushExecutors perDiskflushExecutors = new PerDiskFlushExecutors(DatabaseDescriptor.getFlushWriters(),\n                                                                                                 DatabaseDescriptor.getNonLocalSystemKeyspacesDataFileLocations(),\n                                                                                                 DatabaseDescriptor.useSpecificLocationForLocalSystemData());\n\n    // post-flush executor is single threaded to provide guarantee that any flush Future on a CF will never return until prior flushes have completed\n    private static final ThreadPoolExecutor postFlushExecutor = new JMXEnabledThreadPoolExecutor(1,\n                                                                                                 Stage.KEEP_ALIVE_SECONDS,\n                                                                                                 TimeUnit.SECONDS,\n                                                                                                 new LinkedBlockingQueue<>(),\n                                                                                                 new NamedThreadFactory(\"MemtablePostFlush\"),\n                                                                                                 \"internal\");\n\n    private static final ThreadPoolExecutor reclaimExecutor = new JMXEnabledThreadPoolExecutor(1,\n                                                                                               Stage.KEEP_ALIVE_SECONDS,\n                                                                                               TimeUnit.SECONDS,\n                                                                                               new LinkedBlockingQueue<>(),\n                                                                                               new NamedThreadFactory(\"MemtableReclaimMemory\"),\n                                                                                               \"internal\");\n\n    private static final String[] COUNTER_NAMES = new String[]{\"table\", \"count\", \"error\", \"value\"};\n    private static final String[] COUNTER_DESCS = new String[]\n    { \"keyspace.tablename\",\n      \"number of occurances\",\n      \"error bounds\",\n      \"value\" };\n    private static final CompositeType COUNTER_COMPOSITE_TYPE;\n\n    private static final String SAMPLING_RESULTS_NAME = \"SAMPLING_RESULTS\";\n\n    public static final String SNAPSHOT_TRUNCATE_PREFIX = \"truncated\";\n    public static final String SNAPSHOT_DROP_PREFIX = \"dropped\";\n\n    static\n    {\n        try\n        {\n            OpenType<?>[] counterTypes = new OpenType[] { SimpleType.STRING, SimpleType.LONG, SimpleType.LONG, SimpleType.STRING };\n            COUNTER_COMPOSITE_TYPE = new CompositeType(SAMPLING_RESULTS_NAME, SAMPLING_RESULTS_NAME, COUNTER_NAMES, COUNTER_DESCS, counterTypes);\n        } catch (OpenDataException e)\n        {\n            throw new RuntimeException(e);\n        }\n    }\n\n    public final Keyspace keyspace;\n    public final String name;\n    public final TableMetadataRef metadata;\n    private final String mbeanName;\n    @Deprecated\n    private final String oldMBeanName;\n    private volatile boolean valid = true;\n\n    /**\n     * Memtables and SSTables on disk for this column family.\n     *\n     * We synchronize on the Tracker to ensure isolation when we want to make sure\n     * that the memtable we're acting on doesn't change out from under us.  I.e., flush\n     * syncronizes on it to make sure it can submit on both executors atomically,\n     * so anyone else who wants to make sure flush doesn't interfere should as well.\n     */\n    private final Tracker data;\n\n    /* The read order, used to track accesses to off-heap memtable storage */\n    public final OpOrder readOrdering = new OpOrder();\n\n    /* This is used to generate the next index for a SSTable */\n    private final AtomicInteger fileIndexGenerator = new AtomicInteger(0);\n\n    public final SecondaryIndexManager indexManager;\n    public final TableViews viewManager;\n\n    /* These are locally held copies to be changed from the config during runtime */\n    private volatile DefaultValue<Integer> minCompactionThreshold;\n    private volatile DefaultValue<Integer> maxCompactionThreshold;\n    private volatile DefaultValue<Double> crcCheckChance;\n\n    private final CompactionStrategyManager compactionStrategyManager;\n\n    private final Directories directories;\n\n    public final TableMetrics metric;\n    public volatile long sampleReadLatencyNanos;\n    public volatile long additionalWriteLatencyNanos;\n\n    private final CassandraTableWriteHandler writeHandler;\n    private final CassandraStreamManager streamManager;\n\n    private final TableRepairManager repairManager;\n\n    private final SSTableImporter sstableImporter;\n\n    private volatile boolean compactionSpaceCheck = true;\n\n    @VisibleForTesting\n    final DiskBoundaryManager diskBoundaryManager = new DiskBoundaryManager();\n\n    private volatile boolean neverPurgeTombstones = false;\n\n    public static void shutdownPostFlushExecutor() throws InterruptedException\n    {\n        postFlushExecutor.shutdown();\n        postFlushExecutor.awaitTermination(60, TimeUnit.SECONDS);\n    }\n\n    public static void shutdownExecutorsAndWait(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException\n    {\n        List<ExecutorService> executors = new ArrayList<>();\n        Collections.addAll(executors, reclaimExecutor, postFlushExecutor, flushExecutor);\n        perDiskflushExecutors.appendAllExecutors(executors);\n        ExecutorUtils.shutdownAndWait(timeout, unit, executors);\n    }\n\n    public void reload()\n    {\n        // metadata object has been mutated directly. make all the members jibe with new settings.\n\n        // only update these runtime-modifiable settings if they have not been modified.\n        if (!minCompactionThreshold.isModified())\n            for (ColumnFamilyStore cfs : concatWithIndexes())\n                cfs.minCompactionThreshold = new DefaultValue(metadata().params.compaction.minCompactionThreshold());\n        if (!maxCompactionThreshold.isModified())\n            for (ColumnFamilyStore cfs : concatWithIndexes())\n                cfs.maxCompactionThreshold = new DefaultValue(metadata().params.compaction.maxCompactionThreshold());\n        if (!crcCheckChance.isModified())\n            for (ColumnFamilyStore cfs : concatWithIndexes())\n                cfs.crcCheckChance = new DefaultValue(metadata().params.crcCheckChance);\n\n        compactionStrategyManager.maybeReload(metadata());\n\n        scheduleFlush();\n\n        indexManager.reload();\n\n        // If the CF comparator has changed, we need to change the memtable,\n        // because the old one still aliases the previous comparator.\n        if (data.getView().getCurrentMemtable().initialComparator != metadata().comparator)\n            switchMemtable();\n    }\n\n    void scheduleFlush()\n    {\n        int period = metadata().params.memtableFlushPeriodInMs;\n        if (period > 0)\n        {\n            logger.trace(\"scheduling flush in {} ms\", period);\n            WrappedRunnable runnable = new WrappedRunnable()\n            {\n                protected void runMayThrow()\n                {\n                    synchronized (data)\n                    {\n                        Memtable current = data.getView().getCurrentMemtable();\n                        // if we're not expired, we've been hit by a scheduled flush for an already flushed memtable, so ignore\n                        if (current.isExpired())\n                        {\n                            if (current.isClean())\n                            {\n                                // if we're still clean, instead of swapping just reschedule a flush for later\n                                scheduleFlush();\n                            }\n                            else\n                            {\n                                // we'll be rescheduled by the constructor of the Memtable.\n                                forceFlush();\n                            }\n                        }\n                    }\n                }\n            };\n            ScheduledExecutors.scheduledTasks.schedule(runnable, period, TimeUnit.MILLISECONDS);\n        }\n    }\n\n    public static Runnable getBackgroundCompactionTaskSubmitter()\n    {\n        return () -> {\n            for (Keyspace keyspace : Keyspace.all())\n                for (ColumnFamilyStore cfs : keyspace.getColumnFamilyStores())\n                    CompactionManager.instance.submitBackground(cfs);\n        };\n    }\n\n    public Map<String, String> getCompactionParameters()\n    {\n        return compactionStrategyManager.getCompactionParams().asMap();\n    }\n\n    public String getCompactionParametersJson()\n    {\n        return FBUtilities.json(getCompactionParameters());\n    }\n\n    public void setCompactionParameters(Map<String, String> options)\n    {\n        try\n        {\n            CompactionParams compactionParams = CompactionParams.fromMap(options);\n            compactionParams.validate();\n            compactionStrategyManager.setNewLocalCompactionStrategy(compactionParams);\n        }\n        catch (Throwable t)\n        {\n            logger.error(\"Could not set new local compaction strategy\", t);\n            // dont propagate the ConfigurationException over jmx, user will only see a ClassNotFoundException\n            throw new IllegalArgumentException(\"Could not set new local compaction strategy: \"+t.getMessage());\n        }\n    }\n\n    public void setCompactionParametersJson(String options)\n    {\n        setCompactionParameters(FBUtilities.fromJsonMap(options));\n    }\n\n    public Map<String,String> getCompressionParameters()\n    {\n        return metadata.getLocal().params.compression.asMap();\n    }\n\n    public String getCompressionParametersJson()\n    {\n        return FBUtilities.json(getCompressionParameters());\n    }\n\n    public void setCompressionParameters(Map<String,String> opts)\n    {\n        try\n        {\n            CompressionParams params = CompressionParams.fromMap(opts);\n            params.validate();\n            metadata.setLocalOverrides(metadata().unbuild().compression(params).build());\n        }\n        catch (ConfigurationException e)\n        {\n            throw new IllegalArgumentException(e.getMessage());\n        }\n    }\n\n    public void setCompressionParametersJson(String options)\n    {\n        setCompressionParameters(FBUtilities.fromJsonMap(options));\n    }\n\n    @VisibleForTesting\n    public ColumnFamilyStore(Keyspace keyspace,\n                             String columnFamilyName,\n                             int generation,\n                             TableMetadataRef metadata,\n                             Directories directories,\n                             boolean loadSSTables,\n                             boolean registerBookeeping,\n                             boolean offline)\n    {\n        assert directories != null;\n        assert metadata != null : \"null metadata for \" + keyspace + ':' + columnFamilyName;\n\n        this.keyspace = keyspace;\n        this.metadata = metadata;\n        this.directories = directories;\n        name = columnFamilyName;\n        minCompactionThreshold = new DefaultValue<>(metadata.get().params.compaction.minCompactionThreshold());\n        maxCompactionThreshold = new DefaultValue<>(metadata.get().params.compaction.maxCompactionThreshold());\n        crcCheckChance = new DefaultValue<>(metadata.get().params.crcCheckChance);\n        viewManager = keyspace.viewManager.forTable(metadata.id);\n        fileIndexGenerator.set(generation);\n        sampleReadLatencyNanos = DatabaseDescriptor.getReadRpcTimeout(NANOSECONDS) / 2;\n        additionalWriteLatencyNanos = DatabaseDescriptor.getWriteRpcTimeout(NANOSECONDS) / 2;\n\n        logger.info(\"Initializing {}.{}\", keyspace.getName(), name);\n\n        // Create Memtable only on online\n        Memtable initialMemtable = null;\n        if (DatabaseDescriptor.isDaemonInitialized())\n            initialMemtable = new Memtable(new AtomicReference<>(CommitLog.instance.getCurrentPosition()), this);\n        data = new Tracker(initialMemtable, loadSSTables);\n\n        // Note that this needs to happen before we load the first sstables, or the global sstable tracker will not\n        // be notified on the initial loading.\n        data.subscribe(StorageService.instance.sstablesTracker);\n\n        Collection<SSTableReader> sstables = null;\n        // scan for sstables corresponding to this cf and load them\n        if (data.loadsstables)\n        {\n            Directories.SSTableLister sstableFiles = directories.sstableLister(Directories.OnTxnErr.IGNORE).skipTemporary(true);\n            sstables = SSTableReader.openAll(sstableFiles.list().entrySet(), metadata);\n            data.addInitialSSTablesWithoutUpdatingSize(sstables);\n        }\n\n        // compaction strategy should be created after the CFS has been prepared\n        compactionStrategyManager = new CompactionStrategyManager(this);\n\n        if (maxCompactionThreshold.value() <= 0 || minCompactionThreshold.value() <=0)\n        {\n            logger.warn(\"Disabling compaction strategy by setting compaction thresholds to 0 is deprecated, set the compaction option 'enabled' to 'false' instead.\");\n            this.compactionStrategyManager.disable();\n        }\n\n        // create the private ColumnFamilyStores for the secondary column indexes\n        indexManager = new SecondaryIndexManager(this);\n        for (IndexMetadata info : metadata.get().indexes)\n        {\n            indexManager.addIndex(info, true);\n        }\n\n        metric = new TableMetrics(this);\n\n        if (data.loadsstables)\n        {\n            data.updateInitialSSTableSize(sstables);\n        }\n\n        if (registerBookeeping)\n        {\n            // register the mbean\n            mbeanName = getTableMBeanName(keyspace.getName(), name, isIndex());\n            oldMBeanName = getColumnFamilieMBeanName(keyspace.getName(), name, isIndex());\n\n            String[] objectNames = {mbeanName, oldMBeanName};\n            for (String objectName : objectNames)\n                MBeanWrapper.instance.registerMBean(this, objectName);\n        }\n        else\n        {\n            mbeanName = null;\n            oldMBeanName= null;\n        }\n        writeHandler = new CassandraTableWriteHandler(this);\n        streamManager = new CassandraStreamManager(this);\n        repairManager = new CassandraTableRepairManager(this);\n        sstableImporter = new SSTableImporter(this);\n    }\n\n    public static String getTableMBeanName(String ks, String name, boolean isIndex)\n    {\n        return String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,table=%s\",\n                      isIndex ? \"IndexTables\" : \"Tables\",\n                      ks, name);\n    }\n\n    public static String getColumnFamilieMBeanName(String ks, String name, boolean isIndex)\n    {\n       return String.format(\"org.apache.cassandra.db:type=%s,keyspace=%s,columnfamily=%s\",\n                            isIndex ? \"IndexColumnFamilies\" : \"ColumnFamilies\",\n                            ks, name);\n    }\n\n    public void updateSpeculationThreshold()\n    {\n        try\n        {\n            sampleReadLatencyNanos = metadata().params.speculativeRetry.calculateThreshold(metric.coordinatorReadLatency.getSnapshot(), sampleReadLatencyNanos);\n            additionalWriteLatencyNanos = metadata().params.additionalWritePolicy.calculateThreshold(metric.coordinatorWriteLatency.getSnapshot(), additionalWriteLatencyNanos);\n        }\n        catch (Throwable e)\n        {\n            logger.error(\"Exception caught while calculating speculative retry threshold for {}: {}\", metadata(), e);\n        }\n    }\n\n    public TableWriteHandler getWriteHandler()\n    {\n        return writeHandler;\n    }\n\n    public TableStreamManager getStreamManager()\n    {\n        return streamManager;\n    }\n\n    public TableRepairManager getRepairManager()\n    {\n        return repairManager;\n    }\n\n    public TableMetadata metadata()\n    {\n        return metadata.get();\n    }\n\n    public Directories getDirectories()\n    {\n        return directories;\n    }\n\n    public List<String> getDataPaths() throws IOException\n    {\n        List<String> dataPaths = new ArrayList<>();\n        for (File dataPath : directories.getCFDirectories())\n        {\n            dataPaths.add(dataPath.getCanonicalPath());\n        }\n\n        return dataPaths;\n    }\n\n    public SSTableMultiWriter createSSTableMultiWriter(Descriptor descriptor, long keyCount, long repairedAt, UUID pendingRepair, boolean isTransient, int sstableLevel, SerializationHeader header, LifecycleNewTracker lifecycleNewTracker)\n    {\n        MetadataCollector collector = new MetadataCollector(metadata().comparator).sstableLevel(sstableLevel);\n        return createSSTableMultiWriter(descriptor, keyCount, repairedAt, pendingRepair, isTransient, collector, header, lifecycleNewTracker);\n    }\n\n    public SSTableMultiWriter createSSTableMultiWriter(Descriptor descriptor, long keyCount, long repairedAt, UUID pendingRepair, boolean isTransient, MetadataCollector metadataCollector, SerializationHeader header, LifecycleNewTracker lifecycleNewTracker)\n    {\n        return getCompactionStrategyManager().createSSTableMultiWriter(descriptor, keyCount, repairedAt, pendingRepair, isTransient, metadataCollector, header, indexManager.listIndexes(), lifecycleNewTracker);\n    }\n\n    public boolean supportsEarlyOpen()\n    {\n        return compactionStrategyManager.supportsEarlyOpen();\n    }\n\n    /** call when dropping or renaming a CF. Performs mbean housekeeping and invalidates CFS to other operations */\n    public void invalidate()\n    {\n        invalidate(true);\n    }\n\n    public void invalidate(boolean expectMBean)\n    {\n        // disable and cancel in-progress compactions before invalidating\n        valid = false;\n\n        try\n        {\n            unregisterMBean();\n        }\n        catch (Exception e)\n        {\n            if (expectMBean)\n            {\n                JVMStabilityInspector.inspectThrowable(e);\n                // this shouldn't block anything.\n                logger.warn(\"Failed unregistering mbean: {}\", mbeanName, e);\n            }\n        }\n\n        compactionStrategyManager.shutdown();\n        SystemKeyspace.removeTruncationRecord(metadata.id);\n\n        data.dropSSTables();\n        LifecycleTransaction.waitForDeletions();\n        indexManager.dropAllIndexes();\n\n        invalidateCaches();\n    }\n\n    /**\n     * Removes every SSTable in the directory from the Tracker's view.\n     * @param directory the unreadable directory, possibly with SSTables in it, but not necessarily.\n     */\n    void maybeRemoveUnreadableSSTables(File directory)\n    {\n        data.removeUnreadableSSTables(directory);\n    }\n\n    void unregisterMBean() throws MalformedObjectNameException\n    {\n        ObjectName[] objectNames = {new ObjectName(mbeanName), new ObjectName(oldMBeanName)};\n        for (ObjectName objectName : objectNames)\n        {\n            if (MBeanWrapper.instance.isRegistered(objectName))\n                MBeanWrapper.instance.unregisterMBean(objectName);\n        }\n\n        // unregister metrics\n        metric.release();\n    }\n\n\n    public static ColumnFamilyStore createColumnFamilyStore(Keyspace keyspace, TableMetadataRef metadata, boolean loadSSTables)\n    {\n        return createColumnFamilyStore(keyspace, metadata.name, metadata, loadSSTables);\n    }\n\n    public static synchronized ColumnFamilyStore createColumnFamilyStore(Keyspace keyspace,\n                                                                         String columnFamily,\n                                                                         TableMetadataRef metadata,\n                                                                         boolean loadSSTables)\n    {\n        Directories directories = new Directories(metadata.get());\n        return createColumnFamilyStore(keyspace, columnFamily, metadata, directories, loadSSTables, true, false);\n    }\n\n    /** This is only directly used by offline tools */\n    public static synchronized ColumnFamilyStore createColumnFamilyStore(Keyspace keyspace,\n                                                                         String columnFamily,\n                                                                         TableMetadataRef metadata,\n                                                                         Directories directories,\n                                                                         boolean loadSSTables,\n                                                                         boolean registerBookkeeping,\n                                                                         boolean offline)\n    {\n        // get the max generation number, to prevent generation conflicts\n        Directories.SSTableLister lister = directories.sstableLister(Directories.OnTxnErr.IGNORE).includeBackups(true);\n        List<Integer> generations = new ArrayList<>();\n        for (Map.Entry<Descriptor, Set<Component>> entry : lister.list().entrySet())\n        {\n            Descriptor desc = entry.getKey();\n            generations.add(desc.generation);\n            if (!desc.isCompatible())\n                throw new RuntimeException(String.format(\"Incompatible SSTable found. Current version %s is unable to read file: %s. Please run upgradesstables.\",\n                                                         desc.getFormat().getLatestVersion(), desc));\n        }\n        Collections.sort(generations);\n        int value = (generations.size() > 0) ? (generations.get(generations.size() - 1)) : 0;\n\n        return new ColumnFamilyStore(keyspace, columnFamily, value, metadata, directories, loadSSTables, registerBookkeeping, offline);\n    }\n\n    /**\n     * Removes unnecessary files from the cf directory at startup: these include temp files, orphans, zero-length files\n     * and compacted sstables. Files that cannot be recognized will be ignored.\n     */\n    public static void  scrubDataDirectories(TableMetadata metadata) throws StartupException\n    {\n        Directories directories = new Directories(metadata);\n        Set<File> cleanedDirectories = new HashSet<>();\n\n        // clear ephemeral snapshots that were not properly cleared last session (CASSANDRA-7357)\n        clearEphemeralSnapshots(directories);\n\n        directories.removeTemporaryDirectories();\n\n        logger.trace(\"Removing temporary or obsoleted files from unfinished operations for table {}\", metadata.name);\n        if (!LifecycleTransaction.removeUnfinishedLeftovers(metadata))\n            throw new StartupException(StartupException.ERR_WRONG_DISK_STATE,\n                                       String.format(\"Cannot remove temporary or obsoleted files for %s due to a problem with transaction \" +\n                                                     \"log files. Please check records with problems in the log messages above and fix them. \" +\n                                                     \"Refer to the 3.0 upgrading instructions in NEWS.txt \" +\n                                                     \"for a description of transaction log files.\", metadata.toString()));\n\n        logger.trace(\"Further extra check for orphan sstable files for {}\", metadata.name);\n        for (Map.Entry<Descriptor,Set<Component>> sstableFiles : directories.sstableLister(Directories.OnTxnErr.IGNORE).list().entrySet())\n        {\n            Descriptor desc = sstableFiles.getKey();\n            File directory = desc.directory;\n            Set<Component> components = sstableFiles.getValue();\n\n            if (!cleanedDirectories.contains(directory))\n            {\n                cleanedDirectories.add(directory);\n                for (File tmpFile : desc.getTemporaryFiles())\n                {\n                    logger.info(\"Removing unfinished temporary file {}\", tmpFile);\n                    tmpFile.delete();\n                }\n            }\n\n            File dataFile = new File(desc.filenameFor(Component.DATA));\n            if (components.contains(Component.DATA) && dataFile.length() > 0)\n                // everything appears to be in order... moving on.\n                continue;\n\n            // missing the DATA file! all components are orphaned\n            logger.warn(\"Removing orphans for {}: {}\", desc, components);\n            for (Component component : components)\n            {\n                File file = new File(desc.filenameFor(component));\n                if (file.exists())\n                    FileUtils.deleteWithConfirm(desc.filenameFor(component));\n            }\n        }\n\n        // cleanup incomplete saved caches\n        Pattern tmpCacheFilePattern = Pattern.compile(metadata.keyspace + '-' + metadata.name + \"-(Key|Row)Cache.*\\\\.tmp$\");\n        File dir = new File(DatabaseDescriptor.getSavedCachesLocation());\n\n        if (dir.exists())\n        {\n            assert dir.isDirectory();\n            for (File file : Objects.requireNonNull(dir.listFiles()))\n                if (tmpCacheFilePattern.matcher(file.getName()).matches())\n                    if (!file.delete())\n                        logger.warn(\"could not delete {}\", file.getAbsolutePath());\n        }\n\n        // also clean out any index leftovers.\n        for (IndexMetadata index : metadata.indexes)\n            if (!index.isCustom())\n            {\n                TableMetadata indexMetadata = CassandraIndex.indexCfsMetadata(metadata, index);\n                scrubDataDirectories(indexMetadata);\n            }\n    }\n\n    /**\n     * See #{@code StorageService.importNewSSTables} for more info\n     *\n     * @param ksName The keyspace name\n     * @param cfName The columnFamily name\n     */\n    public static void loadNewSSTables(String ksName, String cfName)\n    {\n        /* ks/cf existence checks will be done by open and getCFS methods for us */\n        Keyspace keyspace = Keyspace.open(ksName);\n        keyspace.getColumnFamilyStore(cfName).loadNewSSTables();\n    }\n\n    @Deprecated\n    public void loadNewSSTables()\n    {\n\n        SSTableImporter.Options options = SSTableImporter.Options.options().resetLevel(true).build();\n        sstableImporter.importNewSSTables(options);\n    }\n\n    /**\n     * #{@inheritDoc}\n     */\n    public synchronized List<String> importNewSSTables(Set<String> srcPaths, boolean resetLevel, boolean clearRepaired, boolean verifySSTables, boolean verifyTokens, boolean invalidateCaches, boolean extendedVerify, boolean copyData)\n    {\n        SSTableImporter.Options options = SSTableImporter.Options.options(srcPaths)\n                                                                 .resetLevel(resetLevel)\n                                                                 .clearRepaired(clearRepaired)\n                                                                 .verifySSTables(verifySSTables)\n                                                                 .verifyTokens(verifyTokens)\n                                                                 .invalidateCaches(invalidateCaches)\n                                                                 .extendedVerify(extendedVerify)\n                                                                 .copyData(copyData).build();\n\n        return sstableImporter.importNewSSTables(options);\n    }\n\n    public List<String> importNewSSTables(Set<String> srcPaths, boolean resetLevel, boolean clearRepaired, boolean verifySSTables, boolean verifyTokens, boolean invalidateCaches, boolean extendedVerify)\n    {\n        return importNewSSTables(srcPaths, resetLevel, clearRepaired, verifySSTables, verifyTokens, invalidateCaches, extendedVerify, false);\n    }\n\n    Descriptor getUniqueDescriptorFor(Descriptor descriptor, File targetDirectory)\n    {\n        Descriptor newDescriptor;\n        do\n        {\n            newDescriptor = new Descriptor(descriptor.version,\n                                           targetDirectory,\n                                           descriptor.ksname,\n                                           descriptor.cfname,\n                                           // Increment the generation until we find a filename that doesn't exist. This is needed because the new\n                                           // SSTables that are being loaded might already use these generation numbers.\n                                           fileIndexGenerator.incrementAndGet(),\n                                           descriptor.formatType);\n        }\n        while (new File(newDescriptor.filenameFor(Component.DATA)).exists());\n        return newDescriptor;\n    }\n\n    public void rebuildSecondaryIndex(String idxName)\n    {\n        rebuildSecondaryIndex(keyspace.getName(), metadata.name, idxName);\n    }\n\n    public static void rebuildSecondaryIndex(String ksName, String cfName, String... idxNames)\n    {\n        ColumnFamilyStore cfs = Keyspace.open(ksName).getColumnFamilyStore(cfName);\n\n        logger.info(\"User Requested secondary index re-build for {}/{} indexes: {}\", ksName, cfName, Joiner.on(',').join(idxNames));\n        cfs.indexManager.rebuildIndexesBlocking(Sets.newHashSet(Arrays.asList(idxNames)));\n    }\n\n    public AbstractCompactionStrategy createCompactionStrategyInstance(CompactionParams compactionParams)\n    {\n        try\n        {\n            Constructor<? extends AbstractCompactionStrategy> constructor =\n                compactionParams.klass().getConstructor(ColumnFamilyStore.class, Map.class);\n            return constructor.newInstance(this, compactionParams.options());\n        }\n        catch (NoSuchMethodException | IllegalAccessException | InvocationTargetException | InstantiationException e)\n        {\n            throw new RuntimeException(e);\n        }\n    }\n\n    @Deprecated\n    public String getColumnFamilyName()\n    {\n        return getTableName();\n    }\n\n    public String getTableName()\n    {\n        return name;\n    }\n\n    public Descriptor newSSTableDescriptor(File directory)\n    {\n        return newSSTableDescriptor(directory, SSTableFormat.Type.current().info.getLatestVersion(), SSTableFormat.Type.current());\n    }\n\n    public Descriptor newSSTableDescriptor(File directory, SSTableFormat.Type format)\n    {\n        return newSSTableDescriptor(directory, format.info.getLatestVersion(), format);\n    }\n\n    public Descriptor newSSTableDescriptor(File directory, Version version, SSTableFormat.Type format)\n    {\n        return new Descriptor(version,\n                              directory,\n                              keyspace.getName(),\n                              name,\n                              fileIndexGenerator.incrementAndGet(),\n                              format);\n    }\n\n    /**\n     * Switches the memtable iff the live memtable is the one provided\n     *\n     * @param memtable\n     */\n    public ListenableFuture<CommitLogPosition> switchMemtableIfCurrent(Memtable memtable)\n    {\n        synchronized (data)\n        {\n            if (data.getView().getCurrentMemtable() == memtable)\n                return switchMemtable();\n        }\n        logger.debug(\"Memtable is no longer current, returning future that completes when current flushing operation completes\");\n        return waitForFlushes();\n    }\n\n    /*\n     * switchMemtable puts Memtable.getSortedContents on the writer executor.  When the write is complete,\n     * we turn the writer into an SSTableReader and add it to ssTables where it is available for reads.\n     * This method does not block except for synchronizing on Tracker, but the Future it returns will\n     * not complete until the Memtable (and all prior Memtables) have been successfully flushed, and the CL\n     * marked clean up to the position owned by the Memtable.\n     */\n    public ListenableFuture<CommitLogPosition> switchMemtable()\n    {\n        synchronized (data)\n        {\n            logFlush();\n            Flush flush = new Flush(false);\n            flushExecutor.execute(flush);\n            postFlushExecutor.execute(flush.postFlushTask);\n            return flush.postFlushTask;\n        }\n    }\n\n    // print out size of all memtables we're enqueuing\n    private void logFlush()\n    {\n        // reclaiming includes that which we are GC-ing;\n        float onHeapRatio = 0, offHeapRatio = 0;\n        long onHeapTotal = 0, offHeapTotal = 0;\n        Memtable memtable = getTracker().getView().getCurrentMemtable();\n        onHeapRatio +=  memtable.getAllocator().onHeap().ownershipRatio();\n        offHeapRatio += memtable.getAllocator().offHeap().ownershipRatio();\n        onHeapTotal += memtable.getAllocator().onHeap().owns();\n        offHeapTotal += memtable.getAllocator().offHeap().owns();\n\n        for (ColumnFamilyStore indexCfs : indexManager.getAllIndexColumnFamilyStores())\n        {\n            MemtableAllocator allocator = indexCfs.getTracker().getView().getCurrentMemtable().getAllocator();\n            onHeapRatio += allocator.onHeap().ownershipRatio();\n            offHeapRatio += allocator.offHeap().ownershipRatio();\n            onHeapTotal += allocator.onHeap().owns();\n            offHeapTotal += allocator.offHeap().owns();\n        }\n\n        logger.info(\"Enqueuing flush of {}: {}\",\n                     name,\n                     String.format(\"%s (%.0f%%) on-heap, %s (%.0f%%) off-heap\",\n                                   FBUtilities.prettyPrintMemory(onHeapTotal),\n                                   onHeapRatio * 100,\n                                   FBUtilities.prettyPrintMemory(offHeapTotal),\n                                   offHeapRatio * 100));\n    }\n\n\n    /**\n     * Flush if there is unflushed data in the memtables\n     *\n     * @return a Future yielding the commit log position that can be guaranteed to have been successfully written\n     *         to sstables for this table once the future completes\n     */\n    public ListenableFuture<CommitLogPosition> forceFlush()\n    {\n        synchronized (data)\n        {\n            Memtable current = data.getView().getCurrentMemtable();\n            for (ColumnFamilyStore cfs : concatWithIndexes())\n                if (!cfs.data.getView().getCurrentMemtable().isClean())\n                    return switchMemtableIfCurrent(current);\n            return waitForFlushes();\n        }\n    }\n\n    /**\n     * Flush if there is unflushed data that was written to the CommitLog before @param flushIfDirtyBefore\n     * (inclusive).\n     *\n     * @return a Future yielding the commit log position that can be guaranteed to have been successfully written\n     *         to sstables for this table once the future completes\n     */\n    public ListenableFuture<?> forceFlush(CommitLogPosition flushIfDirtyBefore)\n    {\n        // we don't loop through the remaining memtables since here we only care about commit log dirtiness\n        // and this does not vary between a table and its table-backed indexes\n        Memtable current = data.getView().getCurrentMemtable();\n        if (current.mayContainDataBefore(flushIfDirtyBefore))\n            return switchMemtableIfCurrent(current);\n        return waitForFlushes();\n    }\n\n    /**\n     * @return a Future yielding the commit log position that can be guaranteed to have been successfully written\n     *         to sstables for this table once the future completes\n     */\n    private ListenableFuture<CommitLogPosition> waitForFlushes()\n    {\n        // we grab the current memtable; once any preceding memtables have flushed, we know its\n        // commitLogLowerBound has been set (as this it is set with the upper bound of the preceding memtable)\n        final Memtable current = data.getView().getCurrentMemtable();\n        ListenableFutureTask<CommitLogPosition> task = ListenableFutureTask.create(() -> {\n            logger.debug(\"forceFlush requested but everything is clean in {}\", name);\n            return current.getCommitLogLowerBound();\n        });\n        postFlushExecutor.execute(task);\n        return task;\n    }\n\n    public CommitLogPosition forceBlockingFlush()\n    {\n        return FBUtilities.waitOnFuture(forceFlush());\n    }\n\n    /**\n     * Both synchronises custom secondary indexes and provides ordering guarantees for futures on switchMemtable/flush\n     * etc, which expect to be able to wait until the flush (and all prior flushes) requested have completed.\n     */\n    private final class PostFlush implements Callable<CommitLogPosition>\n    {\n        final CountDownLatch latch = new CountDownLatch(1);\n        final List<Memtable> memtables;\n        volatile Throwable flushFailure = null;\n\n        private PostFlush(List<Memtable> memtables)\n        {\n            this.memtables = memtables;\n        }\n\n        public CommitLogPosition call()\n        {\n            try\n            {\n                // we wait on the latch for the commitLogUpperBound to be set, and so that waiters\n                // on this task can rely on all prior flushes being complete\n                latch.await();\n            }\n            catch (InterruptedException e)\n            {\n                throw new IllegalStateException();\n            }\n\n            CommitLogPosition commitLogUpperBound = CommitLogPosition.NONE;\n            // If a flush errored out but the error was ignored, make sure we don't discard the commit log.\n            if (flushFailure == null && !memtables.isEmpty())\n            {\n                Memtable memtable = memtables.get(0);\n                commitLogUpperBound = memtable.getCommitLogUpperBound();\n                CommitLog.instance.discardCompletedSegments(metadata.id, memtable.getCommitLogLowerBound(), commitLogUpperBound);\n            }\n\n            metric.pendingFlushes.dec();\n\n            if (flushFailure != null)\n                throw Throwables.propagate(flushFailure);\n\n            return commitLogUpperBound;\n        }\n    }\n\n    /**\n     * Should only be constructed/used from switchMemtable() or truncate(), with ownership of the Tracker monitor.\n     * In the constructor the current memtable(s) are swapped, and a barrier on outstanding writes is issued;\n     * when run by the flushWriter the barrier is waited on to ensure all outstanding writes have completed\n     * before all memtables are immediately written, and the CL is either immediately marked clean or, if\n     * there are custom secondary indexes, the post flush clean up is left to update those indexes and mark\n     * the CL clean\n     */\n    private final class Flush implements Runnable\n    {\n        final OpOrder.Barrier writeBarrier;\n        final List<Memtable> memtables = new ArrayList<>();\n        final ListenableFutureTask<CommitLogPosition> postFlushTask;\n        final PostFlush postFlush;\n        final boolean truncate;\n\n        private Flush(boolean truncate)\n        {\n            if (logger.isTraceEnabled())\n                logger.trace(\"Creating flush task {}@{}\", hashCode(), name);\n            // if true, we won't flush, we'll just wait for any outstanding writes, switch the memtable, and discard\n            this.truncate = truncate;\n\n            metric.pendingFlushes.inc();\n            /*\n             * To ensure correctness of switch without blocking writes, run() needs to wait for all write operations\n             * started prior to the switch to complete. We do this by creating a Barrier on the writeOrdering\n             * that all write operations register themselves with, and assigning this barrier to the memtables,\n             * after which we *.issue()* the barrier. This barrier is used to direct write operations started prior\n             * to the barrier.issue() into the memtable we have switched out, and any started after to its replacement.\n             * In doing so it also tells the write operations to update the commitLogUpperBound of the memtable, so\n             * that we know the CL position we are dirty to, which can be marked clean when we complete.\n             */\n            writeBarrier = Keyspace.writeOrder.newBarrier();\n\n            // submit flushes for the memtable for any indexed sub-cfses, and our own\n            AtomicReference<CommitLogPosition> commitLogUpperBound = new AtomicReference<>();\n            for (ColumnFamilyStore cfs : concatWithIndexes())\n            {\n                // switch all memtables, regardless of their dirty status, setting the barrier\n                // so that we can reach a coordinated decision about cleanliness once they\n                // are no longer possible to be modified\n                Memtable newMemtable = new Memtable(commitLogUpperBound, cfs);\n                Memtable oldMemtable = cfs.data.switchMemtable(truncate, newMemtable);\n                oldMemtable.setDiscarding(writeBarrier, commitLogUpperBound);\n                memtables.add(oldMemtable);\n            }\n\n            // we then ensure an atomic decision is made about the upper bound of the continuous range of commit log\n            // records owned by this memtable\n            setCommitLogUpperBound(commitLogUpperBound);\n\n            // we then issue the barrier; this lets us wait for all operations started prior to the barrier to complete;\n            // since this happens after wiring up the commitLogUpperBound, we also know all operations with earlier\n            // commit log segment position have also completed, i.e. the memtables are done and ready to flush\n            writeBarrier.issue();\n            postFlush = new PostFlush(memtables);\n            postFlushTask = ListenableFutureTask.create(postFlush);\n        }\n\n        public void run()\n        {\n            if (logger.isTraceEnabled())\n                logger.trace(\"Flush task {}@{} starts executing, waiting on barrier\", hashCode(), name);\n\n            long start = System.nanoTime();\n\n            // mark writes older than the barrier as blocking progress, permitting them to exceed our memory limit\n            // if they are stuck waiting on it, then wait for them all to complete\n            writeBarrier.markBlocking();\n            writeBarrier.await();\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"Flush task for task {}@{} waited {} ms at the barrier\", hashCode(), name, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));\n\n            // mark all memtables as flushing, removing them from the live memtable list\n            for (Memtable memtable : memtables)\n                memtable.cfs.data.markFlushing(memtable);\n\n            metric.memtableSwitchCount.inc();\n\n            try\n            {\n                // Flush \"data\" memtable with non-cf 2i first;\n                flushMemtable(memtables.get(0), true);\n                for (int i = 1; i < memtables.size(); i++)\n                    flushMemtable(memtables.get(i), false);\n            }\n            catch (Throwable t)\n            {\n                JVMStabilityInspector.inspectThrowable(t);\n                postFlush.flushFailure = t;\n            }\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"Flush task {}@{} signaling post flush task\", hashCode(), name);\n\n            // signal the post-flush we've done our work\n            postFlush.latch.countDown();\n\n            if (logger.isTraceEnabled())\n                logger.trace(\"Flush task task {}@{} finished\", hashCode(), name);\n        }\n\n        public Collection<SSTableReader> flushMemtable(Memtable memtable, boolean flushNonCf2i)\n        {\n            if (logger.isTraceEnabled())\n                logger.trace(\"Flush task task {}@{} flushing memtable {}\", hashCode(), name, memtable);\n\n            if (memtable.isClean() || truncate)\n            {\n                memtable.cfs.replaceFlushed(memtable, Collections.emptyList());\n                reclaim(memtable);\n                return Collections.emptyList();\n            }\n\n            List<Future<SSTableMultiWriter>> futures = new ArrayList<>();\n            long totalBytesOnDisk = 0;\n            long maxBytesOnDisk = 0;\n            long minBytesOnDisk = Long.MAX_VALUE;\n            List<SSTableReader> sstables = new ArrayList<>();\n            try (LifecycleTransaction txn = LifecycleTransaction.offline(OperationType.FLUSH))\n            {\n                List<Memtable.FlushRunnable> flushRunnables = null;\n                List<SSTableMultiWriter> flushResults = null;\n\n                try\n                {\n                    // flush the memtable\n                    flushRunnables = memtable.flushRunnables(txn);\n                    ExecutorService[] executors = perDiskflushExecutors.getExecutorsFor(keyspace.getName(), name);\n\n                    for (int i = 0; i < flushRunnables.size(); i++)\n                        futures.add(executors[i].submit(flushRunnables.get(i)));\n\n                    /**\n                     * we can flush 2is as soon as the barrier completes, as they will be consistent with (or ahead of) the\n                     * flushed memtables and CL position, which is as good as we can guarantee.\n                     * TODO: SecondaryIndex should support setBarrier(), so custom implementations can co-ordinate exactly\n                     * with CL as we do with memtables/CFS-backed SecondaryIndexes.\n                     */\n                    if (flushNonCf2i)\n                        indexManager.flushAllNonCFSBackedIndexesBlocking();\n\n                    flushResults = Lists.newArrayList(FBUtilities.waitOnFutures(futures));\n                }\n                catch (Throwable t)\n                {\n                    t = memtable.abortRunnables(flushRunnables, t);\n                    t = txn.abort(t);\n                    throw Throwables.propagate(t);\n                }\n\n                try\n                {\n                    Iterator<SSTableMultiWriter> writerIterator = flushResults.iterator();\n                    while (writerIterator.hasNext())\n                    {\n                        @SuppressWarnings(\"resource\")\n                        SSTableMultiWriter writer = writerIterator.next();\n                        if (writer.getFilePointer() > 0)\n                        {\n                            writer.setOpenResult(true).prepareToCommit();\n                        }\n                        else\n                        {\n                            maybeFail(writer.abort(null));\n                            writerIterator.remove();\n                        }\n                    }\n                }\n                catch (Throwable t)\n                {\n                    for (SSTableMultiWriter writer : flushResults)\n                        t = writer.abort(t);\n                    t = txn.abort(t);\n                    Throwables.propagate(t);\n                }\n\n                txn.prepareToCommit();\n\n                Throwable accumulate = null;\n                for (SSTableMultiWriter writer : flushResults)\n                    accumulate = writer.commit(accumulate);\n\n                maybeFail(txn.commit(accumulate));\n\n                for (SSTableMultiWriter writer : flushResults)\n                {\n                    Collection<SSTableReader> flushedSSTables = writer.finished();\n                    for (SSTableReader sstable : flushedSSTables)\n                    {\n                        if (sstable != null)\n                        {\n                            sstables.add(sstable);\n                            long size = sstable.bytesOnDisk();\n                            totalBytesOnDisk += size;\n                            maxBytesOnDisk = Math.max(maxBytesOnDisk, size);\n                            minBytesOnDisk = Math.min(minBytesOnDisk, size);\n                        }\n                    }\n                }\n            }\n            memtable.cfs.replaceFlushed(memtable, sstables);\n            reclaim(memtable);\n            memtable.cfs.compactionStrategyManager.compactionLogger.flush(sstables);\n            logger.debug(\"Flushed to {} ({} sstables, {}), biggest {}, smallest {}\",\n                         sstables,\n                         sstables.size(),\n                         FBUtilities.prettyPrintMemory(totalBytesOnDisk),\n                         FBUtilities.prettyPrintMemory(maxBytesOnDisk),\n                         FBUtilities.prettyPrintMemory(minBytesOnDisk));\n            return sstables;\n        }\n\n        private void reclaim(final Memtable memtable)\n        {\n            // issue a read barrier for reclaiming the memory, and offload the wait to another thread\n            final OpOrder.Barrier readBarrier = readOrdering.newBarrier();\n            readBarrier.issue();\n            postFlushTask.addListener(new WrappedRunnable()\n            {\n                public void runMayThrow()\n                {\n                    readBarrier.await();\n                    memtable.setDiscarded();\n                }\n            }, reclaimExecutor);\n        }\n    }\n\n    // atomically set the upper bound for the commit log\n    private static void setCommitLogUpperBound(AtomicReference<CommitLogPosition> commitLogUpperBound)\n    {\n        // we attempt to set the holder to the current commit log context. at the same time all writes to the memtables are\n        // also maintaining this value, so if somebody sneaks ahead of us somehow (should be rare) we simply retry,\n        // so that we know all operations prior to the position have not reached it yet\n        CommitLogPosition lastReplayPosition;\n        while (true)\n        {\n            lastReplayPosition = new Memtable.LastCommitLogPosition((CommitLog.instance.getCurrentPosition()));\n            CommitLogPosition currentLast = commitLogUpperBound.get();\n            if ((currentLast == null || currentLast.compareTo(lastReplayPosition) <= 0)\n                && commitLogUpperBound.compareAndSet(currentLast, lastReplayPosition))\n                break;\n        }\n    }\n\n    /**\n     * Finds the largest memtable, as a percentage of *either* on- or off-heap memory limits, and immediately\n     * queues it for flushing. If the memtable selected is flushed before this completes, no work is done.\n     */\n    public static CompletableFuture<Boolean> flushLargestMemtable()\n    {\n        float largestRatio = 0f;\n        Memtable largest = null;\n        float liveOnHeap = 0, liveOffHeap = 0;\n        for (ColumnFamilyStore cfs : ColumnFamilyStore.all())\n        {\n            // we take a reference to the current main memtable for the CF prior to snapping its ownership ratios\n            // to ensure we have some ordering guarantee for performing the switchMemtableIf(), i.e. we will only\n            // swap if the memtables we are measuring here haven't already been swapped by the time we try to swap them\n            Memtable current = cfs.getTracker().getView().getCurrentMemtable();\n\n            // find the total ownership ratio for the memtable and all SecondaryIndexes owned by this CF,\n            // both on- and off-heap, and select the largest of the two ratios to weight this CF\n            float onHeap = 0f, offHeap = 0f;\n            onHeap += current.getAllocator().onHeap().ownershipRatio();\n            offHeap += current.getAllocator().offHeap().ownershipRatio();\n\n            for (ColumnFamilyStore indexCfs : cfs.indexManager.getAllIndexColumnFamilyStores())\n            {\n                MemtableAllocator allocator = indexCfs.getTracker().getView().getCurrentMemtable().getAllocator();\n                onHeap += allocator.onHeap().ownershipRatio();\n                offHeap += allocator.offHeap().ownershipRatio();\n            }\n\n            float ratio = Math.max(onHeap, offHeap);\n            if (ratio > largestRatio)\n            {\n                largest = current;\n                largestRatio = ratio;\n            }\n\n            liveOnHeap += onHeap;\n            liveOffHeap += offHeap;\n        }\n\n        CompletableFuture<Boolean> returnFuture = new CompletableFuture<>();\n\n        if (largest != null)\n        {\n            float usedOnHeap = Memtable.MEMORY_POOL.onHeap.usedRatio();\n            float usedOffHeap = Memtable.MEMORY_POOL.offHeap.usedRatio();\n            float flushingOnHeap = Memtable.MEMORY_POOL.onHeap.reclaimingRatio();\n            float flushingOffHeap = Memtable.MEMORY_POOL.offHeap.reclaimingRatio();\n            float thisOnHeap = largest.getAllocator().onHeap().ownershipRatio();\n            float thisOffHeap = largest.getAllocator().offHeap().ownershipRatio();\n            logger.debug(\"Flushing largest {} to free up room. Used total: {}, live: {}, flushing: {}, this: {}\",\n                         largest.cfs, ratio(usedOnHeap, usedOffHeap), ratio(liveOnHeap, liveOffHeap),\n                         ratio(flushingOnHeap, flushingOffHeap), ratio(thisOnHeap, thisOffHeap));\n\n            ListenableFuture<CommitLogPosition> flushFuture = largest.cfs.switchMemtableIfCurrent(largest);\n            flushFuture.addListener(() -> {\n                try\n                {\n                    flushFuture.get();\n                    returnFuture.complete(true);\n                }\n                catch (Throwable t)\n                {\n                    returnFuture.completeExceptionally(t);\n                }\n            }, MoreExecutors.directExecutor());\n        }\n        else\n        {\n            logger.debug(\"Flushing of largest memtable, not done, no memtable found\");\n\n            returnFuture.complete(false);\n        }\n\n        return returnFuture;\n    }\n\n    private static String ratio(float onHeap, float offHeap)\n    {\n        return String.format(\"%.2f/%.2f\", onHeap, offHeap);\n    }\n\n    /**\n     * Insert/Update the column family for this key.\n     * Caller is responsible for acquiring Keyspace.switchLock\n     * param @ lock - lock that needs to be used.\n     * param @ key - key for update/insert\n     * param @ columnFamily - columnFamily changes\n     */\n    public void apply(PartitionUpdate update, UpdateTransaction indexer, OpOrder.Group opGroup, CommitLogPosition commitLogPosition)\n\n    {\n        long start = System.nanoTime();\n        try\n        {\n            Memtable mt = data.getMemtableFor(opGroup, commitLogPosition);\n            long timeDelta = mt.put(update, indexer, opGroup);\n            DecoratedKey key = update.partitionKey();\n            invalidateCachedPartition(key);\n            metric.topWritePartitionFrequency.addSample(key.getKey(), 1);\n            if (metric.topWritePartitionSize.isEnabled()) // dont compute datasize if not needed\n                metric.topWritePartitionSize.addSample(key.getKey(), update.dataSize());\n            StorageHook.instance.reportWrite(metadata.id, update);\n            metric.writeLatency.addNano(System.nanoTime() - start);\n            // CASSANDRA-11117 - certain resolution paths on memtable put can result in very\n            // large time deltas, either through a variety of sentinel timestamps (used for empty values, ensuring\n            // a minimal write, etc). This limits the time delta to the max value the histogram\n            // can bucket correctly. This also filters the Long.MAX_VALUE case where there was no previous value\n            // to update.\n            if(timeDelta < Long.MAX_VALUE)\n                metric.colUpdateTimeDeltaHistogram.update(Math.min(18165375903306L, timeDelta));\n        }\n        catch (RuntimeException e)\n        {\n            throw new RuntimeException(e.getMessage()\n                                       + \" for ks: \"\n                                       + keyspace.getName() + \", table: \" + name, e);\n        }\n    }\n\n    /**\n     * @param sstables\n     * @return sstables whose key range overlaps with that of the given sstables, not including itself.\n     * (The given sstables may or may not overlap with each other.)\n     */\n    public Collection<SSTableReader> getOverlappingLiveSSTables(Iterable<SSTableReader> sstables)\n    {\n        logger.trace(\"Checking for sstables overlapping {}\", sstables);\n\n        // a normal compaction won't ever have an empty sstables list, but we create a skeleton\n        // compaction controller for streaming, and that passes an empty list.\n        if (!sstables.iterator().hasNext())\n            return ImmutableSet.of();\n\n        View view = data.getView();\n\n        List<SSTableReader> sortedByFirst = Lists.newArrayList(sstables);\n        Collections.sort(sortedByFirst, (o1, o2) -> o1.first.compareTo(o2.first));\n\n        List<AbstractBounds<PartitionPosition>> bounds = new ArrayList<>();\n        DecoratedKey first = null, last = null;\n        /*\n        normalize the intervals covered by the sstables\n        assume we have sstables like this (brackets representing first/last key in the sstable);\n        [   ] [   ]    [   ]   [  ]\n           [   ]         [       ]\n        then we can, instead of searching the interval tree 6 times, normalize the intervals and\n        only query the tree 2 times, for these intervals;\n        [         ]    [          ]\n         */\n        for (SSTableReader sstable : sortedByFirst)\n        {\n            if (first == null)\n            {\n                first = sstable.first;\n                last = sstable.last;\n            }\n            else\n            {\n                if (sstable.first.compareTo(last) <= 0) // we do overlap\n                {\n                    if (sstable.last.compareTo(last) > 0)\n                        last = sstable.last;\n                }\n                else\n                {\n                    bounds.add(AbstractBounds.bounds(first, true, last, true));\n                    first = sstable.first;\n                    last = sstable.last;\n                }\n            }\n        }\n        bounds.add(AbstractBounds.bounds(first, true, last, true));\n        Set<SSTableReader> results = new HashSet<>();\n\n        for (AbstractBounds<PartitionPosition> bound : bounds)\n            Iterables.addAll(results, view.liveSSTablesInBounds(bound.left, bound.right));\n\n        return Sets.difference(results, ImmutableSet.copyOf(sstables));\n    }\n\n    /**\n     * like getOverlappingSSTables, but acquires references before returning\n     */\n    public Refs<SSTableReader> getAndReferenceOverlappingLiveSSTables(Iterable<SSTableReader> sstables)\n    {\n        while (true)\n        {\n            Iterable<SSTableReader> overlapped = getOverlappingLiveSSTables(sstables);\n            Refs<SSTableReader> refs = Refs.tryRef(overlapped);\n            if (refs != null)\n                return refs;\n        }\n    }\n\n    /*\n     * Called after a BinaryMemtable flushes its in-memory data, or we add a file\n     * via bootstrap. This information is cached in the ColumnFamilyStore.\n     * This is useful for reads because the ColumnFamilyStore first looks in\n     * the in-memory store and the into the disk to find the key. If invoked\n     * during recoveryMode the onMemtableFlush() need not be invoked.\n     *\n     * param @ filename - filename just flushed to disk\n     */\n    public void addSSTable(SSTableReader sstable)\n    {\n        assert sstable.getColumnFamilyName().equals(name);\n        addSSTables(Collections.singletonList(sstable));\n    }\n\n    public void addSSTables(Collection<SSTableReader> sstables)\n    {\n        data.addSSTables(sstables);\n        CompactionManager.instance.submitBackground(this);\n    }\n\n    /**\n     * Calculate expected file size of SSTable after compaction.\n     *\n     * If operation type is {@code CLEANUP} and we're not dealing with an index sstable,\n     * then we calculate expected file size with checking token range to be eliminated.\n     *\n     * Otherwise, we just add up all the files' size, which is the worst case file\n     * size for compaction of all the list of files given.\n     *\n     * @param sstables SSTables to calculate expected compacted file size\n     * @param operation Operation type\n     * @return Expected file size of SSTable after compaction\n     */\n    public long getExpectedCompactedFileSize(Iterable<SSTableReader> sstables, OperationType operation)\n    {\n        if (operation != OperationType.CLEANUP || isIndex())\n        {\n            return SSTableReader.getTotalBytes(sstables);\n        }\n\n        // cleanup size estimation only counts bytes for keys local to this node\n        long expectedFileSize = 0;\n        Collection<Range<Token>> ranges = StorageService.instance.getLocalReplicas(keyspace.getName()).ranges();\n        for (SSTableReader sstable : sstables)\n        {\n            List<SSTableReader.PartitionPositionBounds> positions = sstable.getPositionsForRanges(ranges);\n            for (SSTableReader.PartitionPositionBounds position : positions)\n                expectedFileSize += position.upperPosition - position.lowerPosition;\n        }\n\n        double compressionRatio = metric.compressionRatio.getValue();\n        if (compressionRatio > 0d)\n            expectedFileSize *= compressionRatio;\n\n        return expectedFileSize;\n    }\n\n    /*\n     *  Find the maximum size file in the list .\n     */\n    public SSTableReader getMaxSizeFile(Iterable<SSTableReader> sstables)\n    {\n        long maxSize = 0L;\n        SSTableReader maxFile = null;\n        for (SSTableReader sstable : sstables)\n        {\n            if (sstable.onDiskLength() > maxSize)\n            {\n                maxSize = sstable.onDiskLength();\n                maxFile = sstable;\n            }\n        }\n        return maxFile;\n    }\n\n    public CompactionManager.AllSSTableOpStatus forceCleanup(int jobs) throws ExecutionException, InterruptedException\n    {\n        return CompactionManager.instance.performCleanup(ColumnFamilyStore.this, jobs);\n    }\n\n    public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTL, int jobs) throws ExecutionException, InterruptedException\n    {\n        return scrub(disableSnapshot, skipCorrupted, reinsertOverflowedTTL, false, checkData, jobs);\n    }\n\n    @VisibleForTesting\n    public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean reinsertOverflowedTTL, boolean alwaysFail, boolean checkData, int jobs) throws ExecutionException, InterruptedException\n    {\n        // skip snapshot creation during scrub, SEE JIRA 5891\n        if(!disableSnapshot)\n            snapshotWithoutFlush(\"pre-scrub-\" + System.currentTimeMillis());\n\n        try\n        {\n            return CompactionManager.instance.performScrub(ColumnFamilyStore.this, skipCorrupted, checkData, reinsertOverflowedTTL, jobs);\n        }\n        catch(Throwable t)\n        {\n            if (!rebuildOnFailedScrub(t))\n                throw t;\n\n            return alwaysFail ? CompactionManager.AllSSTableOpStatus.ABORTED : CompactionManager.AllSSTableOpStatus.SUCCESSFUL;\n        }\n    }\n\n    /**\n     * CASSANDRA-5174 : For an index cfs we may be able to discard everything and just rebuild\n     * the index when a scrub fails.\n     *\n     * @return true if we are an index cfs and we successfully rebuilt the index\n     */\n    public boolean rebuildOnFailedScrub(Throwable failure)\n    {\n        if (!isIndex() || !SecondaryIndexManager.isIndexColumnFamilyStore(this))\n            return false;\n\n        truncateBlocking();\n\n        logger.warn(\"Rebuilding index for {} because of <{}>\", name, failure.getMessage());\n\n        ColumnFamilyStore parentCfs = SecondaryIndexManager.getParentCfs(this);\n        assert parentCfs.indexManager.getAllIndexColumnFamilyStores().contains(this);\n\n        String indexName = SecondaryIndexManager.getIndexName(this);\n\n        parentCfs.rebuildSecondaryIndex(indexName);\n        return true;\n    }\n\n    public CompactionManager.AllSSTableOpStatus verify(Verifier.Options options) throws ExecutionException, InterruptedException\n    {\n        return CompactionManager.instance.performVerify(ColumnFamilyStore.this, options);\n    }\n\n    public CompactionManager.AllSSTableOpStatus sstablesRewrite(boolean excludeCurrentVersion, int jobs) throws ExecutionException, InterruptedException\n    {\n        return CompactionManager.instance.performSSTableRewrite(ColumnFamilyStore.this, excludeCurrentVersion, jobs);\n    }\n\n    public CompactionManager.AllSSTableOpStatus relocateSSTables(int jobs) throws ExecutionException, InterruptedException\n    {\n        return CompactionManager.instance.relocateSSTables(this, jobs);\n    }\n\n    public CompactionManager.AllSSTableOpStatus garbageCollect(TombstoneOption tombstoneOption, int jobs) throws ExecutionException, InterruptedException\n    {\n        return CompactionManager.instance.performGarbageCollection(this, tombstoneOption, jobs);\n    }\n\n    public void markObsolete(Collection<SSTableReader> sstables, OperationType compactionType)\n    {\n        assert !sstables.isEmpty();\n        maybeFail(data.dropSSTables(Predicates.in(sstables), compactionType, null));\n    }\n\n    void replaceFlushed(Memtable memtable, Collection<SSTableReader> sstables)\n    {\n        data.replaceFlushed(memtable, sstables);\n        if (sstables != null && !sstables.isEmpty())\n            CompactionManager.instance.submitBackground(this);\n    }\n\n    public boolean isValid()\n    {\n        return valid;\n    }\n\n    /**\n     * Package protected for access from the CompactionManager.\n     */\n    public Tracker getTracker()\n    {\n        return data;\n    }\n\n    public Set<SSTableReader> getLiveSSTables()\n    {\n        return data.getView().liveSSTables();\n    }\n\n    public Iterable<SSTableReader> getSSTables(SSTableSet sstableSet)\n    {\n        return data.getView().select(sstableSet);\n    }\n\n    public Iterable<SSTableReader> getUncompactingSSTables()\n    {\n        return data.getUncompacting();\n    }\n\n    public Map<UUID, PendingStat> getPendingRepairStats()\n    {\n        Map<UUID, PendingStat.Builder> builders = new HashMap<>();\n        for (SSTableReader sstable : getLiveSSTables())\n        {\n            UUID session = sstable.getPendingRepair();\n            if (session == null)\n                continue;\n\n            if (!builders.containsKey(session))\n                builders.put(session, new PendingStat.Builder());\n\n            builders.get(session).addSSTable(sstable);\n        }\n\n        Map<UUID, PendingStat> stats = new HashMap<>();\n        for (Map.Entry<UUID, PendingStat.Builder> entry : builders.entrySet())\n        {\n            stats.put(entry.getKey(), entry.getValue().build());\n        }\n        return stats;\n    }\n\n    /**\n     * promotes (or demotes) data attached to an incremental repair session that has either completed successfully,\n     * or failed\n     *\n     * @return session ids whose data could not be released\n     */\n    public CleanupSummary releaseRepairData(Collection<UUID> sessions, boolean force)\n    {\n        if (force)\n        {\n            Predicate<SSTableReader> predicate = sst -> {\n                UUID session = sst.getPendingRepair();\n                return session != null && sessions.contains(session);\n            };\n            return runWithCompactionsDisabled(() -> compactionStrategyManager.releaseRepairData(sessions),\n                                              predicate, false, true, true);\n        }\n        else\n        {\n            return compactionStrategyManager.releaseRepairData(sessions);\n        }\n    }\n\n    public boolean isFilterFullyCoveredBy(ClusteringIndexFilter filter,\n                                          DataLimits limits,\n                                          CachedPartition cached,\n                                          int nowInSec,\n                                          boolean enforceStrictLiveness)\n    {\n        // We can use the cached value only if we know that no data it doesn't contain could be covered\n        // by the query filter, that is if:\n        //   1) either the whole partition is cached\n        //   2) or we can ensure than any data the filter selects is in the cached partition\n\n        // We can guarantee that a partition is fully cached if the number of rows it contains is less than\n        // what we're caching. Wen doing that, we should be careful about expiring cells: we should count\n        // something expired that wasn't when the partition was cached, or we could decide that the whole\n        // partition is cached when it's not. This is why we use CachedPartition#cachedLiveRows.\n        if (cached.cachedLiveRows() < metadata().params.caching.rowsPerPartitionToCache())\n            return true;\n\n        // If the whole partition isn't cached, then we must guarantee that the filter cannot select data that\n        // is not in the cache. We can guarantee that if either the filter is a \"head filter\" and the cached\n        // partition has more live rows that queried (where live rows refers to the rows that are live now),\n        // or if we can prove that everything the filter selects is in the cached partition based on its content.\n        return (filter.isHeadFilter() && limits.hasEnoughLiveData(cached,\n                                                                  nowInSec,\n                                                                  filter.selectsAllPartition(),\n                                                                  enforceStrictLiveness))\n               || filter.isFullyCoveredBy(cached);\n    }\n\n    public int gcBefore(int nowInSec)\n    {\n        return nowInSec - metadata().params.gcGraceSeconds;\n    }\n\n    @SuppressWarnings(\"resource\")\n    public RefViewFragment selectAndReference(Function<View, Iterable<SSTableReader>> filter)\n    {\n        long failingSince = -1L;\n        while (true)\n        {\n            ViewFragment view = select(filter);\n            Refs<SSTableReader> refs = Refs.tryRef(view.sstables);\n            if (refs != null)\n                return new RefViewFragment(view.sstables, view.memtables, refs);\n            if (failingSince <= 0)\n            {\n                failingSince = System.nanoTime();\n            }\n            else if (System.nanoTime() - failingSince > TimeUnit.MILLISECONDS.toNanos(100))\n            {\n                List<SSTableReader> released = new ArrayList<>();\n                for (SSTableReader reader : view.sstables)\n                    if (reader.selfRef().globalCount() == 0)\n                        released.add(reader);\n                NoSpamLogger.log(logger, NoSpamLogger.Level.WARN, 1, TimeUnit.SECONDS,\n                                 \"Spinning trying to capture readers {}, released: {}, \", view.sstables, released);\n                failingSince = System.nanoTime();\n            }\n        }\n    }\n\n    public ViewFragment select(Function<View, Iterable<SSTableReader>> filter)\n    {\n        View view = data.getView();\n        List<SSTableReader> sstables = Lists.newArrayList(Objects.requireNonNull(filter.apply(view)));\n        return new ViewFragment(sstables, view.getAllMemtables());\n    }\n\n    // WARNING: this returns the set of LIVE sstables only, which may be only partially written\n    public List<String> getSSTablesForKey(String key)\n    {\n        return getSSTablesForKey(key, false);\n    }\n\n    public List<String> getSSTablesForKey(String key, boolean hexFormat)\n    {\n        ByteBuffer keyBuffer = hexFormat ? ByteBufferUtil.hexToBytes(key) : metadata().partitionKeyType.fromString(key);\n        DecoratedKey dk = decorateKey(keyBuffer);\n        try (OpOrder.Group op = readOrdering.start())\n        {\n            List<String> files = new ArrayList<>();\n            for (SSTableReader sstr : select(View.select(SSTableSet.LIVE, dk)).sstables)\n            {\n                // check if the key actually exists in this sstable, without updating cache and stats\n                if (sstr.getPosition(dk, SSTableReader.Operator.EQ, false) != null)\n                    files.add(sstr.getFilename());\n            }\n            return files;\n        }\n    }\n\n    public void beginLocalSampling(String sampler, int capacity, int durationMillis)\n    {\n        metric.samplers.get(SamplerType.valueOf(sampler)).beginSampling(capacity, durationMillis);\n    }\n\n    @SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n    public List<CompositeData> finishLocalSampling(String sampler, int count) throws OpenDataException\n    {\n        Sampler samplerImpl = metric.samplers.get(SamplerType.valueOf(sampler));\n        List<Sample> samplerResults = samplerImpl.finishSampling(count);\n        List<CompositeData> result = new ArrayList<>(count);\n        for (Sample counter : samplerResults)\n        {\n            //Not duplicating the buffer for safety because AbstractSerializer and ByteBufferUtil.bytesToHex\n            //don't modify position or limit\n            result.add(new CompositeDataSupport(COUNTER_COMPOSITE_TYPE, COUNTER_NAMES, new Object[] {\n                    keyspace.getName() + \".\" + name,\n                    counter.count,\n                    counter.error,\n                    samplerImpl.toString(counter.value) })); // string\n        }\n        return result;\n    }\n\n    public boolean isCompactionDiskSpaceCheckEnabled()\n    {\n        return compactionSpaceCheck;\n    }\n\n    public void compactionDiskSpaceCheck(boolean enable)\n    {\n        compactionSpaceCheck = enable;\n    }\n\n    public void cleanupCache()\n    {\n        Collection<Range<Token>> ranges = StorageService.instance.getLocalReplicas(keyspace.getName()).ranges();\n\n        for (Iterator<RowCacheKey> keyIter = CacheService.instance.rowCache.keyIterator();\n             keyIter.hasNext(); )\n        {\n            RowCacheKey key = keyIter.next();\n            DecoratedKey dk = decorateKey(ByteBuffer.wrap(key.key));\n            if (key.sameTable(metadata()) && !Range.isInRanges(dk.getToken(), ranges))\n                invalidateCachedPartition(dk);\n        }\n\n        if (metadata().isCounter())\n        {\n            for (Iterator<CounterCacheKey> keyIter = CacheService.instance.counterCache.keyIterator();\n                 keyIter.hasNext(); )\n            {\n                CounterCacheKey key = keyIter.next();\n                DecoratedKey dk = decorateKey(key.partitionKey());\n                if (key.sameTable(metadata()) && !Range.isInRanges(dk.getToken(), ranges))\n                    CacheService.instance.counterCache.remove(key);\n            }\n        }\n    }\n\n    public ClusteringComparator getComparator()\n    {\n        return metadata().comparator;\n    }\n\n    public void snapshotWithoutFlush(String snapshotName)\n    {\n        snapshotWithoutFlush(snapshotName, null, false, null);\n    }\n\n    /**\n     * @param ephemeral If this flag is set to true, the snapshot will be cleaned during next startup\n     */\n    public Set<SSTableReader> snapshotWithoutFlush(String snapshotName, Predicate<SSTableReader> predicate, boolean ephemeral, RateLimiter rateLimiter)\n    {\n        if (rateLimiter == null)\n            rateLimiter = DatabaseDescriptor.getSnapshotRateLimiter();\n\n        Set<SSTableReader> snapshottedSSTables = new HashSet<>();\n        final JSONArray filesJSONArr = new JSONArray();\n        for (ColumnFamilyStore cfs : concatWithIndexes())\n        {\n            try (RefViewFragment currentView = cfs.selectAndReference(View.select(SSTableSet.CANONICAL, (x) -> predicate == null || predicate.apply(x))))\n            {\n                for (SSTableReader ssTable : currentView.sstables)\n                {\n                    File snapshotDirectory = Directories.getSnapshotDirectory(ssTable.descriptor, snapshotName);\n                    ssTable.createLinks(snapshotDirectory.getPath(), rateLimiter); // hard links\n                    filesJSONArr.add(ssTable.descriptor.relativeFilenameFor(Component.DATA));\n\n                    if (logger.isTraceEnabled())\n                        logger.trace(\"Snapshot for {} keyspace data file {} created in {}\", keyspace, ssTable.getFilename(), snapshotDirectory);\n                    snapshottedSSTables.add(ssTable);\n                }\n            }\n        }\n\n        writeSnapshotManifest(filesJSONArr, snapshotName);\n        if (!SchemaConstants.isLocalSystemKeyspace(metadata.keyspace) && !SchemaConstants.isReplicatedSystemKeyspace(metadata.keyspace))\n            writeSnapshotSchema(snapshotName);\n\n        if (ephemeral)\n            createEphemeralSnapshotMarkerFile(snapshotName);\n        return snapshottedSSTables;\n    }\n\n    "
        },
        {
            "sender": "PerformanceSensitivity",
            "content": "The configuration is Performance Sensitive: The `memtable_flush_writers` configuration directly impacts the number of concurrent flush operations that can be performed on memtables. Since flushing involves writing data to disk, which is a time-intensive operation, increasing the number of flush writers can significantly reduce the time taken to flush data, thereby affecting overall system performance. If this configuration is set too low, it can lead to increased latency and bottlenecks during write operations, especially under heavy load, making it critical for performance."
        }
    ]
}