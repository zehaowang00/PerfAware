{
    "performance_sensitive": "Yes",
    "config_name": "snapshot_before_compaction",
    "messages": [
        {
            "sender": "System",
            "content": "Initialize the analysis for the target system and configuration."
        },
        {
            "sender": "Developer",
            "content": "/**\n     * For internal use and testing only.  The rest of the system should go through the submit* methods,\n     * which are properly serialized.\n     * Caller is in charge of marking/unmarking the sstables as compacting.\n     */\nprotected void runMayThrow() throws Exception\n    {\n        // The collection of sstables passed may be empty (but not null); even if\n        // it is not empty, it may compact down to nothing if all rows are deleted.\n        assert transaction != null;\n\n        if (transaction.originals().isEmpty())\n            return;\n\n        // Note that the current compaction strategy, is not necessarily the one this task was created under.\n        // This should be harmless; see comments to CFS.maybeReloadCompactionStrategy.\n        CompactionStrategyManager strategy = cfs.getCompactionStrategyManager();\n\n        if (DatabaseDescriptor.isSnapshotBeforeCompaction())\n            cfs.snapshotWithoutFlush(System.currentTimeMillis() + \"-compact-\" + cfs.name);\n\n        try (CompactionController controller = getCompactionController(transaction.originals()))\n        {\n\n            final Set<SSTableReader> fullyExpiredSSTables = controller.getFullyExpiredSSTables();\n\n            // select SSTables to compact based on available disk space.\n            buildCompactionCandidatesForAvailableDiskSpace(fullyExpiredSSTables);\n\n            // sanity check: all sstables must belong to the same cfs\n            assert !Iterables.any(transaction.originals(), new Predicate<SSTableReader>()\n            {\n                @Override\n                public boolean apply(SSTableReader sstable)\n                {\n                    return !sstable.descriptor.cfname.equals(cfs.name);\n                }\n            });\n\n            UUID taskId = transaction.opId();\n\n            // new sstables from flush can be added during a compaction, but only the compaction can remove them,\n            // so in our single-threaded compaction world this is a valid way of determining if we're compacting\n            // all the sstables (that existed when we started)\n            StringBuilder ssTableLoggerMsg = new StringBuilder(\"[\");\n            for (SSTableReader sstr : transaction.originals())\n            {\n                ssTableLoggerMsg.append(String.format(\"%s:level=%d, \", sstr.getFilename(), sstr.getSSTableLevel()));\n            }\n            ssTableLoggerMsg.append(\"]\");\n\n            logger.info(\"Compacting ({}) {}\", taskId, ssTableLoggerMsg);\n\n            RateLimiter limiter = CompactionManager.instance.getRateLimiter();\n            long start = System.nanoTime();\n            long startTime = System.currentTimeMillis();\n            long totalKeysWritten = 0;\n            long estimatedKeys = 0;\n            long inputSizeBytes;\n\n            Set<SSTableReader> actuallyCompact = Sets.difference(transaction.originals(), fullyExpiredSSTables);\n            Collection<SSTableReader> newSStables;\n\n            long[] mergedRowCounts;\n            long totalSourceCQLRows;\n\n            // SSTableScanners need to be closed before markCompactedSSTablesReplaced call as scanners contain references\n            // to both ifile and dfile and SSTR will throw deletion errors on Windows if it tries to delete before scanner is closed.\n            // See CASSANDRA-8019 and CASSANDRA-8399\n            int nowInSec = FBUtilities.nowInSeconds();\n            try (Refs<SSTableReader> refs = Refs.ref(actuallyCompact);\n                 AbstractCompactionStrategy.ScannerList scanners = strategy.getScanners(actuallyCompact);\n                 CompactionIterator ci = new CompactionIterator(compactionType, scanners.scanners, controller, nowInSec, taskId))\n            {\n                long lastCheckObsoletion = start;\n                inputSizeBytes = scanners.getTotalCompressedSize();\n                double compressionRatio = scanners.getCompressionRatio();\n                if (compressionRatio == MetadataCollector.NO_COMPRESSION_RATIO)\n                    compressionRatio = 1.0;\n\n                long lastBytesScanned = 0;\n\n                activeCompactions.beginCompaction(ci);\n                try (CompactionAwareWriter writer = getCompactionAwareWriter(cfs, getDirectories(), transaction, actuallyCompact))\n                {\n                    // Note that we need to re-check this flag after calling beginCompaction above to avoid a window\n                    // where the compaction does not exist in activeCompactions but the CSM gets paused.\n                    // We already have the sstables marked compacting here so CompactionManager#waitForCessation will\n                    // block until the below exception is thrown and the transaction is cancelled.\n                    if (!controller.cfs.getCompactionStrategyManager().isActive())\n                        throw new CompactionInterruptedException(ci.getCompactionInfo());\n                    estimatedKeys = writer.estimatedKeys();\n                    while (ci.hasNext())\n                    {\n                        if (writer.append(ci.next()))\n                            totalKeysWritten++;\n\n\n                        long bytesScanned = scanners.getTotalBytesScanned();\n\n                        //Rate limit the scanners, and account for compression\n                        CompactionManager.compactionRateLimiterAcquire(limiter, bytesScanned, lastBytesScanned, compressionRatio);\n\n                        lastBytesScanned = bytesScanned;\n\n                        if (System.nanoTime() - lastCheckObsoletion > TimeUnit.MINUTES.toNanos(1L))\n                        {\n                            controller.maybeRefreshOverlaps();\n                            lastCheckObsoletion = System.nanoTime();\n                        }\n                    }\n\n                    // point of no return\n                    newSStables = writer.finish();\n                }\n                finally\n                {\n                    activeCompactions.finishCompaction(ci);\n                    mergedRowCounts = ci.getMergedRowCounts();\n                    totalSourceCQLRows = ci.getTotalSourceCQLRows();\n                }\n            }\n\n            if (transaction.isOffline())\n                return;\n\n            // log a bunch of statistics about the result and save to system table compaction_history\n            long durationInNano = System.nanoTime() - start;\n            long dTime = TimeUnit.NANOSECONDS.toMillis(durationInNano);\n            long startsize = inputSizeBytes;\n            long endsize = SSTableReader.getTotalBytes(newSStables);\n            double ratio = (double) endsize / (double) startsize;\n\n            StringBuilder newSSTableNames = new StringBuilder();\n            for (SSTableReader reader : newSStables)\n                newSSTableNames.append(reader.descriptor.baseFilename()).append(\",\");\n            long totalSourceRows = 0;\n            for (int i = 0; i < mergedRowCounts.length; i++)\n                totalSourceRows += mergedRowCounts[i] * (i + 1);\n\n            String mergeSummary = updateCompactionHistory(cfs.keyspace.getName(), cfs.getTableName(), mergedRowCounts, startsize, endsize);\n\n            logger.info(String.format(\"Compacted (%s) %d sstables to [%s] to level=%d.  %s to %s (~%d%% of original) in %,dms.  Read Throughput = %s, Write Throughput = %s, Row Throughput = ~%,d/s.  %,d total partitions merged to %,d.  Partition merge counts were {%s}\",\n                                       taskId,\n                                       transaction.originals().size(),\n                                       newSSTableNames.toString(),\n                                       getLevel(),\n                                       FBUtilities.prettyPrintMemory(startsize),\n                                       FBUtilities.prettyPrintMemory(endsize),\n                                       (int) (ratio * 100),\n                                       dTime,\n                                       FBUtilities.prettyPrintMemoryPerSecond(startsize, durationInNano),\n                                       FBUtilities.prettyPrintMemoryPerSecond(endsize, durationInNano),\n                                       (int) totalSourceCQLRows / (TimeUnit.NANOSECONDS.toSeconds(durationInNano) + 1),\n                                       totalSourceRows,\n                                       totalKeysWritten,\n                                       mergeSummary));\n            if (logger.isTraceEnabled())\n            {\n                logger.trace(\"CF Total Bytes Compacted: {}\", FBUtilities.prettyPrintMemory(CompactionTask.addToTotalBytesCompacted(endsize)));\n                logger.trace(\"Actual #keys: {}, Estimated #keys:{}, Err%: {}\", totalKeysWritten, estimatedKeys, ((double)(totalKeysWritten - estimatedKeys)/totalKeysWritten));\n            }\n            cfs.getCompactionStrategyManager().compactionLogger.compaction(startTime, transaction.originals(), System.currentTimeMillis(), newSStables);\n\n            // update the metrics\n            cfs.metric.compactionBytesWritten.inc(endsize);\n        }\n    }\n\n    "
        },
        {
            "sender": "PerformanceSensitivity",
            "content": "The configuration is Performance Sensitive: The `snapshot_before_compaction` configuration directly impacts the performance of the compaction process by introducing a snapshot operation that can be time-intensive, especially if the size of the SSTables is large. Taking a snapshot without flushing can lead to increased memory usage and longer processing times during compaction, as it requires additional resources to manage the snapshot state. This can significantly affect overall system performance during high-load scenarios where compaction is critical for maintaining efficiency."
        }
    ]
}